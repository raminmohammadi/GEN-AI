{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Assignment (Graded): Generate Synthetic Smiley Faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to your programming assignment on Generative Adversarial Networks! You will build a GAN to generate synthetic smiley faces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The objective of this assignment is to implement and train a Generative Adversarial Network (GAN) to generate synthetic smiley face images with varying facial features such as color, eyes, mouth, and optional accessories (glasses, hats).\n",
    "\n",
    "- The goal is to create a model that can generate realistic smiley faces with diverse characteristics, focusing on both facial structure and color diversity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 2,000 synthetic images of smiley faces generated programmatically using a custom Python script. Each smiley face has:\n",
    "- **Base Color**: Randomly selected from a predefined set of colors, with slight variations.\n",
    "- **Facial Features**: Eyes, mouth, and optional accessories like glasses and hats.\n",
    "- **Image Size**: 28x28 pixels, with 3 color channels (RGB).\n",
    "  \n",
    "The dataset is divided into two categories:  \n",
    "- **Real Images**: A set of real smiley face images, representing actual human-designed patterns.\n",
    "- **Generated Images**: A set of images produced by the GAN to simulate real smiley faces.\n",
    "\n",
    "The GAN will be trained to differentiate between real and generated images, with a focus on improving color diversity and facial feature arrangement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **GAN Architecture**:\n",
    "   - Design the **Generator** model to produce smiley face images from a latent space of 100 dimensions. Modify the network architecture to handle color variations effectively.\n",
    "   - Design the **Discriminator** model to classify images as real or fake (generated). Focus on extracting meaningful features that can differentiate real faces from synthetic ones.\n",
    "\n",
    "2. **Training the GAN**:\n",
    "   - Implement the training loop for the GAN model. Ensure the generator and discriminator are trained alternately, and incorporate the **color diversity loss** into the generator’s objective function.\n",
    "   - Train the model for 300 epochs using the Adam optimizer with a learning rate of 0.0002 and batch size of 64.\n",
    "   - Save model checkpoints every 10 epochs and monitor the generator and discriminator losses.\n",
    "\n",
    "3. **Evaluation and Visualization**:\n",
    "   - After training, generate a set of smiley face images using random latent vectors. Compare the generated images with real images.\n",
    "   - Implement a visualization function (`generate_and_compare_images`) to display both real and generated images side by side for comparison.\n",
    "   - Comment on the quality and diversity of the generated smiley faces. Evaluate the effectiveness of the color diversity loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Only write code when you see any of the below prompts,\n",
    "\n",
    "    ```\n",
    "    # YOUR CODE GOES HERE\n",
    "    # YOUR CODE ENDS HERE\n",
    "    # TODO\n",
    "    ```\n",
    "\n",
    "- Do not modify any other section of the code unless tated otherwise in the comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from typing import Tuple\n",
    "from dataclasses import dataclass\n",
    "from helpers.methods import generate_smiley_faces_dataset, analyze_data_distribution, visualize_images, generate_and_compare_images, setup_device\n",
    "from tests.test_methods import test_generator, test_discriminator, test_color_diversity_loss, test_train_step\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Setup the configurations(Do it iteratively while you traverse the code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Hints:**\n",
    "\n",
    "Complete the configuration by setting up the required fields in the GANConfig class.\n",
    "\n",
    "**Core Parameters**\n",
    "- Set batch_size for number of images per training batch\n",
    "- Define latent_dim for noise vector input\n",
    "- Configure epochs and learning_rate for training\n",
    "\n",
    "**Image Settings**\n",
    "- Set img_size for image dimensions\n",
    "- Define num_samples for total samples\n",
    "- Specify display counts for generated and real images\n",
    "\n",
    "**Storage**\n",
    "- Set checkpoint_dir for model saves\n",
    "\n",
    "Complete these fields using either default values or custom settings for your needs.\n",
    "\n",
    "Note: Implement changes iteratively as you work through the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GANConfig:\n",
    "    \"\"\"Configuration class to store GAN hyperparameters\"\"\"\n",
    "    # Hyperparameters\n",
    "    # WRITE YOUR CODE HERE\n",
    "    batch_size: int = \n",
    "    latent_dim: int = \n",
    "    epochs: int = \n",
    "    learning_rate: float = \n",
    "    num_generated_display: int = 5\n",
    "    checkpoint_dir: str = \"checkpoints\"\n",
    "    img_size: int = 28\n",
    "    num_samples: int = 2000\n",
    "    num_real_display: int = 5\n",
    "    \n",
    "    \n",
    "# Create config instance\n",
    "config = GANConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the checkpoint directory if it doesn't exist: DO NOT MODIFY THE CODE BELOW\n",
    "os.makedirs(config.checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset: DO NOT MODIFY THE BELOW CODE\n",
    "data, real_images = generate_smiley_faces_dataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA - Analyze data distribution: DO NOT MODIFY THE BELOW CODE\n",
    "analyze_data_distribution(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize real images: DO NOT MODIFY THE BELOW CODE\n",
    "visualize_images(real_images, title=\"Real Smiley Face Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Define the Generator Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Hints:**\n",
    "\n",
    "Complete the Generator model architecture with appropriate layers and configurations.\n",
    "\n",
    "**Model Structure**\n",
    "- Build sequential layers for the generator starting with Dense layer\n",
    "- Input shape should match latent_dim from config\n",
    "- Use upsampling pattern with Conv2DTranspose layers\n",
    "\n",
    "**Key Components**\n",
    "- Start with Dense layer (7 * 7 * 256) and reshape\n",
    "- Add BatchNormalization after convolutional layers\n",
    "- Use ReLU activation for intermediate layers\n",
    "- End with tanh activation for final layer\n",
    "\n",
    "**Layer Sequence**\n",
    "1. Dense + Reshape\n",
    "2. First upsampling block (128 filters)\n",
    "3. Second upsampling block (64 filters)\n",
    "4. Final Conv2D layer (3 channels for RGB)\n",
    "\n",
    "Note: Test your generator using the provided test function after implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Model Definitions\n",
    "# =========================================\n",
    "\n",
    "# In the Generator class, modify the architecture to better handle color generation\n",
    "class Generator(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Task: Define the generator model\n",
    "        # Hint: Use Conv2DTranspose layers to upsample the input\n",
    "        # Hint: Use BatchNormalization layers to normalize the input\n",
    "        # Hint: Use ReLU activation functions\n",
    "        # Hint: Use tanh activation function in the final layer\n",
    "        \n",
    "        # YOUR CODE GOES BELOW\n",
    "        self.model = \n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.model(inputs)\n",
    "\n",
    "# DO NOT MODIFY THE CODE BELOW\n",
    "generator = Generator()\n",
    "test_generator(config, generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Define the discriminator model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Hints:**\n",
    "\n",
    "Complete the Discriminator model architecture with appropriate layers and configurations.\n",
    "\n",
    "**Model Structure**\n",
    "- Build sequential layers starting with Input layer\n",
    "- Input shape must match image dimensions (img_size × img_size × 3)\n",
    "- Use downsampling pattern with Conv2D layers\n",
    "\n",
    "**Key Components**\n",
    "- Start with Input layer matching config dimensions\n",
    "- Add Conv2D layers with increasing filters\n",
    "- Use LeakyReLU with alpha=0.2\n",
    "- End with Sigmoid activation for binary classification\n",
    "\n",
    "**Layer Sequence**\n",
    "1. Input layer (matches config.img_size)\n",
    "2. First Conv2D block (64 filters)\n",
    "3. Second Conv2D block (128 filters)\n",
    "4. Flatten and Dense for output\n",
    "\n",
    "Note: Test your discriminator using the provided test function after implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the Discriminator class\n",
    "class Discriminator(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # Task: Define the discriminator model\n",
    "        # Hint: Use Conv2D layers to downsample the input\n",
    "        # Hint: Use LeakyReLU activation functions\n",
    "        # Hint: Use Sigmoid activation function in the final layer\n",
    "        \n",
    "        # YOUR CODE GOES BELOW\n",
    "        self.model = \n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.model(inputs)\n",
    "    \n",
    "# DO NOT MODIFY THE CODE BELOW\n",
    "discriminator = Discriminator()\n",
    "test_discriminator(config, discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Define the GAN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Hints:**\n",
    "\n",
    "Complete the GAN class implementation with training components and loss calculations.\n",
    "\n",
    "**Initialization Setup**\n",
    "- Setup optimizers with learning_rate from config\n",
    "- Set color_diversity_weight for enhanced generation\n",
    "\n",
    "**Training Components**\n",
    "- Implement color_diversity_loss function\n",
    "  - Calculate average colors per batch\n",
    "  - Compute color variance across batch\n",
    "  - Return negative variance for maximization\n",
    "\n",
    "**Training Step Implementation**\n",
    "1. Generate noise and fake images\n",
    "2. Calculate discriminator outputs\n",
    "3. Compute losses:\n",
    "   - Generator loss with color diversity\n",
    "   - Discriminator loss for real/fake images\n",
    "4. Apply gradients to both networks\n",
    "\n",
    "**Training Loop**\n",
    "- Iterate through epochs and batches\n",
    "- Track and display losses\n",
    "- Save model periodically\n",
    "\n",
    "Note: Implement proper gradient calculations and updates using tf.GradientTape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    \"\"\"GAN model that manages training and generation.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.device = '/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'\n",
    "        \n",
    "        # Set the device context\n",
    "        with tf.device(self.device):\n",
    "            # Task: Define the generator and discriminator models\n",
    "            # Hint: Create an instance of the Generator and Discriminator classes\n",
    "            self.generator = \n",
    "            self.discriminator = \n",
    "            \n",
    "            # Task: Define the optimizers and loss function\n",
    "            # Hint: Use Adam optimizer with beta_1=0.5 for both generator and discriminator\n",
    "            # Hint: Use BinaryCrossentropy loss function\n",
    "            self.g_optimizer = \n",
    "            self.d_optimizer = \n",
    "            self.cross_entropy = \n",
    "            \n",
    "        # Task: Define the color diversity weight\n",
    "        # Hint: Set the color diversity weight to 0.1\n",
    "        self.color_diversity_weight = \n",
    "    \n",
    "    \n",
    "    def color_diversity_loss(self, generated_images):\n",
    "        # Calculate the average color for each image in the batch\n",
    "        # Task: Calculate the average color for each image in the batch\n",
    "        # Hint: Use tf.reduce_mean to calculate the average color\n",
    "        # Hint: Set the axis parameter to [1, 2] to calculate the average color for each channel\n",
    "        \n",
    "        # YOUR CODE GOES BELOW\n",
    "        avg_colors = \n",
    "        \n",
    "        # Calculate the variance of colors in the batch\n",
    "        # Task: Calculate the variance of colors in the batch\n",
    "        # Hint: Use tf.math.reduce_variance to calculate the variance of colors\n",
    "        # Hint: Set the axis parameter to 0 to calculate the variance across the batch\n",
    "        # Hint: Use tf.reduce_mean to calculate the mean variance across the color channels\n",
    "        \n",
    "        # YOUR CODE GOES BELOW\n",
    "        color_variance = \n",
    "        \n",
    "        # Return negative variance (we want to maximize variance)\n",
    "        return -color_variance\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, real_images):    \n",
    "    # Task: Define the training step for the GAN model\n",
    "    \n",
    "        # Task: Define the batch size by extracting the first dimension of the real_images tensor\n",
    "        batch_size = \n",
    "        \n",
    "        # Task: Generate random noise with shape [batch_size, config.latent_dim]\n",
    "        noise = \n",
    "        \n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            # Task: Generate images using the generator model with the random noise input and set training=True\n",
    "            generated_images = \n",
    "            \n",
    "            # Task: Get the output of the discriminator for real and generated images with training=True\n",
    "            real_output = \n",
    "            \n",
    "            # Task: Get the output of the discriminator for generated images with training=True\n",
    "            fake_output = \n",
    "            \n",
    "            # Standard GAN losses\n",
    "            # Task: Calculate the generator and discriminator losses\n",
    "            # Hint: Use self.cross_entropy to calculate the loss\n",
    "            # Hint: Use tf.ones_like and tf.zeros_like to create the labels for the loss calculation\n",
    "            gen_loss = \n",
    "            real_loss = \n",
    "            fake_loss = \n",
    "            \n",
    "            # Add color diversity loss to generator loss\n",
    "            # Task: Calculate the color diversity loss using the generated images\n",
    "            # Hint: Use self.color_diversity_loss to calculate the color diversity loss\n",
    "            # Hint: Multiply the color diversity loss by the color diversity weight\n",
    "            # Hint: Add the color diversity loss to the generator loss\n",
    "            # Hint: Use the generated images as input to the color_diversity_loss function\n",
    "            # Hint: The Discriminator loss is the average of the real and fake loss\n",
    "            color_loss = \n",
    "            gen_loss += \n",
    "            \n",
    "            disc_loss = \n",
    "            \n",
    "        # Calculate and apply gradients\n",
    "        # Task: Calculate the gradients for generator and discriminator\n",
    "        # Hint: Use gen_tape.gradient and disc_tape.gradient to calculate the gradients\n",
    "        gen_gradients = \n",
    "        disc_gradients = \n",
    "        \n",
    "        # Task: Apply the gradients to the generator and discriminator using the optimizers\n",
    "        # Hint: Use the apply_gradients method of the optimizers to apply the gradients\n",
    "        # Hint: Use zip to combine the gradients with the trainable variables\n",
    "        # Hint: Apply the generator gradients using the g_optimizer\n",
    "        # Hint: Apply the discriminator gradients using the d_optimizer\n",
    "        self.g_optimizer.\n",
    "        self.d_optimizer.\n",
    "        \n",
    "        return gen_loss, disc_loss\n",
    "\n",
    "    def train(self, dataset):\n",
    "        checkpoint = tf.train.Checkpoint(\n",
    "            generator=self.generator,\n",
    "            discriminator=self.discriminator,\n",
    "            g_optimizer=self.g_optimizer,\n",
    "            d_optimizer=self.d_optimizer\n",
    "        )\n",
    "        manager = tf.train.CheckpointManager(\n",
    "            checkpoint, config.checkpoint_dir, max_to_keep=1\n",
    "        )\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            for batch in dataset:\n",
    "                # Task: Call the train_step function with the batch as input\n",
    "                g_loss, d_loss = \n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Gen Loss: {g_loss:.4f}, Disc Loss: {d_loss:.4f}\")\n",
    "                manager.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Main Execution\n",
    "# =========================================\n",
    "\n",
    "def main():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "    dataset = dataset.shuffle(config.num_samples).batch(config.batch_size)\n",
    "\n",
    "    # Initialize and train GAN\n",
    "    gan = GAN()\n",
    "    gan.train(dataset)\n",
    "\n",
    "    # Generate and display results\n",
    "    generate_and_compare_images(gan, real_images, config)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2-jmbtTJvLR"
      },
      "source": [
        "# Attention Mechanism (Graded)\n",
        "\n",
        "Welcome to your Attention (required) programming assignment! You will build a **Machine Translation** model using Attention. You will be using [Bilingual Sentence Pairs](https://www.kaggle.com/datasets/alincijov/bilingual-sentence-pairs) dataset which contains 81 distinct languages. The dataset is structured in such a way that there's an english translation against each sentence in a specific language.\n",
        "\n",
        "Your goal is to select a dataset of your liking and build a machine translation model to translate it into english!\n",
        "\n",
        "**Instructions:**\n",
        "* Do not modify any of the codes.\n",
        "* Only write code when prompted. For example in some sections you will find the following,\n",
        "```\n",
        "# YOUR CODE GOES HERE\n",
        "# YOUR CODE STARTS HERE\n",
        "# TODO\n",
        "```\n",
        "Only modify those sections of the code.\n",
        "\n",
        "**You will learn:**\n",
        "* Data preprocessing for Machine Translation problem. Cleaning the data, tokenizing it, and padding sequences to a uniform length.\n",
        "* Incorporating attention mechanism into the seq2seq model. This mechanism allows the decoder to focus on relevant parts of the input sequence when generating the output sequence, improving translation accuracy.\n",
        "* Build a robust Machine Translation model.\n",
        "* Inference using trained model to make predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhnPamR75bs9"
      },
      "source": [
        "# Machine Translation using Seq2Seq Attention Mechanism\n",
        "\n",
        "<img src=\"assets/attention-1.png\" width=500>\n",
        "\n",
        "1. **User Input:**\n",
        "  * The user enters an English sentence, which serves as the input to the model.\n",
        "\n",
        "2. **Encoder:**\n",
        "\n",
        "  * The encoder processes the English sentence, converting it into a numerical representation (embeddings). This involves breaking down the sentence into individual words or tokens and representing each token as a dense vector.\n",
        "  * The encoder typically uses a recurrent neural network (RNN) or a transformer to capture the sequential nature of the input. RNNs process the input sequence one word at a time, maintaining a hidden state that stores information about the previously seen words.\n",
        "\n",
        "3. **Seq2Seq Model:**\n",
        "\n",
        "  * This is the core of the model that connects the encoder and decoder.\n",
        "  The Seq2Seq model processes the encoded English sentence and passes it to the decoder.\n",
        "  * It acts as a bridge between the two components, ensuring that the information from the encoder is effectively transmitted to the decoder.\n",
        "\n",
        "4. **Decoder:**\n",
        "\n",
        "  * The decoder generates the Hindi sentence, one word at a time. It uses the encoded English sentence as context and its own previous outputs to predict the next word.\n",
        "  * The decoder often employs an attention mechanism to focus on relevant parts of the encoded English sentence while generating the Hindi output.\n",
        "  * This allows the decoder to align the English words with their corresponding Hindi translations, ensuring that the generated Hindi sentence is coherent and accurate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXoWBoN2ExsO"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import string\n",
        "import random\n",
        "import os\n",
        "from helpers import *\n",
        "from tests import *\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.auto import tqdm\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxGi64zGEfrZ"
      },
      "source": [
        "**Before we get started:**\n",
        "\n",
        "Download any .txt data from here: [Bilingual Sentence Pairs](https://www.kaggle.com/datasets/alincijov/bilingual-sentence-pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49PJZHtdFO0B"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "\n",
        "# Load dataset\n",
        "dataset_path = 'hin.txt' #TODO: Path to your dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhxIskpJL32x"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "\n",
        "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
        "\n",
        "    #TODO: Check the length of the dataset\n",
        "    lines = f.readlines()\n",
        "    num_lines = len(lines)\n",
        "\n",
        "    print(f\"Length of the dataset: {num_lines}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpRMLNfWMcSD"
      },
      "outputs": [],
      "source": [
        "# Printing a few lines from the dataset\n",
        "\n",
        "print(\"Sample lines:\")\n",
        "for i in range(5):\n",
        "    print(lines[i].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbpFM1W5CgvI"
      },
      "source": [
        "# Data Preparation\n",
        "\n",
        "The data is prepared by carrying out the following steps:\n",
        "\n",
        "\n",
        "1. **Data cleaning:** Lowercasing, removing punctuations, excluding special characters etc.\n",
        "2. **Tokenizing and Padding:** Padding input and target tensors to a uniform length.\n",
        "3. **Train/Test Split:** Create `tf.data.Dataset` objects for training and testing. Also shuffle and create batches for efficient training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt93bnOeCRYn"
      },
      "source": [
        "## Data Cleaning\n",
        "\n",
        "* **Lowercasing:** Converting all text to lowercase ensures that words are treated the same regardless of their capitalization.\n",
        "* **Removing Punctuations:** Punctuations can introduce noise and are often not relevant to the meaning of the text.\n",
        "* **Removing Special Characters/aposrophes:** Removing special characters and unnecessary symbols eliminates additional noise.\n",
        "* **Removing Extra Spaces:** Removing extra spaces and standardizing the spacing ensures that the text is uniformly formatted.\n",
        "* **Adding Special Tokens:** Adding special tokens such as <start> and <end> helps the model identify the beginning and end of sequences, which is important for sequence-to-sequence tasks like machine translation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGNKqSyWFB76"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "\n",
        "def preprocess(sent, sp_tokens=False):\n",
        "    '''\n",
        "    Performs all text preprocessing steps such as removing\n",
        "    unncecessary characters, adding special tokens, etc.\n",
        "\n",
        "    sent (str): sentence to preprocess\n",
        "    exclude (str): characters to exclude (like punctuations)\n",
        "    sp_tokens (bool): If True, special tokens '<start>' and '<end>'\n",
        "                      will be added. Default False.\n",
        "\n",
        "    reuturns (str): preprocessed sentence\n",
        "    '''\n",
        "\n",
        "\n",
        "    exclude = # TODO: create a set of punctuations to exclude(eg: !@#$%)\n",
        "    sent = #TODO: Lower case all the words inside 'sent'\n",
        "    sent = #TODO: Remove aposrophes ''\n",
        "    sent = #TODO: Get rid of all the punctuations\n",
        "    sent = #TODO: Remove leading and trailing spaces\n",
        "    sent = #TODO: Replace multiple spaces with a single space\n",
        "\n",
        "    if sp_tokens:\n",
        "        sent = #TODO: Add '<start>' and '<end>' tokens to the sentence\n",
        "\n",
        "    return sent\n",
        "\n",
        "def preprocess_lines(lines):\n",
        "    '''\n",
        "    Preprocesses the given list of sentence pairs.\n",
        "\n",
        "    args:\n",
        "      lines: list of tuples, each containing (source_sentence, target_sentence).\n",
        "\n",
        "    returns:\n",
        "      Preprocessed list of sentence pairs.\n",
        "    '''\n",
        "    prep_lines = [\n",
        "        [preprocess(i, sp_tokens=False),\n",
        "         preprocess(j, sp_tokens=True)]\n",
        "        for i, j in lines\n",
        "    ]\n",
        "\n",
        "    return prep_lines\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQSHuSFJzC8z"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "\n",
        "def load_dataset(path, reverse=False):\n",
        "    '''\n",
        "    Loads and optionally reverses the language pairs inside the dataset.\n",
        "\n",
        "    args:\n",
        "      path: str, path to the translation file.\n",
        "      reverse: bool, optional, if True, reverse the language pairs. Default False.\n",
        "\n",
        "    returns:\n",
        "      lines (list): list of sentence pairs.\n",
        "    '''\n",
        "    lines = #TODO: Create a list of lists of language pairs\n",
        "\n",
        "    if reverse:\n",
        "        lines = #TODO: Reverse the positions of language pairs\n",
        "\n",
        "    return lines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBZnfEc8zDde"
      },
      "source": [
        "**Expected lines to return:**\n",
        "\n",
        "  ```\n",
        "  [['Wow!', 'वाह!'], ['Help!', 'बचाओ!'], ['Jump.', 'उछलो.'] ...... ]\n",
        "\n",
        "  ```\n",
        "\n",
        "***Note:*** *The second value inside the list of lists can be any chosen language that you selected for this task*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtKVc9lR3He3"
      },
      "source": [
        "## Tokenizing and Padding\n",
        "\n",
        "* **Tokenization:** Tokenization is essential to convert text data into a numerical format that can be processed by machine learning models.\n",
        "* **Padding:** Padding ensures that all sequences in a batch have the same length, which is a requirement for many deep learning models, especially those using recurrent neural networks or transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CspH5WAubT0X"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_tensors(prep_lines, inp_lang, tgt_lang):\n",
        "    '''\n",
        "    Converts preprocessed sentences into input and target tensors.\n",
        "\n",
        "    args:\n",
        "      prep_lines: list of preprocessed sentence pairs.\n",
        "      inp_lang: LanguageIndex object for the input language.\n",
        "      tgt_lang: LanguageIndex object for the target language.\n",
        "\n",
        "    returns:\n",
        "      input_tensor: list of input tensors.\n",
        "      target_tensor: list of target tensors.\n",
        "    '''\n",
        "    input_tensor = [[inp_lang.word2idx[w] for w in inp.split(' ')]\n",
        "                    for inp, tgt in prep_lines]\n",
        "\n",
        "    target_tensor = [[tgt_lang.word2idx[w] for w in tgt.split(' ')]\n",
        "                     for inp, tgt in prep_lines]\n",
        "\n",
        "    return input_tensor, target_tensor\n",
        "\n",
        "#TODO\n",
        "def pad_tensors(input_tensor, target_tensor, max_len=None):\n",
        "    '''\n",
        "    Pads input and target tensors to the specified max length.\n",
        "\n",
        "    args:\n",
        "      input_tensor: list of input tensors.\n",
        "      target_tensor: list of target tensors.\n",
        "      max_len: int, optional, maximum length for padding. Default None.\n",
        "\n",
        "    returns:\n",
        "      input_tensor: padded input tensor.\n",
        "      target_tensor: padded target tensor.\n",
        "      max_length_inp: int, maximum length of input tensor.\n",
        "      max_length_tgt: int, maximum length of target tensor.\n",
        "    '''\n",
        "    if max_len:\n",
        "        input_tensor = #TODO: Create a list of lists of input_tensors with sequences <= max_len\n",
        "        target_tensor = #TODO: Create a list of lists of target_tensors with sequences <= max_len\n",
        "        max_length_inp = max_length_tgt = max_len\n",
        "    else:\n",
        "        max_length_inp = #TODO: Get the length of the longest sequence in the input_tensor.\n",
        "        max_length_tgt = #TODO: Get the length of the longest sequence in the target_tensor.\n",
        "\n",
        "    input_tensor =  #TODO: pad input_tensor sequences using keras 'pad_sequences' method.\n",
        "\n",
        "    target_tensor = #TODO: pad target_tensor sequences using keras 'pad_sequences' method.\n",
        "\n",
        "    return input_tensor, target_tensor, max_length_inp, max_length_tgt\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIYrByMq34Hi"
      },
      "source": [
        "## Train/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZiPGwE81dXW"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "def split_dataset(input_tensor, target_tensor, batch_size):\n",
        "    '''\n",
        "    Splits the dataset into training and validation sets.\n",
        "\n",
        "    args:\n",
        "      input_tensor: padded input tensor.\n",
        "      target_tensor: padded target tensor.\n",
        "      batch_size: int, batch size for training and validation sets.\n",
        "\n",
        "    returns:\n",
        "      train_dataset: tf.data.Dataset for training.\n",
        "      test_dataset: tf.data.Dataset for validation.\n",
        "    '''\n",
        "    input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split( #TODO: Split the input and target tensor into train and test sets\n",
        "        input_tensor,\n",
        "        target_tensor,\n",
        "        test_size=0.1,\n",
        "        random_state=42)\n",
        "\n",
        "    train_dataset = #TODO: Create a TF dataset object, shuffle it and divide it into batches\n",
        "\n",
        "    test_dataset = #TODO: Create a TF dataset object, shuffle it and divide it into batches\n",
        "\n",
        "    return train_dataset, test_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8h_5rFft1zm"
      },
      "outputs": [],
      "source": [
        "\n",
        "def data_loader(path, batch_size, max_len=None, reverse=False):\n",
        "    '''\n",
        "    Combines all the above methods to load and preprocess the dataset.\n",
        "\n",
        "    args:\n",
        "\n",
        "      path: str, path to the translation file.\n",
        "      batch_size: int, batch size for training and validation sets.\n",
        "      max_len: int, optional, maximum length for padding. Default None.\n",
        "      reverse: bool, optional, if True, reverse the language pairs. Default False.\n",
        "\n",
        "    returns:\n",
        "\n",
        "      train_dataset: tf.data.Dataset for training.\n",
        "      test_dataset: tf.data.Dataset for validation.\n",
        "      inp_lang: LanguageIndex object for input language.\n",
        "      tgt_lang: LanguageIndex object for target language.\n",
        "      max_length_inp: int, maximum length of input tensor.\n",
        "      max_length_tgt: int, maximum length of target tensor.\n",
        "    '''\n",
        "    lines = #TODO: Load the dataset\n",
        "    prep_lines = #TODO: Preprocess the input sequences\n",
        "    inp_lang, tgt_lang = create_language_indices(prep_lines)\n",
        "    input_tensor, target_tensor = create_tensors(prep_lines, inp_lang, tgt_lang)\n",
        "    input_tensor, target_tensor, max_length_inp, max_length_tgt = #TODO: Pad the input and target tensors\n",
        "\n",
        "    train_dataset, test_dataset = split_dataset(input_tensor, target_tensor, batch_size)\n",
        "\n",
        "    return train_dataset, test_dataset, inp_lang, tgt_lang, max_length_inp, max_length_tgt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AKNcWcM5C5_"
      },
      "source": [
        "# Model training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XT0pNNW6i9J"
      },
      "source": [
        "### Encoder Network\n",
        "\n",
        "The encoder is responsible for processing the input sequence (the English sentence) and transforming it into a fixed-length vector representation. This representation, often called the \"context vector\", captures the essence of the input sequence and is passed to the decoder.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgfkQudwO0Vi"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "\n",
        "# Encoder\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, embed_dim, units, vocab_size):\n",
        "        super().__init__()\n",
        "        self.embedding = #TODO: Add an Embedding layer with mask set to True\n",
        "        self.rnn = #TODO: Add an LSTM layer with return_state and sequences set to True\n",
        "\n",
        "    def call(self, x):\n",
        "        # x => (batch_size, max_len)\n",
        "        x = #TODO: Input Embedding dim (batch_size, s, embed_dim)\n",
        "        enc_outputs = #TODO: RNN encoder inputs\n",
        "        return enc_outputs[0], enc_outputs[1:]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy0QFaMrLI20"
      },
      "source": [
        "### Attention\n",
        "\n",
        "We perform the Additive attention, also known as Bahdanau attention, was introduced by Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio in their 2015 paper, \"Neural Machine Translation by Jointly Learning to Align and Translate.\"\n",
        "\n",
        "Additive attention allows the model to automatically learn which parts of the input sequence are most relevant to each output word during decoding. The name \"additive\" comes from the fact that it combines (adds) the decoder's hidden state and the encoder's hidden states to compute attention scores.\n",
        "\n",
        "the process of calculating attention weights and using them to create a context vector that captures the relevant information from the source sequence for generating the target sequence is as follows:\n",
        "\n",
        "$$\n",
        "e_{ij} = v_a^T \\tanh(W_a s_{i-1} + U_a h_j)\n",
        "$$\n",
        "\n",
        "where,\n",
        "* $eij$ is the alignment score between the i-th element of the source sequence and the j-th element of the target sequence.\n",
        "* $va, Wa$, and Ua are matrices used to calculate the alignment score.\n",
        "* $s_i-1$ is the hidden state of the decoder at the previous time step.\n",
        "*$hj$ is the hidden state of the encoder at the j-th time step.\n",
        "\n",
        "\n",
        "$$\n",
        "a_{ij} = \\frac{exp(e_{ij})}{\\sum_{k=1}^{T_x} exp(e_{ik})}\n",
        "$$\n",
        "\n",
        "where,\n",
        "* $aij$ is the attention weight for the i-th element of the source sequence and the j-th element of the target sequence.\n",
        "* The equation calculates the attention weight by normalizing the alignment scores using the softmax function.\n",
        "\n",
        "\n",
        "$$\n",
        "c_i = \\sum_{j=1}^{T_x} \\alpha_{ij} h_j\n",
        "$$\n",
        "\n",
        "where,\n",
        "* $ci$ is the context vector at the i-th time step of the decoder.\n",
        "* The equation calculates the context vector by taking a weighted sum of the encoder hidden states, where the weights are the attention weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AajRIVoc5_zM"
      },
      "outputs": [],
      "source": [
        "\n",
        "#TODO\n",
        "class Attention(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.W_q = #TODO: define query(dense layer)\n",
        "        self.W_k = #TODO: define key(dense layer)\n",
        "        self.W_v = #TODO: define value(dense layer)\n",
        "\n",
        "\n",
        "    def call(self, query, key, value, mask=None):\n",
        "        query, key = self.W_q(query), self.W_k(key)\n",
        "        # query => (batch_size, t, units)\n",
        "        # key => (batch_size, s, units)\n",
        "\n",
        "        score = self.W_v(\n",
        "            tf.math.tanh(\n",
        "                tf.expand_dims(query, 2) + tf.expand_dims(key, 1)\n",
        "            )\n",
        "        )\n",
        "        score = tf.squeeze(score, -1)\n",
        "        # score => (batch_size, t, s)\n",
        "\n",
        "        if mask is not None:\n",
        "            score = tf.where(mask, score, -1e6)\n",
        "\n",
        "        attention_weights = #TODO: attention weights shape (batch_size, t, s)\n",
        "\n",
        "        context = #TODO: context vector shape (batch_size, t, units)\n",
        "\n",
        "        return context, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0p8eVvLK7Sn"
      },
      "source": [
        "### Decoder Network\n",
        "\n",
        "The decoder is responsible for generating the target sequence (language you chose) word by word, using the context vector produced by the encoder and its own internal state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdDk6YhMx5DR"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, embed_dim, units, vocab_size):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding layer to convert tokens to vectors\n",
        "        self.embedding = tf.keras.layers.Embedding(\n",
        "            vocab_size, embed_dim, mask_zero=True)\n",
        "\n",
        "        # RNN layer\n",
        "        self.rnn = tf.keras.layers.LSTM(\n",
        "            units, return_sequences=True, return_state=True)\n",
        "\n",
        "        # Attention layer\n",
        "        self.attention = Attention(units)\n",
        "\n",
        "        # Final layer to output logits, we can use\n",
        "        # argmax to know which output token is predicted.\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "\n",
        "    def call(self, x, enc_outputs, state, mask=None):\n",
        "        x = #TODO: Add an Embedding layer with x as input\n",
        "        # x => (batch_size, t, embed_dim)\n",
        "\n",
        "        dec_outputs = #TODO: Add an rnn layer with initial_state state to state\n",
        "        output = dec_outputs[0]\n",
        "        state = dec_outputs[1:]\n",
        "        # output   => (batch_size, t, units)\n",
        "        # state[i] => (batch_size, s, units)\n",
        "\n",
        "        context_vector, attention_weights = self.attention(\n",
        "            query=output,\n",
        "            key=enc_outputs,\n",
        "            value=enc_outputs,\n",
        "            mask=mask\n",
        "        )\n",
        "        # context_vector => (batch_size, t, units)\n",
        "        # attention_weights => (batch_size, t, s)\n",
        "\n",
        "        context_rnn_output = tf.concat(\n",
        "            [context_vector, output], axis=-1)\n",
        "        # context_rnn_output => (batch_size, t, 2*units)\n",
        "\n",
        "        pred = self.fc(context_rnn_output)\n",
        "        # pred => (batch_size, t, vocab_size)\n",
        "\n",
        "        return pred, state, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bXhi4hC6d7c"
      },
      "source": [
        "## Custom Loss Function\n",
        "\n",
        "the following custom loss function calculates the cross-entropy loss between the predicted and actual target tokens, but it ignores the padding tokens to avoid penalizing the model for predicting them. This helps the model focus on learning the actual translation task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGs1idKr6a4e"
      },
      "outputs": [],
      "source": [
        "# Custom loss function:\n",
        "def loss_fn(y_true, y_pred):\n",
        "    # y_true => (batch_size, max_len)\n",
        "    # y_pred => (batch_size, max_len, vocab_size)\n",
        "\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')(y_true, y_pred)\n",
        "\n",
        "    # masking the padding tokens\n",
        "    loss = tf.reduce_sum(loss * mask)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-uhCzIGTiaP"
      },
      "source": [
        "## Model Training\n",
        "\n",
        " We're going to orchestrate the entire training process of the machine translation model, from data preparation to model updates and performance tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmxtpcVx6ay6"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "\n",
        "# TODO: Define Hyperparameters\n",
        "BATCH_SIZE = #Minimum 64\n",
        "EMBEDDING_DIM =  #Minimum 50\n",
        "UNITS = #Minimum 50\n",
        "NUM_EPOCHS =\n",
        "\n",
        "validate_data_loader(data_loader, path=dataset_path, batch_size=batch_size)\n",
        "\n",
        "train_dataset, test_dataset, inp_lang, tgt_lang, max_length_inp, max_length_tgt = #TODO: Load data using Dataloader\n",
        "\n",
        "vocab_inp_size = len(inp_lang.word2idx) #Vocab size of English\n",
        "vocab_tgt_size = len(tgt_lang.word2idx) #Vicab size of language you chose\n",
        "\n",
        "#TODO: Load Adam optimizer\n",
        "\n",
        "# Creating instances of encoder and decoder\n",
        "encoder = Encoder(EMBEDDING_DIM, UNITS, vocab_inp_size)\n",
        "validate_encoder(encoder, vocab_inp_size, embed_dim=64, units=50)\n",
        "\n",
        "decoder = Decoder(EMBEDDING_DIM, UNITS, vocab_tgt_size)\n",
        "\n",
        "#Storing loss at each epoch to plot later\n",
        "history = {\n",
        "'loss': []\n",
        "}\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    total_loss = 0.\n",
        "\n",
        "    with tqdm(total=len(train_dataset)) as pbar:\n",
        "        for batch, (x, y) in enumerate(train_dataset):\n",
        "            inp_mask = tf.expand_dims(x != 0, axis=1)\n",
        "            tgt_mask = tf.cast(y != 0, tf.float32)\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                loss = tf.constant(0.0)\n",
        "                enc_outputs, enc_states = encoder(x)\n",
        "                dec_states = enc_states\n",
        "\n",
        "                dec_input = tf.expand_dims(y[:, 0], axis=1)\n",
        "                for t in range(1, x.shape[1]):\n",
        "                    dec_outputs, dec_states, tmp_a = decoder(\n",
        "                        dec_input, enc_outputs,\n",
        "                        dec_states, inp_mask)\n",
        "\n",
        "                    loss += loss_fn(\n",
        "                        tf.expand_dims(y[:, t], axis=1), dec_outputs)\n",
        "                    dec_input = tf.expand_dims(y[:, t], axis=1)\n",
        "\n",
        "                loss = loss/tf.reduce_sum(tgt_mask)\n",
        "\n",
        "            variables = (encoder.trainable_variables +\n",
        "            decoder.trainable_variables)\n",
        "            gradients = tape.gradient(loss, variables)\n",
        "            trainer.apply_gradients(zip(gradients, variables))\n",
        "            total_loss += loss.numpy()\n",
        "            pbar.update(1)\n",
        "\n",
        "    epoch_loss = total_loss/len(train_dataset)\n",
        "    history['loss'].append(epoch_loss)\n",
        "    print(f'Epoch: {epoch} | Loss: {epoch_loss}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmXyRikvtxy2"
      },
      "outputs": [],
      "source": [
        "epochs = list(range(len(history['loss'])))\n",
        "plt.plot(epochs, history['loss'])\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_bRvQHMTZUG"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DueLx9NjTKUV"
      },
      "outputs": [],
      "source": [
        "eng, hin = next(iter(train_dataset))\n",
        "\n",
        "idx = -5\n",
        "actual_seq = detokenize(hin[idx], tgt_lang.idx2word)\n",
        "translation, att_wgts = predict_seq2seq(encoder, decoder, eng[idx], tgt_lang, max_length_tgt)\n",
        "\n",
        "eng_sent = detokenize(eng[idx], inp_lang.idx2word)\n",
        "print(f'English Sentence:      {eng_sent}')\n",
        "print(f'\\nPredicted Translation: {translation}')\n",
        "print(f'Actual Translation:    {actual_seq}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing attention weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4134bCvTKDU"
      },
      "outputs": [],
      "source": [
        "import plotly\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "plotly.offline.init_notebook_mode(connected=True)\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "idx = -5\n",
        "actual_seq = detokenize(hin[idx], tgt_lang.idx2word)\n",
        "translation, att_wgts = predict_seq2seq(encoder, decoder, eng[idx], tgt_lang, max_length_tgt)\n",
        "\n",
        "eng_sent = detokenize(eng[idx], inp_lang.idx2word)\n",
        "print(f'English Sentence:      {eng_sent}')\n",
        "print(f'Predicted Translation: {translation}')\n",
        "print(f'Actual Translation:    {actual_seq}')\n",
        "\n",
        "\n",
        "eng_tokens = eng_sent.strip().split(' ')\n",
        "eng_tokens += ['<PAD>'] * (max_length_inp - len(eng_tokens))\n",
        "\n",
        "trace = go.Heatmap(\n",
        "    z=att_wgts, \n",
        "    x= eng_tokens, \n",
        "    y=translation.split(' '), \n",
        "    colorscale='Reds'\n",
        ")\n",
        "iplot([trace])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

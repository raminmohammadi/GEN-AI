{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "ZQ4sJZSkNStp"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "    <h1>Attention Mechanism</h1>\n",
        "</center>"
      ],
      "metadata": {
        "id": "hpPSpgoL1jES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Brief Recap\n",
        "\n",
        "\n",
        "**Attention** is a concept in machine learning and AI that goes back many years, especially in computer vision. Like the word \"neural network\", attention was inspired by the idea of attention in how human brains deal with the massive amount of visual and audio input. **Attention layers** are deep learning layers that evoke the idea of attention.\n",
        "\n",
        "The most common place you'll see attention layers is in [**transformer**](http://d2l.ai/chapter_attention-mechanisms/transformer.html) neural networks that model sequences. We'll also sometimes see attention in graph neural networks.\n",
        "\n",
        "$$\n",
        "Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* Q represents the query matrix.\n",
        "* K represents the key matrix.\n",
        "* V represents the value matrix.\n",
        "* $d_k$$d_k$ is the dimension of the key vectors.\n",
        "* softmax is the softmax function, which normalizes the attention weights.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JTcqQJ0x1i92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Architecure\n",
        "\n",
        "\n",
        "<img src='assets/attention.png' width=400/>\n",
        "\n",
        "Here's a breakdown of the components:\n",
        "\n",
        "* **Input Sequence:** This is the original input data, typically a sequence of words or tokens.\n",
        "\n",
        "* **Query, Key, Value:** These are three vectors associated with each element in the input sequence.\n",
        "\n",
        "* **Query:** Represents the current element being processed.\n",
        "Key: Represents the information about other elements in the sequence.\n",
        "Value: Contains the actual content or representation of the element.\n",
        "\n",
        "* **Attention Scores:** These are calculated by computing the dot product between the query and each key in the sequence. This measures the similarity or relevance between the current element and other elements.\n",
        "\n",
        "* **Softmax:** This function converts the attention scores into probabilities, ensuring that the sum of all probabilities equals 1.\n",
        "\n",
        "* **Attention Weights:** These are the normalized attention scores, representing the importance of each element in the input sequence for the current processing step.\n",
        "\n",
        "* **Weighted Sum:** The weighted sum is calculated by multiplying each value in the input sequence by its corresponding attention weight and then summing the results. This effectively combines the information from different elements based on their relevance.\n",
        "\n",
        "* **Output:** The final output of the attention mechanism is the weighted sum, which represents the contextually relevant representation of the current element in the input sequence.\n",
        "\n",
        "\n",
        "Overall, the attention mechanism allows the model to dynamically focus on different parts of the input sequence, capturing dependencies and relationships between elements.\n",
        "\n"
      ],
      "metadata": {
        "id": "38SJskfG1tuH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applications\n",
        "\n",
        "**Natural Language Processing (NLP)**\n",
        "* **Machine Translation:** The attention mechanism helps the model focus on relevant parts of the source sentence when generating the target sentence.\n",
        "* **Text Summarization:** By attending to the most important sentences or phrases, the model can generate concise summaries.\n",
        "* **Question Answering:** The model can attend to relevant parts of the text to answer questions accurately.\n",
        "* **Sentiment Analysis:** Understanding the context of words helps the model accurately classify sentiment.\n",
        "\n",
        "**Computer Vision**\n",
        "* **Image Captioning:** The model can attend to different parts of the image to generate descriptive captions.\n",
        "* **Object Detection:** The model can focus on specific regions of the image to identify objects.\n",
        "* **Image Generation:** The attention mechanism can be used to generate images based on textual descriptions.\n",
        "\n",
        "**Other Applications**\n",
        "* **Speech Recognition:** The model can attend to different parts of the audio signal to improve accuracy.\n",
        "* **Recommendation Systems:** The model can focus on relevant user preferences and item features to make personalized recommendations.\n",
        "* **Time Series Forecasting:** The model can attend to different time periods to predict future values.\n"
      ],
      "metadata": {
        "id": "pMyIk7fn1tqJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of Attention using TensorFlow\n",
        "\n",
        "\n",
        "We'll implement it step by step using TensorFlow, focusing on the key components:\n",
        "\n",
        "* **Query, Key, and Value Projections:** These are transformations of input data.\n",
        "* **Score Calculation:** Compute the attention scores between queries and keys.\n",
        "* **Softmax Scaling:** Normalize the scores to get attention weights.\n",
        "* **Weight Application:** Multiply the weights by the values to get the output.\n",
        "\n"
      ],
      "metadata": {
        "id": "R9dogSW87zsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach 1\n",
        "\n",
        "**Step 1: Calculate the Dot Product**\n",
        "\n",
        "```\n",
        "matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "```\n",
        "\n",
        " This calculates the similarity or relevance between the query and each key in the input sequence. The resulting matrix `matmul_qk` stores these similarity scores, which are then used in subsequent steps of the attention mechanism to determine the attention weights.\n",
        "\n",
        "**Step 2: Scale the Scores**<br>\n",
        "```\n",
        "depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "scaled_attention_logits = matmul_qk / tf.math.sqrt(depth)\n",
        "```\n",
        "To stabilize gradients during training, we scale the scores by $\\sqrt{dk}$ (the dimension of keys). This helps to keep the values of the dot product in a range where the softmax function is effective.\n",
        "\n",
        "**Step 3: Apply the Mask (Optional)**<br>\n",
        "The mask is used to prevent certain tokens from being attended to, such as padding tokens. If a position should not be attended to, it is set to a large negative value, making its softmax output near zero.\n",
        "\n",
        "**Step 4: Apply Softmax**<br>\n",
        "\n",
        "```\n",
        "attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "```\n",
        "\n",
        "The softmax function converts these scores into probabilities, highlighting the most relevant positions for each query. The output is a probability distribution over the sequence length.\n",
        "\n",
        "**Step 5: Weighted Sum**<br>\n",
        "```\n",
        "output = tf.matmul(attention_weights, value)\n",
        "```\n",
        "\n",
        "Finally, we multiply these probabilities (attention weights) by the value matrix to get a weighted sum. This weighted sum is the attention output."
      ],
      "metadata": {
        "id": "pD6wQ1F_GUUC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzkxDmGQ1hJb"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from helpers import *\n",
        "\n",
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "    \"\"\"\n",
        "    Calculate the attention weights.\n",
        "\n",
        "    query: shape == (..., seq_len_q, depth)\n",
        "    key: shape == (..., seq_len_k, depth)\n",
        "    value: shape == (..., seq_len_v, depth_v)\n",
        "    mask: shape == (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "    output: shape == (..., seq_len_q, depth_v)\n",
        "    attention_weights: shape == (..., seq_len_q, seq_len_k)\n",
        "    \"\"\"\n",
        "    # Step 1: Calculate the dot product between query and key\n",
        "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "    # Step 2: Scale the dot product by the square root of the dimension of the key\n",
        "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(depth)\n",
        "\n",
        "    # Step 3: Apply the mask (if provided) to prevent attending to certain positions\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    # Step 4: Apply softmax to get the attention weights\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "    # Step 5: Multiply the weights by the value to get the weighted sum (output)\n",
        "    output = tf.matmul(attention_weights, value)\n",
        "\n",
        "    return output, attention_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example Usage\n",
        "\n",
        "We'll use the scaled dot-product attention function to **focus on which words in a sequence each query attends to**.\n",
        "\n",
        "For this demonstration, we'll:\n",
        "\n",
        "* Tokenize a few sentences.\n",
        "* Embed them using a simple embedding matrix.\n",
        "* Use the scaled_dot_product_attention() method on the embedded tokens.\n",
        "* Visualize the attention weights using a heatmap to show which parts of the input sequence each query focuses on.\n"
      ],
      "metadata": {
        "id": "7umotsuj_Q_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Setup and Preprocessing\n",
        "\n",
        "We will use a basic tokenizer and TensorFlow to prepare the input sentences:\n",
        "\n"
      ],
      "metadata": {
        "id": "l8S-2zRODzun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Sample sentences (batch of sentences)\n",
        "sentences = [\n",
        "    \"I love machine learning\",\n",
        "    \"Attention mechanisms are powerful\",\n",
        "    \"Transformers have changed NLP\",\n",
        "    \"Deep learning is amazing\"\n",
        "]\n",
        "\n",
        "# Create a basic word-to-index mapping (tokenizer)\n",
        "vocab = {word: idx for idx, word in enumerate(set(' '.join(sentences).lower().split()), 1)}\n",
        "vocab_size = len(vocab) + 1  # +1 for padding token\n",
        "\n",
        "# Function to tokenize a sentence and convert to indices\n",
        "def tokenize(sentence):\n",
        "    return [vocab[word.lower()] for word in sentence.split()]\n",
        "\n",
        "# Convert each sentence into indices and pad them to equal length\n",
        "tokenized_sentences = [tokenize(sentence) for sentence in sentences]\n",
        "max_len = max(len(sentence) for sentence in tokenized_sentences)\n",
        "padded_sentences = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    tokenized_sentences, maxlen=max_len, padding='post'\n",
        ")\n",
        "\n",
        "print(\"Padded Sentences (Tokenized):\\n\", padded_sentences)"
      ],
      "metadata": {
        "id": "tas1ZfZB_gdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Embedding and Attention Calculation\n",
        "Let's use an embedding layer to convert tokenized inputs into dense vectors:\n",
        "\n"
      ],
      "metadata": {
        "id": "aGtVQqmbDWYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Create an Embedding Layer to get dense representations of tokens\n",
        "embedding_dim = 8  # Embedding dimension\n",
        "embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
        "\n",
        "# Get the embedded representations for each token in each sentence\n",
        "embedded_sentences = embedding_layer(padded_sentences)\n",
        "\n",
        "# Step 3: Use the same embeddings as queries, keys, and values for simplicity\n",
        "query = embedded_sentences\n",
        "key = embedded_sentences\n",
        "value = embedded_sentences\n",
        "\n",
        "# Calculate the attention output and weights using the scaled_dot_product_attention function\n",
        "output, attention_weights = scaled_dot_product_attention(query, key, value)\n",
        "\n",
        "print(\"Attention Weights Shape:\", attention_weights.shape)  # (batch_size, seq_len, seq_len)\n"
      ],
      "metadata": {
        "id": "X7T_DPog9Yii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Visualizing Attention Weights\n",
        "We can plot the attention weights to see which words each token focuses on:\n",
        "\n"
      ],
      "metadata": {
        "id": "0o-jYn3gEDMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a sentence to visualize (e.g., first sentence)\n",
        "sentence_idx = 2\n",
        "original_sentence = sentences[sentence_idx].split()\n",
        "\n",
        "# Step 4: Plot the attention weights for the selected sentence\n",
        "plot_attention_weights(attention_weights.numpy(), original_sentence, sentence_idx)\n"
      ],
      "metadata": {
        "id": "TNhMCmKd_6HW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checkout this [notebook](https://colab.research.google.com/drive/1hXIQ77A4TYS4y3UthWF-Ci7V7vVUoxmQ?usp=sharing) for more interesting visualizations of attention weights."
      ],
      "metadata": {
        "id": "6pxapqVnnEE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach 2\n",
        "\n",
        "Although, the above approach gives you more control in designing your attention mechanism, there is an alternate and more simpler way to do that.\n",
        "\n",
        "TensorFlow provides built-in layers for implementing attention mechanisms. Here's how you can implement attention using Keras APIs:\n",
        "\n",
        "```\n",
        "tf.keras.layers.Attention(use_scale=True, score_mode='dot')\n",
        "```\n",
        "\n",
        "\n",
        "**Arguments:**\n",
        "\n",
        "* **use_scale:** (Boolean, defaults to False) If True, will create a scalar variable to scale the attention scores. This can help stabilize the training process.\n",
        "* **causal:** (Boolean, defaults to False) If True, the attention mask will be a lower triangular matrix, preventing the model from attending to future positions in the sequence. This is often used in autoregressive models.\n",
        "* **dropout:** (Float, defaults to 0.0) The dropout rate to apply to the attention scores. Dropout can help prevent overfitting.\n",
        "\n",
        "**Call arguments:**\n",
        "\n",
        "* `inputs:` A list containing the following tensors:\n",
        "  * `query:` Tensor of shape (..., seq_len_q, depth) representing the query.\n",
        "  * `value:` Tensor of shape (..., seq_len_v, depth_v) representing the value.\n",
        "  * `key:` (Optional) Tensor of shape (..., seq_len_k, depth) representing the key. If not provided, the value tensor will be used as the key.\n",
        "* `mask:` (Optional) A list containing the following tensors:\n",
        "  * `query_mask`: A boolean mask of shape (..., seq_len_q) indicating which query positions are valid.\n",
        "  * `value_mask`: A boolean mask of shape (..., seq_len_v) indicating which value positions are valid.\n",
        "* `return_attention_scores:` (Boolean, defaults to False) If True, the layer will return the attention scores along with the output.\n",
        "\n",
        "**Outputs:**\n",
        "\n",
        "* **output:** Tensor of shape (..., seq_len_q, depth_v) representing the attention output.\n",
        "* **attention_scores:** (Optional) If return_attention_scores=True, the layer will also return a tensor of shape (..., seq_len_q, seq_len_v) representing the attention scores.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t1Vl97xkPpfX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Ze4Gd7BySJqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis using Attention Mechanisms\n",
        "\n",
        "\n",
        "We investigate the use of attention mechanism that allows a model to attend to different positions within the same input sequence, to capture the relationships between words and improve sentiment classification performance. We demonstrate the effectiveness of this approach on a benchmark sentiment analysis dataset and compare it to traditional methods. Our findings highlight the potential of attention mechanisms to enhance the accuracy and robustness of sentiment analysis models.\n",
        "\n",
        "\n",
        "## Dataset Description\n",
        "\n",
        "The [**Yelp Review Full dataset**](https://www.yelp.com/dataset) is a collection of Yelp reviews, specifically designed for text classification tasks, especially sentiment analysis.\n",
        "\n",
        "**Key characteristics:**\n",
        "\n",
        "* **Content:** Contains reviews from Yelp users about various businesses.\n",
        "Labels: Each review is associated with a star rating (1-5), used as the basis for sentiment labels (positive/negative). Reviews with ratings of 4 or 5 are labeled positive, while reviews with ratings less than 4 are negative.\n",
        "* **Size:** The full dataset is quite large. The provided code uses 30,000 samples for training and 6,000 for testing for demonstration purposes.\n",
        "* **Format:** The data is stored as text, and the code uses a tokenizer to convert words into numerical representations.\n",
        "\n"
      ],
      "metadata": {
        "id": "o7xzlsyvJ4xO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "nWb5J07cIaNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional, Input, Layer, Attention, GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n"
      ],
      "metadata": {
        "id": "wNKj_v7BF5hL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and preprocessing the dataset\n",
        "\n",
        "* We use `datasets` library to load the Yelp Review Full dataset.\n",
        "* We preprocesses the labels by assuming a positive sentiment for ratings 4 and above, and negative otherwise.\n",
        "* Then we split the dataset into training and testing sets using `train_test_split`.\n",
        "* Reviews are tokenized using `Tokenizer` and padded using `pad_sequences` to ensure uniform input length for the model."
      ],
      "metadata": {
        "id": "42E3eSEZMHZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Yelp Reviews dataset\n",
        "dataset = load_dataset(\"yelp_review_full\")\n",
        "\n",
        "# Loading just 30k records\n",
        "df = pd.DataFrame(dataset['train'][:30000])\n"
      ],
      "metadata": {
        "id": "eB-4_2VZNnBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess labels: Let's assume sentiment is positive for ratings >= 4, negative for < 4.\n",
        "df['sentiment'] = df['label'].apply(lambda x: 1 if x >= 4 else 0)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(df['text'], df['sentiment'], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "66smz80QG_gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the reviews\n",
        "max_features = 10000  # Max number of words to consider in the tokenizer\n",
        "max_len = 100  # Max length of sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "x_train = tokenizer.texts_to_sequences(x_train)\n",
        "x_test = tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "# Pad sequences to ensure uniform input length\n",
        "x_train = pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = pad_sequences(x_test, maxlen=max_len)"
      ],
      "metadata": {
        "id": "LPVMGs8iHQr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Building\n",
        "\n",
        "* We add an **embedding layer** to convert tokenized inputs into dense vectors.\n",
        "* Then, a **bidirectional LSTM** **(Long Short-Term Memory)** layer processes the embedded sequences.\n",
        "* The core of the model is the **Attention** layer from TensorFlow, applied to the outputs of the **LSTM** layer. This layer calculates attention weights to identify important parts of the review for sentiment classification.\n",
        "* A **global average pooling** layer aggregates the attention outputs into a single vector.\n",
        "* Finally, a dense layer with a **sigmoid** activation function predicts the sentiment (positive or negative)."
      ],
      "metadata": {
        "id": "gpyQDlF2Mjvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model using TensorFlow's built-in Attention layer\n",
        "input = Input(shape=(max_len,))\n",
        "embedding = Embedding(max_features, 128, input_length=max_len)(input)\n",
        "bi_lstm = Bidirectional(LSTM(64, return_sequences=True))(embedding)\n",
        "\n",
        "# Add Attention layer\n",
        "# Query, Key, and Value are the same in this case.\n",
        "attention_output = Attention()([bi_lstm, bi_lstm])\n",
        "\n",
        "# GlobalAveragePooling1D is used to aggregate the attention outputs into a single vector\n",
        "attention_output = GlobalAveragePooling1D()(attention_output)\n",
        "\n",
        "# Create the model with a single output\n",
        "output = Dense(1, activation='sigmoid')(attention_output)\n",
        "\n",
        "model = Model(inputs=input, outputs=output)"
      ],
      "metadata": {
        "id": "OyN3NklcFW7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "uWiZwJquH95Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, batch_size=128, epochs=5, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy:.2f}')"
      ],
      "metadata": {
        "id": "xzkUZ6wiHUpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evalutation and Inference"
      ],
      "metadata": {
        "id": "loPkWbg9OOl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(history)"
      ],
      "metadata": {
        "id": "OHgWS_rNHYuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict sentiment\n",
        "def predict_sentiment(model, text):\n",
        "    \"\"\"Predicts the sentiment of a given text using the trained model.\n",
        "\n",
        "    Args:\n",
        "        model: The trained sentiment analysis model.\n",
        "        text: The input text for sentiment prediction.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the predicted sentiment (\"Positive\" or \"Negative\")\n",
        "        and the confidence score (a value between 0 and 1).\n",
        "    \"\"\"\n",
        "    # Preprocess the input text\n",
        "    preprocessed_text = preprocess_input_text(text)\n",
        "    # Predict the sentiment using the trained model\n",
        "    prediction = model.predict(preprocessed_text)[0][0]\n",
        "    # Convert the prediction into a label\n",
        "    if prediction >= 0.5:\n",
        "        sentiment = \"Positive\"\n",
        "    else:\n",
        "        sentiment = \"Negative\"\n",
        "    return sentiment, prediction\n",
        "\n",
        "# Example usage:\n",
        "review = \"The food was not that great!\"\n",
        "sentiment, confidence = predict_sentiment(model, review)\n",
        "print(f'Review: \"{review}\"')\n",
        "print(f'Sentiment: {sentiment} (Confidence: {confidence:.2f})')\n"
      ],
      "metadata": {
        "id": "vshRyx2LFW4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improvement Strategies\n",
        "\n",
        "1. **Hyperparameter Tuning:**\n",
        "\n",
        "* **Experiment with different embedding dimensions:** Try increasing or decreasing the embedding dimension (embedding_dim) to see if it improves performance.\n",
        "* **Adjust LSTM units:** Explore different numbers of units in the LSTM layers.\n",
        "* **Optimize batch size and epochs:** Fine-tune the batch size and number of training epochs for better convergence.\n",
        "Try different optimizers: Consider using other optimizers like RMSprop or SGD.\n",
        "\n",
        "2. **Regularization:**\n",
        "\n",
        "* **Add dropout:** Introduce dropout layers to prevent overfitting. You can add them after the embedding layer or the LSTM layer.\n",
        "Apply L1/L2 regularization: Consider adding regularization to the dense layers.\n",
        "\n",
        "3. **Pre-trained Embeddings:**\n",
        "\n",
        "* **Use pre-trained word embeddings:** Instead of training embeddings from scratch, leverage pre-trained word embeddings like GloVe or Word2Vec. These embeddings capture semantic relationships between words and can improve performance.\n",
        "\n",
        "4. **Advanced Architectures:**\n",
        "\n",
        "* **Transformer-based models:** Explore using pre-trained transformer models like BERT or RoBERTa. These models have shown state-of-the-art results in various NLP tasks, including sentiment analysis.\n",
        "* **Hierarchical attention:** Implement hierarchical attention mechanisms to capture relationships at different levels of the text, such as word-level and sentence-level.\n",
        "\n",
        "5. **Data Augmentation:**\n",
        "\n",
        "* **Back translation:** Translate reviews to another language and then back to the original language to create synthetic data.\n",
        "* **Synonym replacement:** Replace words with their synonyms to increase data diversity.\n",
        "\n",
        "6. **Handling Class Imbalance:**\n",
        "\n",
        "* If the dataset has a significant class imbalance (e.g., more positive reviews than negative reviews), consider techniques like oversampling the minority class or using weighted loss functions.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZQ4sJZSkNStp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "03zg0EPxJcu6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
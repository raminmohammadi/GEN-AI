{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Vector Databases</h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief Recap of Vector Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Vector databases are specialized database systems designed to store, index, and query high-dimensional vector data efficiently.\n",
    "\n",
    "* They revolutionized similarity search and machine learning applications by enabling fast and accurate retrieval of similar items based on their vector representations.\n",
    "\n",
    "* Vector databases have several advantages over traditional database systems:\n",
    "  1. Similarity search: They can quickly find the most similar vectors to a given query vector.\n",
    "  2. High-dimensional data handling: They efficiently manage data with hundreds or thousands of dimensions.\n",
    "  3. Scalability: Vector databases can handle billions of vectors, making them suitable for large-scale applications.\n",
    "\n",
    "* Vector databases use various indexing techniques like Approximate Nearest Neighbor (ANN) algorithms to achieve fast search capabilities in high-dimensional spaces.\n",
    "\n",
    "* They have become foundational for many AI-powered applications, including recommendation systems, image and text search, anomaly detection, and natural language processing tasks.\n",
    "\n",
    "* Popular vector database systems include Faiss, Milvus, Pinecone, and Weaviate, each offering different features and optimizations.\n",
    "\n",
    "* These databases continue to evolve, finding new applications across industries such as e-commerce, content recommendation, fraud detection, and scientific research.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"static/img1.gif\" alt=\"Vector Databases Example\" style=\"width:50%;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture of Vector Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Vector Database architecture consists of two main components: the indexing system and the query processor. Here's an overview of the key elements:\n",
    "\n",
    "1. **Data Ingestion**: Converts input data into high-dimensional vector representations.\n",
    "\n",
    "2. **Vector Indexing**: Creates efficient data structures for fast similarity search in high-dimensional spaces.\n",
    "\n",
    "3. **Approximate Nearest Neighbor (ANN) Algorithms**: The core component of Vector Databases, allowing for rapid similarity search in high-dimensional spaces.\n",
    "\n",
    "4. **Distance Metrics**: Implement various distance measures (e.g., Euclidean, cosine similarity) for comparing vectors.\n",
    "\n",
    "5. **Dimensionality Reduction**: Techniques to reduce vector dimensions while preserving similarity relationships.\n",
    "\n",
    "6. **Query Processing**: Handles incoming queries and returns the most similar vectors based on the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Points:**\n",
    "\n",
    "* The indexing system organizes and structures the vector data, while the query processor handles search requests.\n",
    "\n",
    "* Many vector databases use distributed architectures to handle large-scale data and provide high availability and fault tolerance.\n",
    "\n",
    "* Advanced features often include CRUD operations, real-time updates, and support for hybrid searches combining vector and scalar data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"static/img2.gif\" alt=\"Working of Vector Databases\" style=\"width:50%;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of Vector Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector databases have found wide-ranging applications across various domains:\n",
    "\n",
    "1. **Information Retrieval:**\n",
    "   - Semantic search\n",
    "   - Document similarity\n",
    "   - Content recommendation\n",
    "   - Plagiarism detection\n",
    "\n",
    "2. **Computer Vision:**\n",
    "   - Image similarity search\n",
    "   - Face recognition\n",
    "   - Visual product search\n",
    "   - Image deduplication\n",
    "\n",
    "3. **Natural Language Processing:**\n",
    "   - Text classification\n",
    "   - Question answering systems\n",
    "   - Language translation\n",
    "   - Chatbots and conversational AI\n",
    "\n",
    "4. **Recommender Systems:**\n",
    "   - Personalized product recommendations\n",
    "   - Content-based filtering\n",
    "   - Collaborative filtering at scale\n",
    "\n",
    "5. **Anomaly Detection:** Identifying unusual patterns in data across various industries.\n",
    "\n",
    "6. **Bioinformatics:** Analyzing genetic sequences and protein structures.\n",
    "\n",
    "7. **Financial Services:**\n",
    "   - Fraud detection\n",
    "   - Risk assessment\n",
    "   - Market trend analysis\n",
    "\n",
    "8. **Audio Processing:**\n",
    "   - Music recommendation\n",
    "   - Speaker identification\n",
    "   - Audio fingerprinting\n",
    "\n",
    "9. **Cybersecurity:**\n",
    "   - Malware detection\n",
    "   - Network intrusion detection\n",
    "   - Phishing URL detection\n",
    "\n",
    "10. **E-commerce:**\n",
    "    - Visual search\n",
    "    - Product matching\n",
    "    - Inventory management\n",
    "\n",
    "11. **Geospatial Analysis:** Processing and querying location-based data efficiently.\n",
    "\n",
    "12. **Healthcare:**\n",
    "    - Medical image analysis\n",
    "    - Drug discovery\n",
    "    - Patient similarity for personalized medicine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing some core concepts of building a Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data ingestion and preprocessing are crucial steps in building a vector database. \n",
    "- This process involves collecting raw data from various sources and preparing it for vectorization. \n",
    "- It's important because clean, well-structured data leads to more accurate vector representations and better search results.\n",
    "\n",
    "**Inputs** \n",
    "- Raw data from diverse sources (e.g., text documents, images, audio files)\n",
    "\n",
    "**Process** \n",
    "- Clean, normalize, and format the data\n",
    "\n",
    "**Outputs** \n",
    "- Preprocessed data ready for vectorization\n",
    "\n",
    "Here's a simple example of text preprocessing using Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/shashwatshahi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shashwatshahi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/shashwatshahi/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed text: example sentence numbers punctuation\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "raw_text = \"This is an example sentence with some numbers (123) and punctuation!\"\n",
    "preprocessed_text = preprocess_text(raw_text)\n",
    "print(f\"Preprocessed text: {preprocessed_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Embedding Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vector embedding generation is a crucial first step in working with vector databases. \n",
    "- It involves converting preprocessed data into numerical vectors that represent the semantic meaning or features of the data. \n",
    "- This process is essential because it allows us to quantify and compare the similarity between different pieces of information in a high-dimensional space.\n",
    "\n",
    "**Inputs** \n",
    "- Preprocessed data (e.g., cleaned text, normalized images)\n",
    "\n",
    "**Process** \n",
    "- Use pre-trained models or custom algorithms to generate vector representations\n",
    "\n",
    "**Outputs** \n",
    "- Numerical vectors (embeddings)\n",
    "\n",
    "Here's a short code snippet demonstrating vector embedding generation for text using the sentence-transformers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shashwatshahi/Desktop/GEN-AI/Labs/Vector Databases/env/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (384,)\n",
      "First few values: [-0.02477177 -0.03728328 -0.06883422 -0.07348053 -0.01428398]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Sample preprocessed text\n",
    "text = \"vector databases powerful tools similarity search\"\n",
    "\n",
    "# Generate embedding\n",
    "embedding = model.encode(text)\n",
    "\n",
    "print(f\"Embedding shape: {embedding.shape}\")\n",
    "print(f\"First few values: {embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vector indexing is the process of organizing vector embeddings in a way that allows for efficient similarity search. \n",
    "- It's crucial for scaling vector databases to handle large amounts of data while maintaining fast query times.\n",
    "- Indexing structures like hierarchical navigable small world (HNSW) or inverted file (IVF) are commonly used to achieve this.\n",
    "\n",
    "**Inputs** \n",
    "- Vector embeddings\n",
    "\n",
    "**Process** \n",
    "- Build an index structure to organize vectors for efficient retrieval\n",
    "\n",
    "**Outputs** \n",
    "- Indexed vector database\n",
    "\n",
    "Here's a simple example using the FAISS library to create an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vectors indexed: 10000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Sample vector data\n",
    "dimension = 128\n",
    "num_vectors = 10000\n",
    "vectors = np.random.random((num_vectors, dimension)).astype('float32')\n",
    "\n",
    "# Create an index\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add vectors to the index\n",
    "index.add(vectors)\n",
    "\n",
    "print(f\"Total vectors indexed: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Metadata management is an essential aspect of building a vector database that is often overlooked. \n",
    "- It involves storing and organizing additional information about the vectors, such as their source, creation date, or associated labels.\n",
    "- Proper metadata management enhances the usability and interpretability of the vector database.\n",
    "\n",
    "**Inputs**\n",
    "- Vector embeddings, associated metadata\n",
    "\n",
    "**Process** \n",
    "- Store metadata alongside vectors, create efficient retrieval mechanisms\n",
    "\n",
    "**Outputs**\n",
    "- Indexed vectors with linked metadata\n",
    "\n",
    "Here's a conceptual example of how to manage metadata using a simple dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved vector: [0.85523206 0.81125873 0.02158552 0.42390928 0.8991076 ]\n",
      "Retrieved metadata: {'source': 'document1.txt', 'category': 'finance'}\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "class VectorWithMetadata:\n",
    "    def __init__(self, vector, metadata):\n",
    "        self.id = str(uuid.uuid4())\n",
    "        self.vector = vector\n",
    "        self.metadata = metadata\n",
    "\n",
    "# Create a dictionary to store vectors with metadata\n",
    "vector_store = {}\n",
    "\n",
    "# Add vectors with metadata\n",
    "vector1 = np.random.random(dimension).astype('float32')\n",
    "metadata1 = {\"source\": \"document1.txt\", \"category\": \"finance\"}\n",
    "vector_with_metadata1 = VectorWithMetadata(vector1, metadata1)\n",
    "vector_store[vector_with_metadata1.id] = vector_with_metadata1\n",
    "\n",
    "# Retrieve vector and metadata\n",
    "retrieved_vector = vector_store[vector_with_metadata1.id]\n",
    "print(f\"Retrieved vector: {retrieved_vector.vector[:5]}\")\n",
    "print(f\"Retrieved metadata: {retrieved_vector.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Search Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Similarity search is the core operation in vector databases. \n",
    "- It involves finding the most similar vectors to a given query vector based on a distance metric (e.g., Euclidean distance, cosine similarity). \n",
    "- This operation is fundamental for various applications such as recommendation systems, image retrieval, and semantic text search.\n",
    "\n",
    "**Inputs** \n",
    "- Query vector, indexed vector database\n",
    "- Process: Search the index for the nearest neighbors of the query vector\n",
    "- Outputs: List of similar vectors and their distances/similarities\n",
    "\n",
    "Here's an example of performing a similarity search using the previously created FAISS index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of 5 nearest neighbors: [5086 2470 1156 3986 5690]\n",
      "Distances to 5 nearest neighbors: [14.633185  14.765137  15.56501   15.658556  15.7717905]\n"
     ]
    }
   ],
   "source": [
    "# Perform a similarity search\n",
    "k = 5  # Number of nearest neighbors to retrieve\n",
    "query_vector = np.random.random((1, dimension)).astype('float32')\n",
    "\n",
    "distances, indices = index.search(query_vector, k)\n",
    "\n",
    "print(f\"Indices of {k} nearest neighbors: {indices[0]}\")\n",
    "print(f\"Distances to {k} nearest neighbors: {distances[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Build a Real world project to understand the concept of Vector Databases better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Search Engine using Vector Database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to build a semantic search engine using a vector database to enable efficient and meaningful search across a large corpus of text documents. This project will demonstrate the effectiveness of vector databases in capturing semantic relationships between documents and enabling fast, relevant search results based on the meaning of queries rather than just keyword matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Wikipedia Articles dataset consists of 100,000 Wikipedia articles, split into 90,000 for indexing and 10,000 for testing queries.\n",
    "- Each article contains the title, full text content, and associated categories.\n",
    "- The dataset provides a diverse range of topics and writing styles, ideal for testing semantic search capabilities.\n",
    "- Key features of the dataset:\n",
    "  - 100,000 Wikipedia articles (90,000 for indexing, 10,000 for testing)\n",
    "  - Rich text data including titles, full content, and categories\n",
    "  - Wide variety of topics covering general knowledge\n",
    "  - Varied article lengths and complexities\n",
    "- For more information about the Wikipedia Articles dataset, you can visit the following link: [Wikipedia Articles Dataset](https://huggingface.co/datasets/wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing the Wikipedia Dataset\n",
    "\n",
    "We use the Hugging Face datasets library to load the Wikipedia dataset. The with_info=True parameter returns dataset info along with the dataset itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Wikipedia dataset\n",
    "dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train[:100000]\")\n",
    "\n",
    "# Convert to pandas DataFrame for easier handling\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Preprocess the text (combine title and text)\n",
    "df['combined_text'] = df['title'] + \" \" + df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Vector Embeddings\n",
    "\n",
    "We use the SentenceTransformer model to generate vector embeddings for each article's combined text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3125/3125 [09:10<00:00,  5.68it/s]\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(df['combined_text'].tolist(), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Vector Database\n",
    "\n",
    "We use FAISS to create an efficient index for our vector embeddings, enabling fast similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vectors indexed: 100000\n"
     ]
    }
   ],
   "source": [
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add vectors to the index\n",
    "index.add(embeddings.astype('float32'))\n",
    "\n",
    "print(f\"Total vectors indexed: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Semantic Search\n",
    "\n",
    "Now we can perform semantic searches using our vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, top_k=5):\n",
    "    # Encode the query\n",
    "    query_vector = model.encode([query])[0].astype('float32').reshape(1, -1)\n",
    "    \n",
    "    # Perform the search\n",
    "    distances, indices = index.search(query_vector, top_k)\n",
    "    \n",
    "    # Return results\n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        results.append({\n",
    "            'title': df.iloc[idx]['title'],\n",
    "            'text': df.iloc[idx]['text'][:200] + \"...\",  # Preview of text\n",
    "            'distance': distances[0][i]\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Quantum computing\n",
      "Preview: Quantum computing is a type of computation that harnesses the collective properties of quantum states, such as superposition, interference, and entanglement, to perform calculations. The devices that ...\n",
      "Distance: 0.7054274082183838\n",
      "---\n",
      "Title: Applications of quantum mechanics\n",
      "Preview: Quantum physics is a branch of modern physics in which energy and matter are described at their most fundamental level, that of energy quanta, elementary particles, and quantum fields. Quantum physics...\n",
      "Distance: 0.9135433435440063\n",
      "---\n",
      "Title: Quantum key distribution\n",
      "Preview: Quantum key distribution (QKD) is a secure communication method which implements a cryptographic protocol involving components of quantum mechanics. It enables two parties to produce a shared random s...\n",
      "Distance: 0.9649192094802856\n",
      "---\n",
      "Title: Shor's algorithm\n",
      "Preview: Shor's algorithm is a quantum computer algorithm for finding the prime factors of an integer. It was discovered in 1994 by the American mathematician Peter Shor.\n",
      "\n",
      "On a quantum computer, to factor an i...\n",
      "Distance: 0.9820106029510498\n",
      "---\n",
      "Title: Grover's algorithm\n",
      "Preview: In quantum computing, Grover's algorithm, also known as the quantum search algorithm, refers to a quantum algorithm for unstructured search that finds with high probability the unique input to a black...\n",
      "Distance: 0.9916479587554932\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Example search\n",
    "search_results = semantic_search(\"quantum computing applications\")\n",
    "for result in search_results:\n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"Preview: {result['text']}\")\n",
    "    print(f\"Distance: {result['distance']}\")\n",
    "    print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

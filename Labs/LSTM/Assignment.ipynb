{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Assignment (Graded): Sentiment Analysis on IMDB Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to your programming assignment on LSTM! You will build a Deep Learning Model with LSTMs to to perform sentiment analysis on movie reviews from the IMDB dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this project, you will develop a deep learning model to perform sentiment analysis on movie reviews from the IMDB dataset.\n",
    "\n",
    "- The goal is to create a model that can accurately classify movie reviews as either positive or negative based on the text content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Only write code when you see any of the below prompts,\n",
    "\n",
    "    ```\n",
    "    # YOUR CODE GOES HERE\n",
    "    # YOUR CODE ENDS HERE\n",
    "    # TODO\n",
    "    ```\n",
    "\n",
    "- Do not modify any other section of the code unless tated otherwise in the comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The IMDB dataset consists of 50,000 movie reviews, split evenly into 25,000 training and 25,000 testing samples. \n",
    "\n",
    "- Each review is labeled as either positive (1) or negative (0). \n",
    "\n",
    "- The dataset has been preprocessed to contain only the most frequent 10,000 words.\n",
    "\n",
    "- Key features of the dataset:\n",
    "    - 50,000 movie reviews (25,000 for training, 25,000 for testing)\n",
    "    - Binary sentiment classification (positive or negative)\n",
    "    - Preprocessed to include only the top 10,000 most frequent words\n",
    "    - Variable length reviews\n",
    "\n",
    "- For more information about the dataset, refer to this link: [IMDB TF Documentation](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb) || [IMDB Stanford Dataset](https://ai.stanford.edu/%7Eamaas/data/sentiment/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Data Preprocessing**\n",
    "   - Explore the dataset structure and characteristics\n",
    "   - Pad the sequences to a fixed length for model input\n",
    "\n",
    "2. **Model Architecture**\n",
    "   - Design a deep learning model for sentiment classification\n",
    "   - Include appropriate layers for text processing (e.g., Embedding layer)\n",
    "\n",
    "3. **Model Training**\n",
    "   - Split the data into training and validation sets\n",
    "   - Implement callbacks for early stopping and learning rate reduction\n",
    "   - Train the model and monitor its performance\n",
    "\n",
    "4. **Evaluation and Analysis**\n",
    "   - Evaluate the model on the test set\n",
    "   - Plot training and validation accuracy/loss curves\n",
    "\n",
    "5. **Prediction and Interpretation**\n",
    "   - Use the trained model to make predictions on new, unseen reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from helpers.methods import load_imdb_data, detect_and_set_device, plot_history\n",
    "from tests.test_methods import test_pad_sequences_data, test_create_model, test_train_model, test_evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "num_words = 10000\n",
    "(x_train, y_train), (x_test, y_test) = load_imdb_data(num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Let's get to know about our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape of the dataset: Testing and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Shape of the data\n",
    "\n",
    "x_train_shape = \n",
    "x_test_shape = \n",
    "y_train_shape = \n",
    "y_test_shape = \n",
    "\n",
    "\n",
    "print(f\"Training Data Shape: {x_train_shape}\")\n",
    "print(f\"Training Labels Shape: {y_train_shape}\")\n",
    "print(f\"Test Data Shape: {x_test_shape}\")\n",
    "print(f\"Test Labels Shape: {y_test_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Distribution of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_distribution(labels, dataset_type):\n",
    "    \"\"\"Plot the distribution of positive and negative reviews.\"\"\"\n",
    "    # YOUR CODE GOES HERE\n",
    "    unique, counts = \n",
    "    # YOUR CODE ENDS HERE\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.barplot(x=unique, y=counts)\n",
    "    plt.title(f'Class Distribution in {dataset_type} Data')\n",
    "    plt.xticks([0, 1], ['Negative (0)', 'Positive (1)'])\n",
    "    plt.ylabel('Number of Reviews')\n",
    "    plt.show()\n",
    "\n",
    "# Plot class distribution for training and test sets\n",
    "plot_class_distribution(y_train, \"Training\")\n",
    "plot_class_distribution(y_test, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Hints:**\n",
    "\n",
    "Complete the pad_sequences_data method for preprocessing the data.\n",
    "\n",
    "- Use the pad_sequences function from TensorFlow/Keras to pad the sequences in both the training data (x_train) and test data (x_test) to ensure consistent input length across all samples.\n",
    "\n",
    "- Set the maximum length for the sequences to 200 using the maxlen parameter in pad_sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences_data(x_train, x_test):\n",
    "    \"\"\"Pad sequences to ensure consistent input length.\"\"\"\n",
    "    # YOUR CODE GOES HERE\n",
    "\n",
    "    x_train_padded = \n",
    "    x_test_padded = \n",
    "\n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    \n",
    "    test_pad_sequences_data(x_train_padded, x_test_padded)\n",
    "    return x_train_padded, x_test_padded\n",
    "\n",
    "# x_train_padded, x_test_padded = pad_sequences_data(x_train, x_test)\n",
    "# print(x_train_padded[:, -1])\n",
    "# print(x_test_padded[:, -1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Hints:**\n",
    "\n",
    "Complete the create_model function\n",
    "\n",
    "- Create a sequential model using `tf.keras.Sequential()`, starting with an embedding layer that accepts a vocabulary size (`vocab_size`) and a specified input length (`max_len`).\n",
    "\n",
    "- Add a bidirectional LSTM layer with 32 units and set `return_sequences=True` to ensure that the LSTM outputs sequences of the same length as its input.\n",
    "\n",
    "- Use a `GlobalAveragePooling1D` layer to downsample the output by taking the average across all time steps.\n",
    "\n",
    "- Add a `Dense` layer with 32 units, ReLU activation (`relu`), and apply L2 regularization (`kernel_regularizer=tf.keras.optimizers.l2(0.03)`).\n",
    "\n",
    "- Include a `Dropout` layer with a dropout rate of 0.4 to prevent overfitting.\n",
    "\n",
    "- Add a final `Dense` layer with 1 unit and sigmoid activation (`sigmoid`) to output a probability for binary classification, with the same L2 regularization as the previous dense layer.\n",
    "\n",
    "- Compile the model using the Adam optimizer with a learning rate of 0.001 and a `clipnorm` of 1.0 to stabilize the training. Set the loss function to `'binary_crossentropy'` and include accuracy as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # TODO: Define the vocabulary size and maximum sequence length\n",
    "    vocab_size = \n",
    "    max_len = \n",
    "    model = tf.keras.Sequential([\n",
    "        # YOUR CODE GOES HERE\n",
    "\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "    ])\n",
    "    \n",
    "    # TODO: Compile the model and define the optimizer, loss function, and metrics\n",
    "    \n",
    "\n",
    "    \n",
    "    test_create_model(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Hints:**\n",
    "\n",
    "Implement the `train_model` function, which will take a compiled model along with training data (`x_train`, `y_train`) and test data (`x_test`, `y_test`) for training the model.\n",
    "\n",
    "- Set the number of `epochs` to 30 and the `batch_size` to 256.\n",
    "\n",
    "- Use the `EarlyStopping` callback to stop training early if validation accuracy (`val_accuracy`) does not improve for 5 consecutive epochs, while restoring the best weights.\n",
    "\n",
    "- Use the `ReduceLROnPlateau` callback to reduce the learning rate when the validation loss (`val_loss`) plateaus, with a reduction factor of `0.2` and a patience of 3 epochs. Set the minimum learning rate (`min_lr`) to `1e-6`.\n",
    "\n",
    "- Prepare the training dataset by converting the input data into a TensorFlow `Dataset`. Shuffle the data with a buffer size of 10,000, batch it with a specified batch size, and prefetch data to optimize the input pipeline.\n",
    "\n",
    "- Train the model using the `fit` method, specifying the training dataset and validation data. Include the `early_stopping` and `lr_reducer` callbacks, and set `verbose=1` to print training progress.\n",
    "\n",
    "- Return the `history` object containing metrics and loss values during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train, x_test, y_test, runtime_device):\n",
    "    # TODO: Define the number of epochs and batch size, early stopping, and learning rate reducer\n",
    "    epochs=\n",
    "    batch_size=2\n",
    "    early_stopping = \n",
    "    lr_reducer = \n",
    "    \n",
    "    # TODO: Define the training dataset and batch it\n",
    "\n",
    "\n",
    "    with tf.device('/' + runtime_device + ':0'):\n",
    "        # YOUR CODE GOES HERE\n",
    "\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "        )\n",
    "    test_train_model(history)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, x_test, y_test):\n",
    "    # YOUR CODE GOES HERE\n",
    "\n",
    "\n",
    "    # YOUR CODE ENDS HERE\n",
    "    test_evaluate_model(loss, accuracy)\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Driver code to run the built pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------- Do not change the code below ----------------#\n",
    "def main(x_train, y_train, x_test, y_test):\n",
    "    # Set device to 'cpu' or 'gpu'\n",
    "    device = detect_and_set_device()\n",
    "\n",
    "    # Pad data\n",
    "    x_train, x_test = pad_sequences_data(x_train, x_test)\n",
    "\n",
    "    # Create and train model\n",
    "    model = create_model()\n",
    "    history = train_model(model, x_train, y_train, x_test, y_test, device)\n",
    "    \n",
    "    # Evaluate model\n",
    "    loss, accuracy = evaluate_model(model, x_test, y_test)\n",
    "    \n",
    "    \n",
    "    return loss, accuracy, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------- Do not change the code below ----------------#\n",
    "if __name__ == \"__main__\":\n",
    "    loss, accuracy, history = main(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checl the loss and accuracy\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------- Do not change the code below ----------------#\n",
    "# Run this cell to plot the training history\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

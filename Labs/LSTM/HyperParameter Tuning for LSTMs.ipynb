{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Hyperparameter Tuning for Long Short-Term Memory Networks (LSTMs)</h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hyperparameter tuning is a crucial aspect of developing effective LSTM models. \n",
    "- While the model learns its internal parameters during training, hyperparameters are the configuration settings we must specify beforehand. \n",
    "- These settings significantly impact the model's learning process and final performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Hyperparameter Tuning Matters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding hyperparameter tuning is essential because:\n",
    "\n",
    "1. Different problems require different hyperparameter configurations\n",
    "2. The wrong hyperparameters can lead to poor model performance or failed training\n",
    "3. Optimal hyperparameters can significantly improve model accuracy and training efficiency\n",
    "4. Proper tuning helps prevent overfitting and underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"static/image2.gif\" alt=\"Hyperparameter Tuning\" style=\"width:50%;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key LSTM Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of LSTM Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Controls the model's depth and capacity to learn complex patterns\n",
    "- More layers can capture more abstract features but increase computational cost\n",
    "- Typical range: 1-3 layers for most applications\n",
    "- Consider starting with one layer and adding more if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Hidden Units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Determines the dimensionality of the LSTM layer's output\n",
    "- Affects the model's capacity to learn patterns\n",
    "- Larger numbers can capture more complex relationships but require more data\n",
    "- Common ranges: 32, 64, 128, 256, 512\n",
    "- Rule of thumb: Start with the square root of your input dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Helps prevent overfitting by randomly deactivating neurons during training\n",
    "- Applied to both input and recurrent connections\n",
    "- Typical range: 0.1 to 0.5\n",
    "- Start with 0.2 and adjust based on validation performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Number of samples processed before model update\n",
    "- Affects training stability and speed\n",
    "- Smaller batches: More frequent updates, noisier gradients\n",
    "- Larger batches: More stable updates, better gradient estimates\n",
    "- Common values: 16, 32, 64, 128, 256\n",
    "- Memory constraints often limit maximum batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Controls how much to adjust the model in response to errors\n",
    "- Critical for model convergence\n",
    "- Too high: Unstable training\n",
    "- Too low: Slow convergence\n",
    "- Typical range: 0.1 to 0.0001\n",
    "- Common starting point: 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Complete passes through the training dataset\n",
    "- Too few: Underfitting\n",
    "- Too many: Overfitting\n",
    "- Use early stopping to determine optimal number\n",
    "- Start with 50-100 epochs with early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a grid search for LSTM hyperparameters using the keras-tuner library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Tune number of LSTM layers\n",
    "    for i in range(hp.Int('num_layers', 1, 3)):\n",
    "        model.add(keras.layers.LSTM(\n",
    "            units=hp.Int(f'units_{i}', 32, 512, step=32),\n",
    "            return_sequences=i != hp.Int('num_layers', 1, 3) - 1,\n",
    "            dropout=hp.Float(f'dropout_{i}', 0.1, 0.5, step=0.1)\n",
    "        ))\n",
    "    \n",
    "    model.add(keras.layers.Dense(1))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')\n",
    "        ),\n",
    "        loss='mean_squared_error'\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the tuner\n",
    "tuner = kt.GridSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random search can be more efficient than grid search for high-dimensional spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random search tuner\n",
    "random_tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian optimization uses past trials to inform future hyperparameter choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bayesian optimization tuner\n",
    "bayesian_tuner = kt.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Example: Stock Price Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply hyperparameter tuning to our stock price prediction model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTuner:\n",
    "    def __init__(self, sequence_length=60):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM tuner with configuration parameters.\n",
    "        \n",
    "        Args:\n",
    "            sequence_length (int): Number of time steps to look back\n",
    "        \"\"\"\n",
    "        self.sequence_length = sequence_length\n",
    "        self.scaler = None\n",
    "    \n",
    "    def prepare_stock_data(self, symbol='AAPL', start_date='2020-01-01', end_date='2023-12-31'):\n",
    "        \"\"\"\n",
    "        Fetch and prepare stock data for LSTM training.\n",
    "        Returns scaled data and stores the scaler object for inverse transformation.\n",
    "        \"\"\"\n",
    "        # Download stock data\n",
    "        print(f\"Downloading {symbol} stock data...\")\n",
    "        stock_data = yf.download(symbol, start=start_date, end=end_date)\n",
    "        \n",
    "        # Extract closing prices and convert to numpy array\n",
    "        prices = stock_data['Close'].values.reshape(-1, 1)\n",
    "        \n",
    "        # Scale the data\n",
    "        self.scaler = MinMaxScaler()\n",
    "        scaled_prices = self.scaler.fit_transform(prices)\n",
    "        \n",
    "        return scaled_prices\n",
    "\n",
    "    def create_sequences(self, data):\n",
    "        \"\"\"\n",
    "        Create sequences for LSTM training.\n",
    "        Each sequence uses sequence_length days to predict the next day's price.\n",
    "        \"\"\"\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - self.sequence_length):\n",
    "            X.append(data[i:i + self.sequence_length])\n",
    "            y.append(data[i + self.sequence_length])\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    def create_lstm_model(self, hp):\n",
    "        \"\"\"\n",
    "        Create an LSTM model with tunable hyperparameters.\n",
    "        This function will be used by the hyperparameter tuners.\n",
    "        \"\"\"\n",
    "        model = keras.Sequential()\n",
    "        \n",
    "        # First LSTM layer\n",
    "        model.add(keras.layers.LSTM(\n",
    "            units=hp.Int('lstm_units_1', 32, 256, step=32),\n",
    "            return_sequences=True,\n",
    "            dropout=hp.Float('dropout_1', 0.1, 0.5, step=0.1),\n",
    "            input_shape=(self.sequence_length, 1)\n",
    "        ))\n",
    "        \n",
    "        # Second LSTM layer\n",
    "        model.add(keras.layers.LSTM(\n",
    "            units=hp.Int('lstm_units_2', 32, 256, step=32),\n",
    "            dropout=hp.Float('dropout_2', 0.1, 0.5, step=0.1)\n",
    "        ))\n",
    "        \n",
    "        # Output layer\n",
    "        model.add(keras.layers.Dense(1))\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(\n",
    "                hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')\n",
    "            ),\n",
    "            loss='mean_squared_error'\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def run_hyperparameter_tuning(self):\n",
    "        \"\"\"\n",
    "        Run different hyperparameter tuning methods and compare results.\n",
    "        Returns a dictionary containing the results for each method.\n",
    "        \"\"\"\n",
    "        # Prepare data\n",
    "        scaled_prices = self.prepare_stock_data()\n",
    "        X, y = self.create_sequences(scaled_prices)\n",
    "        \n",
    "        # Split data into train and validation sets\n",
    "        train_size = int(len(X) * 0.8)\n",
    "        X_train, X_val = X[:train_size], X[train_size:]\n",
    "        y_train, y_val = y[:train_size], y[train_size:]\n",
    "        \n",
    "        print(f\"Training data shape: {X_train.shape}\")\n",
    "        print(f\"Validation data shape: {X_val.shape}\")\n",
    "        \n",
    "        # Create directory for saving results\n",
    "        results_dir = f\"hyperparameter_tuning_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        # Initialize tuners\n",
    "        tuners = {\n",
    "            'grid': kt.GridSearch(\n",
    "                self.create_lstm_model,\n",
    "                objective='val_loss',\n",
    "                max_trials=10,\n",
    "                directory=results_dir,\n",
    "                project_name='grid_search'\n",
    "            ),\n",
    "            'random': kt.RandomSearch(\n",
    "                self.create_lstm_model,\n",
    "                objective='val_loss',\n",
    "                max_trials=10,\n",
    "                directory=results_dir,\n",
    "                project_name='random_search'\n",
    "            ),\n",
    "            'bayesian': kt.BayesianOptimization(\n",
    "                self.create_lstm_model,\n",
    "                objective='val_loss',\n",
    "                max_trials=10,\n",
    "                directory=results_dir,\n",
    "                project_name='bayesian_opt'\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Dictionary to store results\n",
    "        results = {}\n",
    "        \n",
    "        # Run each tuner\n",
    "        for tuner_name, tuner in tuners.items():\n",
    "            print(f\"\\nStarting {tuner_name} search...\")\n",
    "            \n",
    "            # Search for best hyperparameters\n",
    "            tuner.search(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                validation_data=(X_val, y_val),\n",
    "                callbacks=[\n",
    "                    keras.callbacks.EarlyStopping(\n",
    "                        monitor='val_loss',\n",
    "                        patience=5,\n",
    "                        restore_best_weights=True\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # Get best hyperparameters and model\n",
    "            best_hps = tuner.get_best_hyperparameters(1)[0]\n",
    "            best_model = tuner.hypermodel.build(best_hps)\n",
    "            \n",
    "            # Train the model with best hyperparameters\n",
    "            history = best_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                validation_data=(X_val, y_val),\n",
    "                callbacks=[\n",
    "                    keras.callbacks.EarlyStopping(\n",
    "                        monitor='val_loss',\n",
    "                        patience=5,\n",
    "                        restore_best_weights=True\n",
    "                    )\n",
    "                ],\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            results[tuner_name] = {\n",
    "                'best_hps': best_hps,\n",
    "                'history': history.history,\n",
    "                'best_model': best_model\n",
    "            }\n",
    "            \n",
    "            # Print best hyperparameters\n",
    "            print(f\"\\nBest hyperparameters for {tuner_name}:\")\n",
    "            for param, value in best_hps.values.items():\n",
    "                print(f\"{param}: {value}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def visualize_results(self, results):\n",
    "        \"\"\"\n",
    "        Create visualizations comparing the performance of different tuning methods.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Plot training histories\n",
    "        plt.subplot(2, 1, 1)\n",
    "        for method, result in results.items():\n",
    "            plt.plot(result['history']['val_loss'], label=f'{method} validation loss')\n",
    "        plt.title('Validation Loss Comparison')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot final validation losses\n",
    "        plt.subplot(2, 1, 2)\n",
    "        final_losses = {method: min(result['history']['val_loss']) \n",
    "                       for method, result in results.items()}\n",
    "        plt.bar(final_losses.keys(), final_losses.values())\n",
    "        plt.title('Best Validation Loss by Method')\n",
    "        plt.ylabel('Loss')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create tuner instance\n",
    "tuner = LSTMTuner(sequence_length=60)\n",
    "\n",
    "# Run hyperparameter tuning\n",
    "print(\"Starting hyperparameter tuning...\")\n",
    "results = tuner.run_hyperparameter_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "print(\"\\nGenerating visualizations...\")\n",
    "tuner.visualize_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final summary\n",
    "print(\"\\nFinal Results Summary:\")\n",
    "for method, result in results.items():\n",
    "    best_val_loss = min(result['history']['val_loss'])\n",
    "    print(f\"{method} - Best validation loss: {best_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Start with a Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Begin with default hyperparameters\n",
    "- Establish baseline performance\n",
    "- Identify areas for improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Systematic Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Change one hyperparameter at a time\n",
    "- Document impact of each change\n",
    "- Keep track of all experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ensures robust performance estimates\n",
    "- Helps prevent overfitting to validation set\n",
    "- Particularly important for time series data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Monitor Multiple Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training loss\n",
    "- Validation loss\n",
    "- Domain-specific metrics\n",
    "- Training time and resource usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Pitfalls and Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Symptom**: Low training loss, high validation loss\n",
    "- **Solutions**:\n",
    "  - Increase dropout rate\n",
    "  - Reduce model capacity\n",
    "  - Add regularization\n",
    "  - Collect more training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Symptom**: High training and validation loss\n",
    "- **Solutions**:\n",
    "  - Increase model capacity\n",
    "  - Decrease dropout rate\n",
    "  - Train for more epochs\n",
    "  - Adjust learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unstable Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Symptom**: Fluctuating loss values\n",
    "- **Solutions**:\n",
    "  - Reduce learning rate\n",
    "  - Increase batch size\n",
    "  - Add gradient clipping\n",
    "  - Normalize inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effective hyperparameter tuning is crucial for developing high-performing LSTM models. The process requires:\n",
    "- Understanding the role of each hyperparameter\n",
    "- Systematic experimentation\n",
    "- Careful monitoring of results\n",
    "- Patience and persistence\n",
    "\n",
    "Remember that there's often no single \"best\" set of hyperparameters - the optimal configuration depends on your specific problem, data, and requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

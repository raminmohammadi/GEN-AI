{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzIw3G0q2su9"
      },
      "source": [
        "<center>\n",
        "    <h1>Transformers</h1>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA40snKL5a9g"
      },
      "source": [
        "# Brief Recap of Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm_wqFn66gJd"
      },
      "source": [
        "*   Transformers are a type of neural network architecture introduced in 2017 by Vaswani et al. in the paper \"Attention Is All You Need\".\n",
        "\n",
        "*   They revolutionized natural language processing (NLP) tasks by introducing the self-attention mechanism, which allows the model to weigh the importance of different parts of the input sequence when processing each element\n",
        "\n",
        "*   Transformers have several advantages over previous architectures:\n",
        "    1. Parallelization: They can process entire sequences simultaneously, unlike recurrent neural networks (RNNs).\n",
        "    2. Long-range dependencies: They can capture relationships between distant elements in a sequence more effectively.\n",
        "    3. Scalability: Transformers can be trained on larger datasets and have led to the development of large language models.\n",
        "\n",
        "*   Transformers have become the foundation for many state-of-the-art AI models, including GPT (Generative Pre-trained Transformer) series, BERT (Bidirectional Encoder Representations from Transformers), and their variants.\n",
        "\n",
        "*   These models have significantly advanced the field of AI and continue to find new applications across various industries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<center>\n",
        "    <img src=\"static/img1.gif\" alt=\"Transformers Example\" style=\"width:50%;\">\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QsiFwHs6sFO"
      },
      "source": [
        "## Architecture of Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MW_8yObk60D9"
      },
      "source": [
        "*   The Transformer architecture consists of two main components: the encoder and the decoder. Here's an overview of the key elements:\n",
        "\n",
        "    1. **Input Embedding**: Converts input tokens into continuous vector representations.\n",
        "\n",
        "    2. **Positional Encoding**: Adds information about the position of each token in the sequence.\n",
        "\n",
        "    3. **Multi-Head Attention**: The core component of Transformers, allowing the model to attend to different parts of the input sequence simultaneously.\n",
        "\n",
        "    4. **Feed-Forward Networks**: Process the output of the attention layers.\n",
        "\n",
        "    5. **Layer Normalization and Residual Connections**: Help stabilize training and allow for deeper networks.\n",
        "\n",
        "    6. **Output Layer**: Produces the final output, often a probability distribution over possible tokens.\n",
        "\n",
        "*   The encoder processes the input sequence, while the decoder generates the output sequence.\n",
        "\n",
        "*   The attention mechanism in the decoder also attends to the encoder's output, allowing it to incorporate information from the input sequence when generating each output token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<center>\n",
        "    <img src=\"static/image2.webp\" alt=\"Transformers Architecture\" style=\"width:50%;\">\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nJHsQ-B8BJ5"
      },
      "source": [
        "## Applications of Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0vfGTFT5AFL"
      },
      "source": [
        "Transformers have found wide-ranging applications across various domains:\n",
        "\n",
        "1. **Natural Language Processing**:\n",
        "   - Machine translation\n",
        "   - Text summarization\n",
        "   - Named entity recognition\n",
        "   - Sentiment analysis\n",
        "   - Question answering\n",
        "\n",
        "2. **Computer Vision**:\n",
        "   - Image classification\n",
        "   - Object detection\n",
        "   - Image generation\n",
        "\n",
        "3. **Speech Recognition**: Converting audio signals to transcribed text.\n",
        "\n",
        "4. **Multimodal Tasks**:\n",
        "   - Image captioning\n",
        "   - Visual question answering\n",
        "   - Text-to-image generation (e.g., DALL-E)\n",
        "\n",
        "5. **Biological Sequence Analysis**: Analyzing DNA and protein sequences.\n",
        "\n",
        "6. **Time Series Prediction**: Forecasting in various domains, including finance and weather.\n",
        "\n",
        "7. **Code Generation**: Writing computer code based on natural language requirements.\n",
        "\n",
        "8. **Recommendation Systems**: Providing personalized recommendations.\n",
        "\n",
        "9. **Music Generation**: Creating original musical compositions.\n",
        "\n",
        "10. **Robotics**: Improving robot control and decision-making processes.\n",
        "\n",
        "11. **Game Playing**: Evaluating chess board positions and other game-related tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs_PMM3hF3_v"
      },
      "source": [
        "# Implementing Transformers with Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqDDGv1A1v7n"
      },
      "outputs": [],
      "source": [
        "# importing necessary libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7zrVI3dJxYL"
      },
      "source": [
        "##  Implement Positional Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxpmoPQaJ7iu"
      },
      "source": [
        "The positional_encoding function generates positional encodings for input sequences, which help the transformer model understand the order of the tokens.\n",
        "\n",
        "- **Inputs:**\n",
        "  - position: An integer representing the position in the sequence (e.g., token index).\n",
        "  - d_model: An integer representing the dimensionality of the model (e.g., embedding size).\n",
        "\n",
        "- **Process:**\n",
        "  1. The `get_angles` function computes the angle rates based on the position and dimension, using a formula that ensures unique encodings for each position.\n",
        "  2. The angle values are calculated for all positions and dimensions using NumPy operations.\n",
        "  3. The sine function is applied to the even indices, while the cosine function is applied to the odd indices to create the final positional encodings.\n",
        "  4. The output is reshaped to include a batch dimension and cast to a TensorFlow float32 tensor.\n",
        "\n",
        "- **Output:**\n",
        "  - The function returns a TensorFlow tensor containing the positional encodings, which can be added to the input embeddings to incorporate position information into the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RrzhXlVJ2Kr"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(position, d_model):\n",
        "    \"\"\"\n",
        "    Generate positional encoding for a given position and model dimension.\n",
        "\n",
        "    Args:\n",
        "    position (int): The position in the sequence.\n",
        "    d_model (int): The dimension of the model.\n",
        "\n",
        "    Returns:\n",
        "    numpy array: Positional encoding for the given position.\n",
        "    \"\"\"\n",
        "    def get_angles(pos, i, d_model):\n",
        "        angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "        return pos * angle_rates\n",
        "\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                            np.arange(d_model)[np.newaxis, :],\n",
        "                            d_model)\n",
        "\n",
        "    # Apply sin to even indices in the array\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # Apply cos to odd indices in the array\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoF8YZ1_Kecf"
      },
      "source": [
        "## Define the Scaled Dot Product Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrqeTF9eKsVA"
      },
      "source": [
        "The `scaled_dot_product_attention` function computes the attention output and attention weights using the scaled dot-product attention mechanism.\n",
        "\n",
        "- **Inputs:**\n",
        "  - q (query): A tensor of shape `(..., seq_len_q, depth)` representing the query vectors.\n",
        "  - k (key): A tensor of shape `(..., seq_len_k, depth)` representing the key vectors.\n",
        "  - v (value): A tensor of shape `(..., seq_len_v, depth_v)` representing the value vectors.\n",
        "  - mask: A tensor used to prevent attention to certain positions, with a shape that can be broadcasted to `(..., seq_len_q, seq_len_k)`.\n",
        "\n",
        "- **Process:**\n",
        "  1. **Matrix Multiplication:** The query (`q`) is multiplied with the key (`k`) transposed to get the raw attention scores (`matmul_qk`), which have the shape `(..., seq_len_q, seq_len_k)`.\n",
        "  2. **Scaling:** The attention scores are scaled by dividing by the square root of the depth of the key (`dk`), which helps stabilize gradients during training.\n",
        "  3. **Masking:** If a mask is provided, it is added to the scaled attention scores, where masked positions are set to a very large negative value (`-1e9`) to ensure they receive zero attention after applying softmax.\n",
        "  4. **Softmax:** The softmax function is applied to the scaled scores along the last axis (`seq_len_k`) to obtain the attention weights, which sum to 1 for each query.\n",
        "  5. **Output Calculation:** The attention weights are multiplied by the value (`v`) to generate the final output, which captures the relevant information from the values based on the attention weights.\n",
        "\n",
        "- **Output:**\n",
        "  - The function returns the attention output and the computed attention weights. The output shape is `(..., seq_len_q, depth_v)`, while the attention weights shape is `(..., seq_len_q, seq_len_k)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJEkIH3CKaf6"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"\n",
        "    Calculate the attention weights.\n",
        "\n",
        "    Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable to (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    Returns:\n",
        "    output, attention_weights\n",
        "    \"\"\"\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    # Scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # Add the mask to the scaled tensor\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    # Softmax is normalized on the last axis (seq_len_k)\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogW2dg4fLAoh"
      },
      "source": [
        "## Implement Multi-Head Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJEWBPreL2iK"
      },
      "source": [
        "The `MultiHeadAttention` class implements the multi-head attention mechanism used in transformer models, allowing the model to focus on different parts of the input sequence simultaneously.\n",
        "\n",
        "- **Initialization (`__init__` method):**\n",
        "  - **Parameters:**\n",
        "    - d_model: The dimensionality of the model (embedding size).\n",
        "    - num_heads: The number of attention heads.\n",
        "  - The class asserts that `d_model` is divisible by `num_heads`, ensuring that each head has an equal share of the dimensionality.\n",
        "  - depth: The dimensionality of each attention head, calculated as `d_model // num_heads`.\n",
        "  - Four dense layers are defined for transforming the query (`wq`), key (`wk`), value (`wv`), and the output (`dense`).\n",
        "\n",
        "- **Splitting Heads (`split_heads` method):**\n",
        "  - This method reshapes the input tensor `x` to split the last dimension into multiple heads and permutes the dimensions to have the shape `(batch_size, num_heads, seq_len, depth)`. This allows parallel processing of different attention heads.\n",
        "\n",
        "- **Call Method (`call` method):**\n",
        "  - **Inputs:**\n",
        "    - `v`, `k`, `q`: Value, key, and query tensors.\n",
        "    - `mask`: An optional tensor to mask certain positions.\n",
        "  - The method retrieves the batch size and applies the dense layers to the input tensors `q`, `k`, and `v`, transforming them into the model dimension.\n",
        "  - The transformed tensors are split into multiple heads using the `split_heads` method.\n",
        "  - The `scaled_dot_product_attention` function is called with the split queries, keys, values, and mask, returning the scaled attention output and attention weights.\n",
        "  - The attention output is transposed and reshaped to combine the attention heads.\n",
        "  - The final output is computed by passing the concatenated attention through the output dense layer.\n",
        "\n",
        "- **Output:**\n",
        "  - The method returns the attention output (shape: `(batch_size, seq_len_q, d_model)`) and the attention weights (shape: `(batch_size, num_heads, seq_len_q, seq_len_k)`), which indicate the importance of each key for each query in the multi-head attention mechanism."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0q5INlUK1yD"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arPYG3BYMIj8"
      },
      "source": [
        "## Implement Feed-Forward Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUmcbr0-MJtF"
      },
      "source": [
        "The `point_wise_feed_forward_network` function creates a feed-forward neural network that processes each position in the input independently and identically, as used in transformer models.\n",
        "\n",
        "- **Parameters:**\n",
        "  - d_model: The dimensionality of the model (input/output size).\n",
        "  - dff: The dimensionality of the feed-forward network's hidden layer (intermediate size).\n",
        "\n",
        "- **Process:**\n",
        "  - The function returns a `tf.keras.Sequential` model consisting of two dense layers:\n",
        "    1. The first dense layer transforms the input from `d_model` to `dff` dimensions using the ReLU activation function. This layer captures non-linear relationships in the data.\n",
        "    2. The second dense layer transforms the output back from `dff` to `d_model` dimensions, ensuring that the final output maintains the same shape as the original input.\n",
        "\n",
        "- **Output:**\n",
        "  - The resulting feed-forward network processes inputs of shape `(batch_size, seq_len, d_model)` and outputs tensors of the same shape `(batch_size, seq_len, d_model)`, with the intermediate representation shaped `(batch_size, seq_len, dff)`. This structure allows each position in the sequence to be processed independently while preserving the overall dimensionality of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyjBu4M7MG0h"
      },
      "outputs": [],
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WldRHVd-MaP1"
      },
      "source": [
        "## Implement Encoder Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0xq0p12Mba_"
      },
      "source": [
        "The `EncoderLayer` class implements a single layer of the encoder in a transformer model, combining multi-head attention and feed-forward neural networks with normalization and dropout to enhance learning.\n",
        "\n",
        "- **Initialization (`__init__` method):**\n",
        "  - **Parameters:**\n",
        "    - d_model: The dimensionality of the model (embedding size).\n",
        "    - num_heads: The number of attention heads for multi-head attention.\n",
        "    - dff: The dimensionality of the feed-forward network's hidden layer.\n",
        "    - rate: The dropout rate, with a default value of 0.1.\n",
        "  - The class initializes:\n",
        "    - mha: An instance of the `MultiHeadAttention` class for performing attention.\n",
        "    - ffn: A point-wise feed-forward network created using the `point_wise_feed_forward_network` function.\n",
        "    - Two layer normalization layers (`layernorm1` and `layernorm2`) to stabilize training.\n",
        "    - Two dropout layers (`dropout1` and `dropout2`) to prevent overfitting.\n",
        "\n",
        "- **Call Method (`call` method):**\n",
        "  - **Inputs:**\n",
        "    - `x`: The input tensor of shape `(batch_size, input_seq_len, d_model)`.\n",
        "    - `training`: A boolean indicating whether the model is in training mode (used for dropout).\n",
        "    - `mask`: An optional tensor to mask certain positions in the input.\n",
        "  - The method performs the following steps:\n",
        "    1. **Multi-Head Attention:** Computes the attention output using the input `x` as query, key, and value. The output shape is `(batch_size, input_seq_len, d_model)`.\n",
        "    2. **Dropout:** Applies dropout to the attention output to reduce overfitting.\n",
        "    3. **Add & Norm:** Adds the original input `x` to the attention output and applies layer normalization (`layernorm1`). This residual connection helps in training deep networks.\n",
        "    4. **Feed-Forward Network:** Passes the normalized output through the feed-forward network (`ffn_output`), retaining the shape `(batch_size, input_seq_len, d_model)`.\n",
        "    5. **Dropout:** Applies dropout to the feed-forward network output.\n",
        "    6. **Add & Norm:** Adds the output of the feed-forward network to the result from the first normalization step and applies another layer normalization (`layernorm2`).\n",
        "\n",
        "- **Output:**\n",
        "  - The method returns the final output of the encoder layer, which has the same shape as the input: `(batch_size, input_seq_len, d_model)`. This output can then be fed into subsequent layers of the transformer model, maintaining the rich contextual information learned through attention and feed-forward processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYxc7-ETMDMZ"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return out2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9BUnCf6Mo3O"
      },
      "source": [
        "## Implement Decoder Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04NViJHrMy6h"
      },
      "source": [
        "The `DecoderLayer` class implements a single layer of the decoder in a transformer model. It combines two multi-head attention mechanisms with a feed-forward neural network, incorporating layer normalization and dropout for stability and regularization.\n",
        "\n",
        "- **Initialization (`__init__` method):**\n",
        "  - **Parameters:**\n",
        "    - d_model: The dimensionality of the model (embedding size).\n",
        "    - num_heads: The number of attention heads for multi-head attention.\n",
        "    - dff: The dimensionality of the feed-forward network's hidden layer.\n",
        "    - rate: The dropout rate, defaulting to 0.1.\n",
        "  - The class initializes:\n",
        "    - mha1: The first multi-head attention mechanism, which is responsible for self-attention within the decoder.\n",
        "    - mha2: The second multi-head attention mechanism, which performs cross-attention with the encoder's output.\n",
        "    - ffn: A point-wise feed-forward network created using the `point_wise_feed_forward_network` function.\n",
        "    - Three layer normalization layers (`layernorm1`, `layernorm2`, `layernorm3`) to stabilize training.\n",
        "    - Three dropout layers (`dropout1`, `dropout2`, `dropout3`) to prevent overfitting.\n",
        "\n",
        "- **Call Method (`call` method):**\n",
        "  - **Inputs:**\n",
        "    - `x`: The input tensor of shape `(batch_size, target_seq_len, d_model)` representing the decoder's current input.\n",
        "    - `enc_output`: The encoder's output of shape `(batch_size, input_seq_len, d_model)`.\n",
        "    - `training`: A boolean indicating whether the model is in training mode (used for dropout).\n",
        "    - `look_ahead_mask`: A mask to prevent attention to future tokens in the target sequence.\n",
        "    - `padding_mask`: A mask to ignore padding tokens in the encoder's output.\n",
        "  - The method performs the following steps:\n",
        "    1. **Self-Attention (First MHA):** Computes self-attention on `x` using `mha1`, applying the look-ahead mask to ensure that the model can only attend to the current and previous tokens. The output shape is `(batch_size, target_seq_len, d_model)`.\n",
        "    2. **Dropout & Add & Norm:** Applies dropout to the attention output, then adds the original input `x` and applies layer normalization (`layernorm1`).\n",
        "    3. **Cross-Attention (Second MHA):** Computes attention using the encoder output (`enc_output`) and the output from the first attention block (`out1`). This layer allows the decoder to focus on relevant parts of the input sequence while generating the output.\n",
        "    4. **Dropout & Add & Norm:** Applies dropout to the second attention output, adds the output from the first normalization step, and applies another layer normalization (`layernorm2`).\n",
        "    5. **Feed-Forward Network:** Passes the output through the feed-forward network (`ffn_output`), retaining the shape `(batch_size, target_seq_len, d_model)`.\n",
        "    6. **Dropout & Add & Norm:** Applies dropout to the feed-forward output, adds it to the output from the second normalization step, and applies a final layer normalization (`layernorm3`).\n",
        "\n",
        "- **Output:**\n",
        "  - The method returns:\n",
        "    - `out3`: The final output of the decoder layer, shaped `(batch_size, target_seq_len, d_model)`, which can be used as input for subsequent decoder layers.\n",
        "    - `attn_weights_block1`: The attention weights from the first multi-head attention layer (self-attention).\n",
        "    - `attn_weights_block2`: The attention weights from the second multi-head attention layer (cross-attention), allowing for visualization of how the decoder attends to the encoder's outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mW7bvgtXMfaI"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "\n",
        "    def call(self, x, enc_output, training,\n",
        "             look_ahead_mask, padding_mask):\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(\n",
        "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ULfDxryNGwQ"
      },
      "source": [
        "## Implement Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HGGU6WkNNum"
      },
      "source": [
        "The `Encoder` class implements the encoder part of the Transformer architecture. It consists of multiple layers of the `EncoderLayer`, with embedding and positional encoding applied to the input sequence. Here’s a breakdown of its components and functionality:\n",
        "\n",
        "- Initialization (`__init__` Method)\n",
        "\n",
        "  - **Parameters:**\n",
        "    - `num_layers`: Number of encoder layers to stack.\n",
        "    - `d_model`: Dimensionality of the model (embedding size).\n",
        "    - `num_heads`: Number of attention heads in multi-head attention.\n",
        "    - `dff`: Dimensionality of the feed-forward network's hidden layer.\n",
        "    - `input_vocab_size`: Size of the input vocabulary for the embedding layer.\n",
        "    - `maximum_position_encoding`: The maximum length of the input sequence to define positional encoding.\n",
        "    - `rate`: Dropout rate, defaulting to 0.1.\n",
        "  \n",
        "  - **Attributes:**\n",
        "    - `self.embedding`: An embedding layer to convert input token indices into dense vectors of shape `(batch_size, input_seq_len, d_model)`.\n",
        "    - `self.pos_encoding`: Positional encoding added to the input embeddings to retain the sequence order. This is computed using the `positional_encoding` function.\n",
        "    - `self.enc_layers`: A list of `EncoderLayer` instances created based on `num_layers`.\n",
        "    - `self.dropout`: A dropout layer applied to the output of the embedding and positional encoding.\n",
        "\n",
        "- Call Method (`call` Method)\n",
        "\n",
        "  - **Inputs:**\n",
        "    - `x`: The input tensor of shape `(batch_size, input_seq_len)` representing token indices.\n",
        "    - `training`: A boolean indicating whether the model is in training mode (used for dropout).\n",
        "    - `mask`: A mask to prevent attention to certain positions (e.g., padding tokens).\n",
        "\n",
        "  - **Functionality:**\n",
        "    1. **Get Sequence Length:** The method retrieves the sequence length from the input `x` using `tf.shape(x)[1]`.\n",
        "    \n",
        "    2. **Embedding and Positional Encoding:**\n",
        "      - The input tokens are converted into embeddings: `x = self.embedding(x)` gives a shape of `(batch_size, input_seq_len, d_model)`.\n",
        "      - The embeddings are scaled by the square root of `d_model` to counteract the effect of the embeddings' size.\n",
        "      - Positional encoding is added: `x += self.pos_encoding[:, :seq_len, :]` allows the model to incorporate the position of each token in the sequence.\n",
        "    \n",
        "    3. **Dropout:** The dropout layer is applied to the combined embeddings and positional encodings: `x = self.dropout(x, training=training)`.\n",
        "\n",
        "    4. **Pass Through Encoder Layers:** The input `x` is sequentially passed through each of the `EncoderLayer` instances: `x = self.enc_layers[i](x, training, mask)`. Each layer performs multi-head attention and feed-forward operations.\n",
        "\n",
        "- **Output:**\n",
        "  - The final output `x` has a shape of `(batch_size, input_seq_len, d_model)`, representing the encoded input sequence with learned features from the encoder layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jIDD63MMwYI"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
        "                           for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        # Adding embedding and position encoding.\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O76O1RljNqHN"
      },
      "source": [
        "## Implement Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBpJ0GofNt8A"
      },
      "source": [
        "The `Decoder` class implements the decoder part of the Transformer architecture. It consists of multiple layers of the `DecoderLayer`, with embedding and positional encoding applied to the target sequence. Here's a breakdown of its components and functionality:\n",
        "\n",
        "- Initialization (`__init__` Method)\n",
        "\n",
        "  - **Parameters:**\n",
        "    - `num_layers`: Number of decoder layers to stack.\n",
        "    - `d_model`: Dimensionality of the model (embedding size).\n",
        "    - `num_heads`: Number of attention heads in multi-head attention.\n",
        "    - `dff`: Dimensionality of the feed-forward network's hidden layer.\n",
        "    - `target_vocab_size`: Size of the target vocabulary for the embedding layer.\n",
        "    - `maximum_position_encoding`: The maximum length of the target sequence to define positional encoding.\n",
        "    - `rate`: Dropout rate, defaulting to 0.1.\n",
        "\n",
        "  - **Attributes:**\n",
        "    - `self.embedding`: An embedding layer to convert target token indices into dense vectors of shape `(batch_size, target_seq_len, d_model)`.\n",
        "    - `self.pos_encoding`: Positional encoding added to the input embeddings to retain the sequence order. This is computed using the `positional_encoding` function.\n",
        "    - `self.dec_layers`: A list of `DecoderLayer` instances created based on `num_layers`.\n",
        "    - `self.dropout`: A dropout layer applied to the output of the embedding and positional encoding.\n",
        "\n",
        "- Call Method (`call` Method)\n",
        "\n",
        "  - **Inputs:**\n",
        "    - `x`: The input tensor of shape `(batch_size, target_seq_len)` representing token indices from the target sequence.\n",
        "    - `enc_output`: The output from the encoder, which contains the encoded representations of the input sequence.\n",
        "    - `training`: A boolean indicating whether the model is in training mode (used for dropout).\n",
        "    - `look_ahead_mask`: A mask to prevent attention to future tokens in the target sequence.\n",
        "    - `padding_mask`: A mask to prevent attention to padding tokens in the encoder output.\n",
        "\n",
        "  - **Functionality:**\n",
        "    1. **Get Sequence Length:** The method retrieves the sequence length from the input `x` using `tf.shape(x)[1]`.\n",
        "    \n",
        "    2. **Embedding and Positional Encoding:**\n",
        "      - The input tokens are converted into embeddings: `x = self.embedding(x)` gives a shape of `(batch_size, target_seq_len, d_model)`.\n",
        "      - The embeddings are scaled by the square root of `d_model` to counteract the effect of the embeddings' size.\n",
        "      - Positional encoding is added: `x += self.pos_encoding[:, :seq_len, :]` allows the model to incorporate the position of each token in the sequence.\n",
        "    \n",
        "    3. **Dropout:** The dropout layer is applied to the combined embeddings and positional encodings: `x = self.dropout(x, training=training)`.\n",
        "\n",
        "    4. **Pass Through Decoder Layers:** The input `x` is sequentially passed through each of the `DecoderLayer` instances. During this process, both the encoder output (`enc_output`) and the target sequence's output from the previous layer are used:\n",
        "      - Each decoder layer returns both the output of the layer and the attention weights from both multi-head attention blocks.\n",
        "      - The attention weights for each layer are stored in the `attention_weights` dictionary:\n",
        "        ```python\n",
        "        attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "        attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "        ```\n",
        "\n",
        "- **Output:**\n",
        "  - The final output `x` has a shape of `(batch_size, target_seq_len, d_model)`, representing the decoded target sequence with learned features from the decoder layers.\n",
        "  - The `attention_weights` dictionary contains the attention weights for each layer, which can be useful for visualization or understanding model behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlX9o0VxNMBs"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
        "                           for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                                   look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNyIQYSMN6sk"
      },
      "source": [
        "## Implement Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngEo2h0NN-bl"
      },
      "source": [
        "The `Transformer` class encapsulates the entire Transformer architecture, combining the encoder and decoder to facilitate tasks such as machine translation, text summarization, and other sequence-to-sequence applications. Here is a detailed breakdown of its components and functionality:\n",
        "\n",
        "- Initialization (`__init__` Method)\n",
        "\n",
        "  - **Parameters:**\n",
        "    - `num_layers`: Number of encoder and decoder layers in the Transformer.\n",
        "    - `d_model`: Dimensionality of the model (size of the embedding and output).\n",
        "    - `num_heads`: Number of attention heads for the multi-head attention mechanism.\n",
        "    - `dff`: Dimensionality of the feed-forward network’s hidden layer.\n",
        "    - `input_vocab_size`: Size of the input vocabulary, which is used for embedding the input tokens.\n",
        "    - `target_vocab_size`: Size of the target vocabulary for embedding the output tokens.\n",
        "    - `pe_input`: Maximum position encoding for the input sequence.\n",
        "    - `pe_target`: Maximum position encoding for the target sequence.\n",
        "    - `rate`: Dropout rate (defaulting to 0.1).\n",
        "\n",
        "  - **Attributes:**\n",
        "    - `self.encoder`: An instance of the `Encoder` class, initialized with the parameters specified.\n",
        "    - `self.decoder`: An instance of the `Decoder` class, initialized with the relevant parameters.\n",
        "    - `self.final_layer`: A dense layer that converts the decoder output into logits for each token in the target vocabulary.\n",
        "\n",
        "- Call Method (`call` Method)\n",
        "\n",
        "  - **Inputs:**\n",
        "    - `inp`: The input tensor of shape `(batch_size, inp_seq_len)` representing the source sequence.\n",
        "    - `tar`: The target tensor of shape `(batch_size, tar_seq_len)` representing the target sequence (usually shifted right).\n",
        "    - `training`: A boolean indicating whether the model is in training mode (used for dropout).\n",
        "    - `enc_padding_mask`: A mask to prevent attention to padding tokens in the encoder input.\n",
        "    - `look_ahead_mask`: A mask to prevent attention to future tokens in the decoder input.\n",
        "    - `dec_padding_mask`: A mask to prevent attention to padding tokens in the decoder input.\n",
        "\n",
        "  - **Functionality:**\n",
        "    1. **Encoder Output:** The input sequence is passed through the encoder:\n",
        "      ```python\n",
        "      enc_output = self.encoder(inp, training, enc_padding_mask)\n",
        "      ```\n",
        "      This returns the encoded representation of the input, with the shape `(batch_size, inp_seq_len, d_model)`.\n",
        "\n",
        "    2. **Decoder Output:** The target sequence is then passed through the decoder along with the encoder output:\n",
        "      ```python\n",
        "      dec_output, attention_weights = self.decoder(\n",
        "          tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "      ```\n",
        "      This returns the decoder output of shape `(batch_size, tar_seq_len, d_model)` and a dictionary of attention weights for each decoder layer.\n",
        "\n",
        "    3. **Final Output:** The decoder output is then passed through the final dense layer to generate logits for each token in the target vocabulary:\n",
        "      ```python\n",
        "      final_output = self.final_layer(dec_output)\n",
        "      ```\n",
        "      The shape of `final_output` is `(batch_size, tar_seq_len, target_vocab_size)`.\n",
        "\n",
        "- **Output:**\n",
        "  - The method returns two outputs:\n",
        "    - `final_output`: The logits for each token in the target vocabulary, which can be used for loss calculation or predictions.\n",
        "    - `attention_weights`: A dictionary containing the attention weights from each decoder layer, useful for visualizations and understanding model behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWyxpYAMNogG"
      },
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                 target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
        "                               input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
        "                               target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(self, inp, tar, training, enc_padding_mask,\n",
        "             look_ahead_mask, dec_padding_mask):\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "        return final_output, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TD2j5tT3Tu71"
      },
      "source": [
        "# Let's Build a Real world project to understand the concept of Transformers better"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UwLklz8aLSG"
      },
      "source": [
        "# News Article Classification using BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmHo677MeeM7"
      },
      "source": [
        "## Problem Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sbJza9Lej6R"
      },
      "source": [
        "We aim to build a news article classification model using a Transformer architecture to categorize articles into predefined categories. This project will demonstrate the effectiveness of Transformer models in capturing contextual information from text data for multi-class classification of news articles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qdra5uqMm7Zu"
      },
      "source": [
        "## Dataset Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LilxbXPFm9kP"
      },
      "source": [
        "- The AG News dataset consists of 127,600 news articles, split into 120,000 training and 7,600 testing samples.\n",
        "\n",
        "- Each article is labeled as one of four categories: World, Sports, Business, or Sci/Tech.\n",
        "\n",
        "- The dataset contains the title and description of each news article.\n",
        "\n",
        "- Key features of the dataset:\n",
        "  - 127,600 news articles (120,000 for training, 7,600 for testing)\n",
        "  - Multi-class classification (4 categories)\n",
        "  - Raw text data including both title and description\n",
        "  - Balanced distribution across categories\n",
        "\n",
        "\n",
        "- For more information about the AG News dataset, you can visit the following link: [AG News Dataset](https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "C7hbyEzOTWtn"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMHtxKH1qT8y"
      },
      "source": [
        "## Loading the AG News dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use tfds.load() to load the AG News subset dataset. The with_info=True parameter returns dataset info along with the dataset itself. The as_supervised=True parameter ensures that we get (input, label) pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "sT430sLrqNmr"
      },
      "outputs": [],
      "source": [
        "# Load the AG News dataset\n",
        "(train_data, test_data), info = tfds.load('ag_news_subset', split=['train', 'test'], with_info=True, as_supervised=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fGAjrhktMrT",
        "outputId": "e926cd5c-0af6-4128-8794-5d030d086232"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tfds.core.DatasetInfo(\n",
              "    name='ag_news_subset',\n",
              "    full_name='ag_news_subset/1.0.0',\n",
              "    description=\"\"\"\n",
              "    AG is a collection of more than 1 million news articles. News articles have been\n",
              "    gathered from more than 2000 news sources by ComeToMyHead in more than 1 year of\n",
              "    activity. ComeToMyHead is an academic news search engine which has been running\n",
              "    since July, 2004. The dataset is provided by the academic comunity for research\n",
              "    purposes in data mining (clustering, classification, etc), information retrieval\n",
              "    (ranking, search, etc), xml, data compression, data streaming, and any other\n",
              "    non-commercial activity. For more information, please refer to the link\n",
              "    http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html .\n",
              "    \n",
              "    The AG's news topic classification dataset is constructed by Xiang Zhang\n",
              "    (xiang.zhang@nyu.edu) from the dataset above. It is used as a text\n",
              "    classification benchmark in the following paper: Xiang Zhang, Junbo Zhao, Yann\n",
              "    LeCun. Character-level Convolutional Networks for Text Classification. Advances\n",
              "    in Neural Information Processing Systems 28 (NIPS 2015).\n",
              "    \n",
              "    The AG's news topic classification dataset is constructed by choosing 4 largest\n",
              "    classes from the original corpus. Each class contains 30,000 training samples\n",
              "    and 1,900 testing samples. The total number of training samples is 120,000 and\n",
              "    testing 7,600.\n",
              "    \"\"\",\n",
              "    homepage='https://arxiv.org/abs/1509.01626',\n",
              "    data_dir='/root/tensorflow_datasets/ag_news_subset/1.0.0',\n",
              "    file_format=tfrecord,\n",
              "    download_size=11.24 MiB,\n",
              "    dataset_size=35.79 MiB,\n",
              "    features=FeaturesDict({\n",
              "        'description': Text(shape=(), dtype=string),\n",
              "        'label': ClassLabel(shape=(), dtype=int64, num_classes=4),\n",
              "        'title': Text(shape=(), dtype=string),\n",
              "    }),\n",
              "    supervised_keys=('description', 'label'),\n",
              "    disable_shuffling=False,\n",
              "    splits={\n",
              "        'test': <SplitInfo num_examples=7600, num_shards=1>,\n",
              "        'train': <SplitInfo num_examples=120000, num_shards=1>,\n",
              "    },\n",
              "    citation=\"\"\"@misc{zhang2015characterlevel,\n",
              "        title={Character-level Convolutional Networks for Text Classification},\n",
              "        author={Xiang Zhang and Junbo Zhao and Yann LeCun},\n",
              "        year={2015},\n",
              "        eprint={1509.01626},\n",
              "        archivePrefix={arXiv},\n",
              "        primaryClass={cs.LG}\n",
              "    }\"\"\",\n",
              ")"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdgVDkZzrFc5",
        "outputId": "ca4d0e3b-9876-451a-a1ea-5aa2101bcf18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: AMD #39;s new dual-core Opteron chip is designed mainly for corporate computing applications, including databases, Web services, and financial transactions.\n",
            "Label: 3\n",
            "-------------------------------------\n",
            "Text: Reuters - Major League Baseball\\Monday announced a decision on the appeal filed by Chicago Cubs\\pitcher Kerry Wood regarding a suspension stemming from an\\incident earlier this season.\n",
            "Label: 1\n",
            "-------------------------------------\n",
            "Text: President Bush #39;s  quot;revenue-neutral quot; tax reform needs losers to balance its winners, and people claiming the federal deduction for state and local taxes may be in administration planners #39; sights, news reports say.\n",
            "Label: 2\n",
            "-------------------------------------\n",
            "Text: Britain will run out of leading scientists unless science education is improved, says Professor Colin Pillinger.\n",
            "Label: 3\n",
            "-------------------------------------\n",
            "Text: London, England (Sports Network) - England midfielder Steven Gerrard injured his groin late in Thursday #39;s training session, but is hopeful he will be ready for Saturday #39;s World Cup qualifier against Austria.\n",
            "Label: 1\n",
            "-------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Visualize the sample dataset\n",
        "for text, label in train_data.take(5):\n",
        "    print(f\"Text: {text.numpy().decode('utf-8')}\")\n",
        "    print(f\"Label: {label.numpy()}\")\n",
        "    print('-------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJzCx4PQ4ZEr"
      },
      "source": [
        "## Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code sets up a `TextVectorization` layer from TensorFlow, which is used to preprocess the text data before feeding it into the Transformer model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVtLL8pX5npb"
      },
      "source": [
        "- Vocabulary Size (vocab_size): This parameter defines the maximum number of unique tokens (words) that the model will consider. In this case, the model will use the 10,000 most frequent words in the dataset. Any words outside this vocabulary will be replaced with an out-of-vocabulary token.\n",
        "\n",
        "- Maximum Sequence Length (max_length): This value specifies the maximum number of tokens in each sequence. Sequences shorter than this length will be padded, and longer sequences will be truncated to fit this specified length.\n",
        "\n",
        "- TextVectorization Layer: This layer transforms the raw text into a sequence of integers. Each integer corresponds to a token in the vocabulary, making it possible for the model to process the data numerically. It standardizes the text data, tokenizes it into words, and then converts these words into their corresponding token indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "FjtEORjZyalS"
      },
      "outputs": [],
      "source": [
        "# Preprocess the data\n",
        "vocab_size = 10000\n",
        "max_length = 100\n",
        "\n",
        "vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=max_length\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf62FUJ_481u"
      },
      "source": [
        "## Adapting the TextVectorization Layer to Training Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code snippet demonstrates how to adapt the `TextVectorization` layer to the training data in order to build a vocabulary based on the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er6JgW_w50Fs"
      },
      "source": [
        "- Training Text Extraction:\n",
        "\n",
        "  - train_text = train_data.map(lambda text, label: text) extracts only the text component from the training dataset. The use of lambda text, label: text ensures that labels are ignored during this operation, as the focus is on analyzing the text itself.\n",
        "\n",
        "- Layer Adaptation:\n",
        "\n",
        "  - The vectorize_layer.adapt(train_text) command tunes the TextVectorization layer to the vocabulary present in the training data. During this process, the layer scans through the dataset and builds a list of the most frequent words, limiting the vocabulary to the specified vocab_size.\n",
        "  - This adaptation step ensures that the layer is familiar with the words and their relative frequencies in the training set, enabling it to accurately transform text into integer sequences based on the built vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "bLZAsTI4ydyf"
      },
      "outputs": [],
      "source": [
        "# Adapt the layer to the training data\n",
        "train_text = train_data.map(lambda text, label: text)\n",
        "vectorize_layer.adapt(train_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLe1vGh74_ao"
      },
      "source": [
        "## Text Vectorization Function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code defines a function named `vectorize_text` that converts the raw text into integer sequences using the previously adapted `TextVectorization` layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCx7qzoo6GhW"
      },
      "source": [
        "- Function Purpose:\n",
        "\n",
        "  - The vectorize_text function takes two inputs: text and label.\n",
        "  It processes the text using the vectorize_layer, converting the text into a sequence of integers that represent token indices from the vocabulary.\n",
        "  The function then returns the transformed text along with its corresponding label.\n",
        "\n",
        "- Input Parameters:\n",
        "\n",
        "  - text: The raw text data that needs to be converted into numerical format.\n",
        "  - label: The label associated with the text, which represents the category of the news article.\n",
        "\n",
        "- Vectorization Process:\n",
        "\n",
        "  - text = vectorize_layer(text) applies the TextVectorization layer to the input text, transforming it into a sequence of integers.\n",
        "  This transformation ensures that each word in the text is represented by its corresponding token ID, making the data compatible with the Transformer model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "k0Ho5nH_yeyP"
      },
      "outputs": [],
      "source": [
        "def vectorize_text(text, label):\n",
        "    text = vectorize_layer(text)\n",
        "    return text, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Yjqgi2P5FJp"
      },
      "source": [
        "## Creating Vectorized Datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code creates vectorized versions of the training and test datasets by applying the `vectorize_text` function to each sample in the datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ07mXIC6qEY"
      },
      "source": [
        "- Vectorized Training Dataset:\n",
        "\n",
        "  - train_ds = train_data.map(vectorize_text) applies the vectorize_text function to each entry in the training dataset.\n",
        "  - The map operation ensures that each text sample in the training dataset is converted into its corresponding integer sequence, as defined by the\n",
        "  TextVectorization layer.\n",
        "  - The labels remain unchanged, allowing the model to train on the vectorized text data paired with their respective categories.\n",
        "\n",
        "- Vectorized Test Dataset:\n",
        "\n",
        "  - test_ds = test_data.map(vectorize_text) performs a similar operation on the test dataset, ensuring that the text samples in the test set are also transformed into integer sequences.\n",
        "  - This step guarantees that both the training and test datasets are in the same numerical format, enabling consistent performance evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "DqtYXGAJyklh"
      },
      "outputs": [],
      "source": [
        "# Create vectorized datasets\n",
        "train_ds = train_data.map(vectorize_text)\n",
        "test_ds = test_data.map(vectorize_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puJbEoiV5LCY"
      },
      "source": [
        "## Configuring the Dataset for Performance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code optimizes the performance of the training and test datasets by using caching, shuffling, batching, and prefetching techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDWxkJb87GCM"
      },
      "source": [
        "- AUTOTUNE:\n",
        "\n",
        "  - AUTOTUNE = tf.data.AUTOTUNE enables TensorFlow to dynamically adjust and optimize data loading for better performance during training and evaluation.\n",
        "\n",
        "- Batch Size (BATCH_SIZE):\n",
        "\n",
        "  - BATCH_SIZE = 32 sets the number of samples that will be processed together in each step of training. Batching helps improve computation speed by processing multiple samples simultaneously.\n",
        "\n",
        "- Dataset Optimization Techniques:\n",
        "\n",
        "  - Caching (cache()): train_ds.cache() stores the dataset in memory after the first epoch, which speeds up training by eliminating the need to reload data from disk during each subsequent epoch.\n",
        "\n",
        "  - Shuffling (shuffle()): train_ds.shuffle(10000) randomly shuffles the dataset with a buffer size of 10,000 to ensure that the training data is presented in a different order in each epoch. This prevents the model from learning patterns based on the order of the data.\n",
        "\n",
        "  - Batching (batch()): .batch(BATCH_SIZE) groups the dataset into batches of 32 samples. Batching improves training efficiency and helps utilize computational resources more effectively.\n",
        "\n",
        "  - Prefetching (prefetch()): .prefetch(AUTOTUNE) allows the data loading and model training to overlap. While the model is training on the current batch, the next batch is already being prepared in the background, reducing training time and improving overall throughput."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "VwjNGzo7ymXz"
      },
      "outputs": [],
      "source": [
        "# Configure the dataset for performance\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_ds = train_ds.cache().shuffle(10000).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "test_ds = test_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kcdH0tG5RFJ"
      },
      "source": [
        "## Defining the Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code defines a custom layer for a Transformer model, specifically a building block known as `TransformerBlock`. This block is responsible for performing self-attention and feedforward operations essential for the model's performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94QsKDRZ7gCR"
      },
      "source": [
        "- Class Initialization (__init__ method):\n",
        "\n",
        "  - The TransformerBlock class inherits from tf.keras.layers.Layer, enabling it to function as a layer within the Keras model.\n",
        "  \n",
        "  - Parameters:\n",
        "    - embed_dim: The dimensionality of the embedding space, representing the size of the token embeddings.\n",
        "    \n",
        "    - num_heads: The number of attention heads used in the multi-head attention mechanism. This allows the model to focus on different parts of the input text simultaneously.\n",
        "    \n",
        "    - ff_dim: The dimensionality of the feedforward network hidden layer.\n",
        "    \n",
        "    - rate: The dropout rate, which helps prevent overfitting during training.\n",
        "\n",
        "- Layer Components:\n",
        "\n",
        "  - Multi-Head Attention (self.att): tf.keras.layers.MultiHeadAttention computes the self-attention scores, allowing the model to weigh the importance of different tokens in the input text relative to each other.\n",
        "\n",
        "  - Feedforward Network (self.ffn): A sequential model that consists of two dense layers:\n",
        "    - The first layer applies a ReLU activation function to introduce non-linearity.\n",
        "    - The second layer projects the output back to the embedding dimension.\n",
        "\n",
        "  - Layer Normalization (self.layernorm1 and self.layernorm2): Normalizes the output of each sub-layer, stabilizing the learning process and helping to maintain gradients during backpropagation.\n",
        "\n",
        "  - Dropout Layers (self.dropout1 and self.dropout2): Apply dropout regularization to reduce overfitting by randomly setting a fraction of the input units to zero during training.\n",
        "\n",
        "- Forward Pass (call method): The call method defines how the layer processes its inputs during the forward pass:\n",
        "  - Attention Output: Computes self-attention scores by applying the multi-head attention mechanism to the inputs.\n",
        "  \n",
        "  - Dropout on Attention Output: Applies dropout to the attention output to mitigate overfitting.\n",
        "  \n",
        "  - Layer Normalization and Residual Connection: Adds the original inputs to the attention output and normalizes the result.\n",
        "  \n",
        "  - Feedforward Network Output: Passes the normalized output through the feedforward network.\n",
        "  \n",
        "  - Final Dropout and Layer Normalization: Applies dropout to the feedforward network output, adds it to the previous result, and normalizes it again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "xSjBfasDyoqI"
      },
      "outputs": [],
      "source": [
        "# Define the Transformer model\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDE7faVY9E_5"
      },
      "source": [
        "## Defining the Transformer Model Architecture\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code constructs a Transformer-based model for classifying news articles into predefined categories. It incorporates an embedding layer, a custom Transformer block, and several dense layers to produce the final classification outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtpO6rPU9Hjo"
      },
      "source": [
        "- Model Parameters:\n",
        "\n",
        "  - embed_dim: Specifies the size of the embedding vector for each token. Here, it is set to 32.\n",
        "  - num_heads: Indicates the number of attention heads in the multi-head attention mechanism, set to 2.\n",
        "  - ff_dim: Defines the size of the hidden layer in the feedforward network within the Transformer block, also set to 32.\n",
        "\n",
        "- Input Layer:\n",
        "\n",
        "  - tf.keras.layers.Input(shape=(max_length,)): Creates an input layer that accepts sequences of integers with a length defined by max_length. This corresponds to the preprocessed tokenized input sequences.\n",
        "\n",
        "- Embedding Layer:\n",
        "\n",
        "  - tf.keras.layers.Embedding(vocab_size, embed_dim): Initializes an embedding layer that maps integer token indices to dense vectors of size embed_dim. The vocab_size parameter determines the size of the embedding matrix.\n",
        "\n",
        "- Transformer Block:\n",
        "\n",
        "  - transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim): Instantiates the previously defined TransformerBlock, which will process the embedded inputs.\n",
        "  - x = transformer_block(x): Passes the embedded input through the Transformer block, allowing the model to learn contextual relationships among the tokens.\n",
        "\n",
        "- Global Average Pooling Layer:\n",
        "\n",
        "  - tf.keras.layers.GlobalAveragePooling1D()(x): Averages the sequence of output vectors from the Transformer block, reducing the dimensionality and creating a fixed-size output regardless of the input sequence length.\n",
        "\n",
        "- Dropout Layers:\n",
        "\n",
        "  - tf.keras.layers.Dropout(0.1)(x): Applies dropout with a rate of 0.1 after the global pooling layer to reduce overfitting.\n",
        "  \n",
        "  - Another dropout layer follows the dense layer to further mitigate the risk of overfitting.\n",
        "\n",
        "- Dense Layers:\n",
        "\n",
        "  - tf.keras.layers.Dense(20, activation=\"relu\")(x): A fully connected dense layer with 20 units and ReLU activation, enabling the model to learn non-linear combinations of features.\n",
        "  \n",
        "  - outputs = tf.keras.layers.Dense(4, activation=\"softmax\")(x): The final output layer, which has 4 units corresponding to the number of categories in the classification task, and uses the softmax activation function to produce probability distributions over the categories.\n",
        "\n",
        "- Model Instantiation:\n",
        "\n",
        "  - model = tf.keras.Model(inputs=inputs, outputs=outputs): Constructs the Keras model by specifying the input and output layers, encapsulating the entire architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "3BpXLHH-ytSU"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape=(max_length,))\n",
        "embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "x = tf.keras.layers.Dropout(0.1)(x)\n",
        "x = tf.keras.layers.Dense(20, activation=\"relu\")(x)\n",
        "x = tf.keras.layers.Dropout(0.1)(x)\n",
        "outputs = tf.keras.layers.Dense(4, activation=\"softmax\")(x)\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obdgmpQj95Gu"
      },
      "source": [
        "## Compiling the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code compiles the previously defined Transformer-based model, setting the optimizer, loss function, and evaluation metrics that will be used during training and evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do-eUqbM960q"
      },
      "source": [
        "- Model Compilation: model.compile(...): This method configures the model for training by specifying the optimizer, loss function, and metrics.\n",
        "\n",
        "- Optimizer: optimizer=\"adam\": The Adam optimizer is chosen for its adaptive learning rate capabilities, which can help the model converge more quickly and efficiently during training. It combines the benefits of two other extensions of stochastic gradient descent, namely AdaGrad and RMSProp.\n",
        "\n",
        "- Loss Function: loss=\"sparse_categorical_crossentropy\": This loss function is appropriate for multi-class classification tasks where the target labels are provided as integers (not one-hot encoded). It computes the cross-entropy loss between the predicted probabilities (output from the softmax layer) and the true class labels, measuring how well the predicted distribution aligns with the actual distribution.\n",
        "\n",
        "- Metrics: metrics=[\"accuracy\"]: Accuracy is selected as the metric to evaluate the model's performance. It calculates the proportion of correctly classified samples out of the total samples, providing a straightforward measure of the model's predictive performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "6SA8nLiGyv8a"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCV8rjwt-Hnf"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code trains the compiled Transformer-based model on the training dataset while evaluating its performance on the validation dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXjTRHpc-PiT"
      },
      "source": [
        "- Model Training: history = model.fit(...): This method starts the training process for the model, using the provided training and validation datasets over a specified number of epochs.\n",
        "\n",
        "- Training Dataset: train_ds: The dataset used to train the model. This dataset consists of vectorized text inputs and their corresponding labels.\n",
        "\n",
        "- Validation Dataset: validation_data=test_ds: The dataset used to evaluate the model's performance after each epoch of training. This helps monitor the model's ability to generalize to unseen data.\n",
        "\n",
        "- Epochs: epochs=5: The number of complete passes through the training dataset. In this case, the model will train for 5 epochs. Each epoch consists of a forward pass and backward pass, allowing the model to update its weights based on the computed gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kegFyu5uyxmd",
        "outputId": "c6e7b610-4535-4455-ca39-c087ff2def7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 8ms/step - accuracy: 0.7614 - loss: 0.5917 - val_accuracy: 0.9014 - val_loss: 0.2921\n",
            "Epoch 2/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - accuracy: 0.9148 - loss: 0.2568 - val_accuracy: 0.9004 - val_loss: 0.2982\n",
            "Epoch 3/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 4ms/step - accuracy: 0.9247 - loss: 0.2276 - val_accuracy: 0.8996 - val_loss: 0.3086\n",
            "Epoch 4/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 4ms/step - accuracy: 0.9301 - loss: 0.2061 - val_accuracy: 0.8961 - val_loss: 0.3302\n",
            "Epoch 5/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - accuracy: 0.9335 - loss: 0.1893 - val_accuracy: 0.8939 - val_loss: 0.3729\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=test_ds,\n",
        "    epochs=5\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUps1KCk-j9L"
      },
      "source": [
        "## Evaluating the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code evaluates the performance of the trained Transformer-based model on the test dataset, providing metrics such as loss and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z81cvw5Yyz3E",
        "outputId": "491bc8b2-6a4d-4362-fe64-7e5093f04f61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8960 - loss: 0.3684\n",
            "Test accuracy: 0.894\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(test_ds)\n",
        "print(f\"Test accuracy: {test_accuracy:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWmOXXjH-qRy"
      },
      "source": [
        "## Making Predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code demonstrates how to use the trained Transformer-based model to make predictions on a set of sample texts. It involves vectorizing the texts and then obtaining predictions from the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zXX_B28y3I5",
        "outputId": "7e24bba4-3f27-4a99-cc2c-39a04d835b8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n"
          ]
        }
      ],
      "source": [
        "# Make predictions on some sample texts\n",
        "sample_texts = [\n",
        "    \"The stock market reached new highs today as tech companies reported strong earnings.\",\n",
        "    \"The national soccer team won the World Cup after a thrilling final match.\",\n",
        "    \"Scientists discover a new species of deep-sea creature in the Pacific Ocean.\",\n",
        "    \"Global leaders met to discuss climate change policies and renewable energy.\",\n",
        "]\n",
        "\n",
        "# Vectorize the sample texts\n",
        "sample_vectorized = vectorize_layer(sample_texts)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(sample_vectorized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5qauAYA-yLj"
      },
      "source": [
        "## Printing the Classification Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code snippet prints the predicted class for each sample text based on the model's predictions. It interprets the model's output and displays the corresponding news category for each article."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbOaJiIKc1RV",
        "outputId": "85c37100-8dd9-4a9b-ab1a-c61495c3b03c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: The stock market reached new highs today as tech companies reported strong earnings.\n",
            "Predicted class: Business\n",
            "\n",
            "Text: The national soccer team won the World Cup after a thrilling final match.\n",
            "Predicted class: Sports\n",
            "\n",
            "Text: Scientists discover a new species of deep-sea creature in the Pacific Ocean.\n",
            "Predicted class: Sci/Tech\n",
            "\n",
            "Text: Global leaders met to discuss climate change policies and renewable energy.\n",
            "Predicted class: Sci/Tech\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print the results\n",
        "classes = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
        "for text, pred in zip(sample_texts, predictions):\n",
        "    predicted_class = classes[np.argmax(pred)]\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Predicted class: {predicted_class}\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "3QsiFwHs6sFO",
        "P7zrVI3dJxYL",
        "WoF8YZ1_Kecf",
        "ogW2dg4fLAoh",
        "arPYG3BYMIj8",
        "WldRHVd-MaP1",
        "r9BUnCf6Mo3O",
        "_ULfDxryNGwQ",
        "O76O1RljNqHN",
        "gNyIQYSMN6sk"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

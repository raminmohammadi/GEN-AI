{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Attention Mechanism Assignment (Graded): Text Classification with AG News Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to your programming assignment on Self Attention Mechanism! You will build a Deep Learning Model with Self Attention Mechanism for text classification on the AG News Dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this assignment, you will explore and implement self-attention mechanisms for text classification using the AG News dataset.\n",
    "\n",
    "- Your goal is to build and train a text classification model that utilizes self-attention to categorize news articles into predefined topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The AG News dataset is a collection of news articles categorized into four classes:\n",
    "    - World\n",
    "\n",
    "    - Sports\n",
    "    - Business\n",
    "    - Sci/Tech\n",
    "\n",
    "- The dataset consists of:\n",
    "    - 120,000 training examples\n",
    "    - 7,600 test examples\n",
    "\n",
    "- Each example in the dataset contains:\n",
    "    - Text: The news article text\n",
    "    - Label: An integer (0-3) representing the category of the news article\n",
    "\n",
    "- For more information about the AG News dataset, you can visit the following link: [AG News Dataset](https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Data Preparation and Exploration**\n",
    "    - Explore the dataset structure, including the number of samples, class distribution, and text length statistics.\n",
    "    \n",
    "    - Implement a function to preprocess the text data, including tokenization and padding.\n",
    "\n",
    "2. **Implement a Self-Attention Layer**\n",
    "    - Create a custom Keras layer that implements the self-attention mechanism.\n",
    "\n",
    "    - The layer should take a sequence of word embeddings as input and output attention-weighted representations.\n",
    "\n",
    "    - Implement the attention score calculation and softmax normalization.\n",
    "\n",
    "3. **Build the Text Classification Model**\n",
    "\n",
    "    - Design a neural network architecture that incorporates the self-attention layer.\n",
    "    \n",
    "    - The model should include an embedding layer, the self-attention layer, and output layers for classification.\n",
    "    \n",
    "    - Compile the model with appropriate loss function and optimizer.\n",
    "\n",
    "4. **Train and Evaluate the Model**\n",
    "    - Split the training data into training and validation sets.\n",
    "\n",
    "    - Train the model on the training set and monitor its performance on the validation set.\n",
    "\n",
    "    - Implement early stopping to prevent overfitting.\n",
    "\n",
    "    - Evaluate the trained model on the test set and report accuracy and other relevant metrics.\n",
    "\n",
    "5. **Prediction and Interpretation**\n",
    "   - Use the trained model to make predictions on new, unseen reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Only write code when you see any of the below prompts,\n",
    "\n",
    "    ```\n",
    "    # YOUR CODE GOES HERE\n",
    "    # YOUR CODE ENDS HERE\n",
    "    # TODO\n",
    "    ```\n",
    "\n",
    "- Do not modify any other section of the code unless tated otherwise in the comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "from helpers.methods import load_data, plot_results, detect_and_set_device\n",
    "from tests.test_methods import test_vectorize_text, test_self_attention, test_create_model, test_train_model, test_evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # load data\n",
    "train_data, test_data = load_data()\n",
    "\n",
    "# Split training data into train and validation sets\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "print(\"Data Spit into Train and Validation Sets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Let's get to know about our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the training data\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape of the dataset: Testing and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Number of samples\n",
    "train_samples = \n",
    "test_samples = \n",
    "\n",
    "print(\"Dataset Structure:\")\n",
    "print(f\"Number of training samples: {train_samples}\")\n",
    "print(f\"Number of test samples: {test_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Class distribution\n",
    "class_distribution = \n",
    "print(\"\\nClass distribution:\")\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "class_distribution.plot(kind='bar')\n",
    "plt.title('Class Distribution in Training Set')\n",
    "plt.xlabel('Class Label')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text length statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length statistics\n",
    "text_length_stats = \n",
    "\n",
    "print(\"\\nText length statistics:\")\n",
    "print(text_length_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing text length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize text length distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "train_data['text_length'].hist(bins=50)\n",
    "plt.title('Distribution of Text Lengths in Training Set')\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Hints:**\n",
    "\n",
    "Complete the vectorize_text function for preprocessing the text data.\n",
    "\n",
    "- Use the TextVectorization layer from TensorFlow/Keras to convert the text data into sequences of integers.\n",
    "\n",
    "- Set the maximum number of words in the vocabulary to 20,000 using the max_tokens parameter in TextVectorization.\n",
    "\n",
    "- Set the maximum length for the sequences to 100 using the output_sequence_length parameter in TextVectorization.\n",
    "\n",
    "- Adapt the vectorization layer to the training data using the adapt method, passing in the 'Text' column values from the train_data DataFrame.\n",
    "\n",
    "- Return the configured vectorize_layer for use in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Vectorization\n",
    "def vectorize_text(train_data, test_data):\n",
    "    max_features = \n",
    "    sequence_length = \n",
    "    \n",
    "    # YOUR CODE GOES HERE\n",
    "    vectorize_layer\n",
    "\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    test_vectorize_text(vectorize_layer)\n",
    "    \n",
    "    return vectorize_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Building the Self-Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Hints:**\n",
    "\n",
    "Implement the SelfAttention class as a custom layer in TensorFlow/Keras.\n",
    "\n",
    "- Create a class that inherits from tf.keras.layers.Layer.\n",
    "\n",
    "- In the __init__ method:\n",
    "  - Initialize the parent class using super().\n",
    "  - Create three Dense layers: W1 and W2 with the specified number of units, and V with one unit.\n",
    "\n",
    "- Implement the call method to perform the self-attention mechanism:\n",
    "  - Calculate the attention scores using the W1, W2, and V layers.\n",
    "  - Apply softmax to get attention weights.\n",
    "  - Use the attention weights to create a context vector.\n",
    "  - Ensure the output shape is (batch_size, embedding_dim).\n",
    "\n",
    "- Add comments to explain the shape of tensors at each step of the attention mechanism.\n",
    "\n",
    "- Consider implementing a get_config method for serialization if you plan to save the model.\n",
    "\n",
    "- Test the layer with sample input to ensure it produces the expected output shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-Attention Layer\n",
    "class SelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # TODO: inputs shape: (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        \n",
    "        # TODO: score shape: (batch_size, seq_len, 1)\n",
    "        \n",
    "        \n",
    "        # TODO: attention_weights shape: (batch_size, seq_len, 1)\n",
    "        \n",
    "        \n",
    "        # TODO: context_vector shape: (batch_size, embedding_dim)\n",
    "        \n",
    "        \n",
    "        return context_vector\n",
    "    \n",
    "test_self_attention(SelfAttention(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Hints:**\n",
    "\n",
    "Complete the create_model function to build the text classification model with self-attention.\n",
    "\n",
    "- Use the Sequential API from Keras to create the model.\n",
    "\n",
    "- Add the following layers in order:\n",
    "  1. The vectorize_layer as the first layer to process input text.\n",
    "  2. An Embedding layer with 20,000 input dimensions and 128 output dimensions.\n",
    "  3. A Bidirectional LSTM layer with 64 units, setting return_sequences=True.\n",
    "  4. The custom SelfAttention layer with 64 units.\n",
    "  5. A Dense layer with 64 units and ReLU activation.\n",
    "  6. A Dropout layer with a rate of 0.5 for regularization.\n",
    "  7. A final Dense layer with 4 units (one for each class) and softmax activation.\n",
    "\n",
    "- Compile the model with:\n",
    "  - The sparse categorical crossentropy loss function.\n",
    "  - The Adam optimizer.\n",
    "  - Accuracy as the metric to monitor.\n",
    "\n",
    "- Return the compiled model.\n",
    "\n",
    "- Consider adding a summary() call to print the model architecture for verification.\n",
    "\n",
    "- Ensure that the input and output shapes of each layer are compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "def create_model(vectorize_layer):\n",
    "    embedding_dim = \n",
    "    \n",
    "    # YOUR CODE GOES HERE\n",
    "\n",
    "\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    test_create_model(model)\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Hints:**\n",
    "\n",
    "Complete the train_model function to train the text classification model with early stopping.\n",
    "\n",
    "- Set up an EarlyStopping callback:\n",
    "  - Monitor the validation loss ('val_loss').\n",
    "  - Set the patience to 3 epochs.\n",
    "  - Enable restoring the best weights.\n",
    "\n",
    "- Use the model.fit method to train the model:\n",
    "  - Pass the training data (train_data['Text']) and labels (train_data['Class'] - 1).\n",
    "  - Provide validation data using the validation_data parameter.\n",
    "  - Set the number of epochs to 20.\n",
    "  - Use a batch size of 32.\n",
    "  - Include the early stopping callback in the callbacks list.\n",
    "\n",
    "- Remember to subtract 1 from the class labels to make them 0-indexed, as required by the loss function.\n",
    "\n",
    "- Return the training history object for later analysis and visualization.\n",
    "\n",
    "- Consider adding a verbose parameter to control the output during training.\n",
    "\n",
    "- Ensure that the input data types match what the model expects (text data for inputs, integer labels for targets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train_model(model, train_data, val_data, runtime_device):\n",
    "    # TODO: Define early stopping callback\n",
    "    \n",
    "    with tf.device('/' + runtime_device + ':0'):\n",
    "        # YOUR CODE GOES HERE\n",
    "        history = \n",
    "\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "        )\n",
    "        \n",
    "    test_train_model(history)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Hints:**\n",
    "\n",
    "Complete the evaluate_model function to assess the performance of the trained model on the test data.\n",
    "\n",
    "- Use model.evaluate to compute the loss and accuracy on the test set:\n",
    "  - Pass the test data (test_data['Text']) and labels (test_data['Class'] - 1).\n",
    "  - Remember to subtract 1 from the class labels to make them 0-indexed.\n",
    "  - Print the test accuracy with 4 decimal places.\n",
    "\n",
    "- Generate predictions using model.predict:\n",
    "  - Use the test data (test_data['Text']) as input.\n",
    "  - Convert the predicted probabilities to class labels using np.argmax.\n",
    "\n",
    "- Create a classification report:\n",
    "  - Use sklearn's classification_report function.\n",
    "  - Compare the true labels (test_data['Class'] - 1) with the predicted classes.\n",
    "  - Print the classification report, which includes precision, recall, and F1-score for each class.\n",
    "\n",
    "- Return the predicted classes for further analysis or visualization.\n",
    "\n",
    "- Consider adding additional evaluation metrics if needed, such as confusion matrix or ROC AUC score.\n",
    "\n",
    "- Ensure that the input data types match what the model expects (text data for inputs, integer labels for targets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "def evaluate_model(model, test_data):\n",
    "    \n",
    "    # TODO: Evaluate the model on the test data\n",
    "    accuracy, loss = \n",
    "    print(f\"Test accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # TODO: Make predictions on the test data\n",
    "    \n",
    "    # TODO: Convert the predictions to class labels\n",
    "\n",
    "    \n",
    "    print(classification_report(test_data['Class'] - 1, predicted_classes))\n",
    "    \n",
    "    test_evaluate_model(predicted_classes)\n",
    "    \n",
    "    return predicted_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Driver code to run the built pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------- Do not change the code below ----------------#\n",
    "\n",
    "def main():\n",
    "    # Set device\n",
    "    device = detect_and_set_device()\n",
    "    # Load and preprocess data\n",
    "    train_data, test_data = load_data()\n",
    "    \n",
    "    # Split training data into train and validation sets\n",
    "    train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create vectorization layer\n",
    "    vectorize_layer = vectorize_text(train_data, test_data)\n",
    "    \n",
    "    # Create and compile model\n",
    "    model = create_model(vectorize_layer)\n",
    "    \n",
    "    # Train model\n",
    "    history = train_model(model, train_data, val_data, device)\n",
    "    \n",
    "    # Evaluate model\n",
    "    predicted_classes = evaluate_model(model, test_data)\n",
    "    \n",
    "    return history, test_data, predicted_classes\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    history, test_data, predicted_classes = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------- Do not change the code below ----------------#\n",
    "# Run this cell to visualize results\n",
    "plot_results(history, test_data, predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Perceptron\n",
        "\n",
        "A **Perceptron** is a simple artificial neural network unit that takes multiple inputs, performs a weighted sum of these inputs, and applies a threshold function to produce an output.\n",
        "\n",
        "It's essentially a basic building block of neural networks, serving as a simplified model of a biological neuron.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=18Ar9x38STGTjuRCzJe90Oha6cDmFhE3J\" width=\"400\">\n",
        "\n",
        "**Inputs**: A neuron takes multiple inputs, often represented as x\n",
        "1\n",
        "​\n",
        " ,x\n",
        "2\n",
        "​\n",
        " ,…,x\n",
        "n\n",
        "​\n",
        " , each of which is associated with a weight w\n",
        "1\n",
        "​\n",
        " ,w\n",
        "2\n",
        "​\n",
        " ,…,w\n",
        "n\n",
        "​\n",
        " . A bias term b is usually added as well.\n",
        "\n",
        "**Weighted Sum**: The neuron calculates a weighted sum of the inputs z, using the formula:\n",
        "\n",
        "$$\n",
        "z = \\sum_{i=1}^{n} w_i x_i + b\n",
        "$$\n",
        "\n",
        "**Activation Function**: The neuron then applies an activation function to this weighted sum to introduce non-linearity into the model.\n",
        "$$\n",
        "a = f(z)\n",
        "$$\n",
        "\n",
        "Common activation functions include the sigmoid, ReLU, and tanh functions.\n",
        "\n",
        "**Output**: The result, α  is the neuron's output, which is passed to the next layer in the network."
      ],
      "metadata": {
        "id": "nEjJ7raoDTu_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perceptron implementation using TensorFlow on synthetic dataset\n",
        "\n",
        "Lets create synthetic classification data using [`sklearn.datasets.make_classification`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) and train a perceptron classifier model for 50 epochs.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qDLZzhffGmvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_classification(n_samples=10000, n_features=20, n_classes=2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "xsgkjXfIuzqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the dataset\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(X[:, 4], X[:, 1], c=y, cmap='viridis')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Visualization of Classes')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mE6p_l6WvFds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "j3VawA4uu1iR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and train the perceptron\n",
        "perceptron = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(20,))\n",
        "])"
      ],
      "metadata": {
        "id": "BDYaoRz0yD1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important Arguments**\n",
        "* `Dense` layer represents the perceptron\n",
        "* `sigmoid` activation function is used to produce output values between 0 and 1."
      ],
      "metadata": {
        "id": "obEnXx_gx1jb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilation and training\n",
        "perceptron.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "perceptron_history = perceptron.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n"
      ],
      "metadata": {
        "id": "hh24ikoWu3OM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important Arguments**\n",
        "\n",
        "* `Adam optimizer` is used to update weights during training. It maintains moving averages of the first and second moments of gradients.\n",
        "* `Binary_crossentropy` cost function is used to measure the model's error. It calculates the difference between the predicted probabilities and the true binary labels. It penalizes incorrect predictions more heavily.\n",
        "* `accuracy` is used to validate the performace of the model."
      ],
      "metadata": {
        "id": "fhHyXh0vyGzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on the test set\n",
        "perceptron_loss, perceptron_accuracy = perceptron.evaluate(X_test_scaled, y_test, verbose=0)"
      ],
      "metadata": {
        "id": "BMqoPUK-yew8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Perceptron Test Accuracy: {perceptron_accuracy:.4f}\")\n",
        "print(f\"Perceptron Test Loss: {perceptron_loss:.4f}\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "\n",
        "plt.plot(perceptron_history.history['accuracy'], label='Perceptron Training')\n",
        "plt.plot(perceptron_history.history['val_accuracy'], label='Perceptron Validation')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(perceptron_history.history['loss'], label='Perceptron Training')\n",
        "plt.plot(perceptron_history.history['val_loss'], label='Perceptron Validation')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GQAnvYQ-u7FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "kuTEkmbZN7v9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Networks\n",
        "\n",
        "A **neural network** is created by connecting several nodes so that the output of some nodes serves as the input\n",
        "to others.\n",
        "In a neural network, the layers of nodes are organized into three main types: the input layer, the hidden\n",
        "layer, and the output layer. The input layer consists of nodes that represent the input features of the data,\n",
        "and it also includes special bias units that always output a value of +1. <br><br> The hidden layer is where the\n",
        "network processes the information through weights and biases to learn patterns. This layer’s nodes are not\n",
        "directly observed in the data but play a crucial role in transforming the inputs into meaningful outputs.\n",
        "Finally, the output layer provides the final prediction or result of the network. In our example, there are 3\n",
        "input nodes, 3 hidden nodes, and 1 output node.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1ZNF4e_DWy3hZhZRKKB7orJPudD8kOx4t\" width=400>\n",
        "\n",
        "**Basic Architecture:**\n",
        "* **Input Layer:** The first layer receives the input data, which can be images, text, numerical data, or any other suitable format.\n",
        "* **Hidden Layers:** These layers process the input data and extract relevant features. They can be multiple layers, each with a different number of neurons.\n",
        "* **Output Layer:** The final layer produces the output, which can be a classification, regression, or other desired result.\n",
        "\n",
        "**Key Components:**\n",
        "* **Neurons:** The fundamental units of a neural network, each representing a simple computational unit.\n",
        "* **Weights:** Numerical values that determine the strength of connections between neurons.\n",
        "* **Biases:** Additional parameters that adjust the output of a neuron.\n",
        "Activation Functions: Non-linear functions that introduce complexity and enable the network to learn complex patterns."
      ],
      "metadata": {
        "id": "O5JkMTe_N98k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Networks implementation using TensorFlow on synthetic dataset\n",
        "\n",
        "A neural network with 3 hidden layers is defined and trained for 50 epochs.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZS1S2nlyJ_5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and train the neural network\n",
        "neural_network = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(20,)), # 1 hidden layer with 64 neurons\n",
        "    tf.keras.layers.Dense(32, activation='relu'), # 1 hidden layer with 32 neurons\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid') # Output layer with 1 neuron\n",
        "])"
      ],
      "metadata": {
        "id": "NwOto9_ixD51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neural_network.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "nn_history = neural_network.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n"
      ],
      "metadata": {
        "id": "Vtsm4Bt7xFBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test set\n",
        "nn_loss, nn_accuracy = neural_network.evaluate(X_test_scaled, y_test, verbose=0)\n"
      ],
      "metadata": {
        "id": "ZA074xhzxGkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Neural Network Test Accuracy: {nn_accuracy:.4f}\")\n",
        "print(f\"Neural Network Test Loss: {nn_loss:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(nn_history.history['accuracy'], label='Neural Network Training')\n",
        "plt.plot(nn_history.history['val_accuracy'], label='Neural Network Validation')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(nn_history.history['loss'], label='Neural Network Training')\n",
        "plt.plot(nn_history.history['val_loss'], label='Neural Network Validation')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4QA_r8_KxMo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Performance Comparison\n",
        "\n",
        "1. **Model Complexity**: The neural network's superior performance can be attributed to its higher complexity. With multiple layers, it can learn more intricate patterns in the data that a single-layer perceptron might miss.\n",
        "\n",
        "2. **Generalization**: The neural network's ability to achieve both higher accuracy and lower loss on the test set indicates better generalization to unseen data.\n",
        "\n",
        "3. **Non-linear Decision Boundary**: The neural network can create non-linear decision boundaries, allowing it to separate classes more effectively in complex datasets.\n",
        "\n",
        "4. **Feature Extraction**: The hidden layers in the neural network can automatically learn and extract relevant features from the input data, which contributes to its improved performance.\n",
        "\n",
        "5. **Marginal Improvement**: While the neural network performs better, the perceptron still achieves a respectable accuracy of 89.30%. This suggests that the classification task might not be extremely complex, as even a simple model performs well.\n",
        "\n",
        "\n",
        "The neural network demonstrates superior performance in both accuracy and loss metrics. However, the perceptron's performance is also quite good, indicating that the problem may be largely linearly separable. The choice between these models would depend on factors such as:\n",
        "\n",
        "- The complexity of the dataset\n",
        "- Computational resources available\n",
        "- The need for interpretability (perceptrons are generally more interpretable)\n",
        "- The required level of accuracy for the specific application\n",
        "\n",
        "For tasks where marginal improvements in accuracy are crucial, the neural network would be the preferred choice. However, if simplicity and faster training are priorities, and the slight decrease in accuracy is acceptable, the perceptron could be a suitable option."
      ],
      "metadata": {
        "id": "nHUyInc7dAA3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Networks implementation on `MNIST` data using TensorFlow\n",
        "\n",
        "## MNIST Dataset\n",
        "\n",
        "The MNIST (Modified National Institute of Standards and Technology) database is a widely used dataset in the field of machine learning,\n",
        "specifically for image classification tasks. It consists of 60,000 training images and 10,000 testing images, each 28x28 grayscale images of **handwritten digits** from 0 to 9. ([learn more](https://colah.github.io/posts/2014-10-Visualizing-MNIST/))\n",
        "\n",
        "<img src='https://miro.medium.com/v2/resize:fit:1280/format:webp/1*B9pCFLFsx50PGaCYy2U_sw.gif' width=300>\n",
        "\n",
        "## Problem\n",
        "Now the idea is to input each handwritten digit to the neural network and classify what that image of the hand written digit signifies.\n",
        "\n",
        "<img src='https://miro.medium.com/v2/resize:fit:1160/format:webp/0*u5-PcKYVfUE5s2by.gif' width=400>\n",
        "\n",
        "As the above image suggests,\n",
        "* Each image of the handwritten digit is **flattened** first. That is,\n",
        "  * **Image shape:** (28, 28, 1)\n",
        "  * **After Flattening:** (784, 1)\n",
        "  * We are simply multiplying the first 2 dimensions of the image.\n",
        "* Now this flattened image is passed down to the feed forward neural network through the hidden layers and finally outputs the result between 0 to 9.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jmQZJUlule_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and preprocess the dataset"
      ],
      "metadata": {
        "id": "Fytw8oaADMr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "hfY5qZ8NCUZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "iEja82t9CaJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalization\n",
        "\n",
        "The pixel values are **normalized** to the range\n",
        "[0,1] by dividing by 255.\n",
        "\n",
        "* **Without normalization**, the network would see input features (pixel values) that vary significantly in scale (some very high, others low).\n",
        "* When features have large values, they can cause larger weight updates during backpropagation, leading to a very erratic optimization process."
      ],
      "metadata": {
        "id": "KTxdOFg7uZ1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the pixel values to be between 0 and 1\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0"
      ],
      "metadata": {
        "id": "25RTufA4Ce_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the images to 1D vectors (28*28 = 784 features)\n",
        "x_train = x_train.reshape(-1, 28 * 28)\n",
        "x_test = x_test.reshape(-1, 28 * 28)\n"
      ],
      "metadata": {
        "id": "e9W_01TLChzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting labels to one hot vector?\n",
        "\n",
        "The digits 0,1,2,…,9 are categorical (i.e., different classes), not ordinal. If you use the raw label numbers (e.g., 0, 1, 2, etc.) directly, the model may assume there's a numerical relationship between them, like 9 is greater than 0 or 2 is less than 3. This is not true in classification tasks.\n",
        "\n",
        "One-hot encoding prevents the model from assigning importance or relationships based on the magnitude of the class labels, by creating an independent binary vector for each class.\n",
        "\n",
        "**One hot encoding representation:**\n",
        "\n",
        "<img src='https://codecraft.tv/courses/tensorflowjs/neural-networks/mnist-training-data/img/exports/5.mnist.001.jpeg' width=500>\n",
        "\n",
        "Here:\n",
        "\n",
        "* The length of the vector is equal to the number of classes (10 in the case of MNIST).\n",
        "* The value 1 is placed in the position corresponding to the true class (9 in this case), and all other positions are 0.\n"
      ],
      "metadata": {
        "id": "66rpQepjqmUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to one-hot encoding\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n"
      ],
      "metadata": {
        "id": "WwmfArZ3Ck6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the model\n",
        "\n",
        "We shall be building a neural network using `tf.keras.Sequential`.\n",
        "* **Input:** 784 flattened features of the image\n",
        "* **Hidden layer 1:** 128 neurons\n",
        "* **Hidden layer 2:** 64 neurons\n",
        "* **Output:** 10 neurons"
      ],
      "metadata": {
        "id": "y1Pk2L7pCoKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Build the feedforward neural network model\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(28 * 28,)),  # Input layer (28x28 = 784 flattened image)\n",
        "    layers.Dense(128, activation='relu'),  # Fully connected layer with 128 neurons\n",
        "    layers.Dense(64, activation='relu'),  # Another fully connected layer with 64 neurons\n",
        "    layers.Dense(10, activation='softmax')  # Output layer with 10 neurons for 10 classes\n",
        "])\n"
      ],
      "metadata": {
        "id": "QNadah0mCK_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "1laiDEwG0u5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compiling the model"
      ],
      "metadata": {
        "id": "iRMq7BNsC8U_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Z84GScNoCMnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model"
      ],
      "metadata": {
        "id": "iPxBl8vYDAKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Train the model\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.1)"
      ],
      "metadata": {
        "id": "nkPYiPTQCMpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the model"
      ],
      "metadata": {
        "id": "oKdJq0sZDFap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Evaluate the model\n",
        "\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc}')"
      ],
      "metadata": {
        "id": "mZWVJpbpCMra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting training & validation accuracy values\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# Plotting training & validation loss values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IRt3Y4zqCMtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pBKGRPqqCMyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YnXgwJKvCNXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "miLcBE_pVZsK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
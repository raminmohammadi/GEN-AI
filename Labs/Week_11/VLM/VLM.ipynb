{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUgRVUfXQKl4"
      },
      "source": [
        "<h1>\n",
        "Vision Language Models(VLMs)\n",
        "</h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDUpAOnYQQSo"
      },
      "source": [
        "# Brief Recap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxHI6AdUQRdh"
      },
      "source": [
        "The vision language models are multimodal models that can learn both from images and text.\n",
        "The journey began with the adaptation of language models for visual tasks. Initial models such as **[visual-BERT](https://arxiv.org/abs/1908.03557)** and **[ViLBERT](https://arxiv.org/abs/1908.02265)** were inspired by the success of BERT in natural language processing. These models combined textual and visual tokens and were trained using dual objectives: a masked modeling task and a sentence-image prediction task. This led to improved performance in tasks such as visual question answering and image captioning.\n",
        "\n",
        "These models can output bounding boxes or segmentation masks when prompted to detect or segment a particular subject, or they can localize different entities or answer questions about their relative or absolute positions. There’s a lot of diversity within the existing set of large vision language models, the data they were trained on, how they encode images, and, thus, their capabilities.\n",
        "\n",
        "\n",
        "<img src='assets/vlm-1.jpg' width=500>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJBQQrOzTLoi"
      },
      "source": [
        "#### **Overview of some of the open source VLMs**\n",
        "\n",
        "Some of the most prominent VLMs are shown in the table below.\n",
        "\n",
        "* There are base models, and models fine-tuned for chat that can be used in conversational mode.\n",
        "* Some of these models have a feature called “grounding” which reduces model hallucinations.\n",
        "* All models are trained on English unless stated otherwise.\n",
        "\n",
        "\n",
        "| Model              | Permissive License | Model Size | Image Resolution | Additional Capabilities               |\n",
        "|--------------------|--------------------|------------|------------------|---------------------------------------|\n",
        "| [LLAVA 1.6 (Hermes 34B)](https://huggingface.co/llava-hf/llava-v1.6-34b-hf) | ✅                 | 34B        | 672x672          |                                       |\n",
        "| [deepseek-vl-7b-base](https://huggingface.co/deepseek-ai/deepseek-vl-7b-base)   | ✅                 | 7B         | 384x384          |                                       |\n",
        "| [DeepSeek-VL-Chat](https://huggingface.co/deepseek-ai/deepseek-vl-7b-chat)      | ✅                 | 7B         | 384x384          | Chat                                  |\n",
        "| [moondream2](https://huggingface.co/vikhyatk/moondream2)            | ✅                 | ~2B        | 378x378          |                                       |\n",
        "| [CogVLM-base](https://huggingface.co/THUDM/cogvlm-base-490-hf)           | ✅                 | 17B        | 490x490          |                                       |\n",
        "| [CogVLM-Chat](https://huggingface.co/THUDM/cogvlm-chat-hf)           | ✅                 | 17B        | 490x490          | Grounding, chat                       |\n",
        "| [Fuyu-8B](https://huggingface.co/adept/fuyu-8b)               | ❌                 | 8B         | 300x300          | Text detection within image           |\n",
        "| [KOSMOS-2](https://huggingface.co/microsoft/kosmos-2-patch14-224)              | ✅                 | ~2B        | 224x224          | Grounding, zero-shot object detection |\n",
        "| [Qwen-VL](https://huggingface.co/Qwen/Qwen-VL)               | ✅                 | 4B         | 448x448          | Zero-shot object detection            |\n",
        "| [Qwen-VL-Chat](https://huggingface.co/Qwen/Qwen-VL-Chat)          | ✅                 | 4B         | 448x448          | Chat                                  |\n",
        "| [Yi-VL-34B](https://huggingface.co/01-ai/Yi-VL-34B)             | ✅                 | 34B        | 448x448          | Bilingual (English, Chinese)          |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWQGPeAmYLMq"
      },
      "source": [
        "# Architecture\n",
        "\n",
        "<img src='assets/vlm-2.png' width=400>\n",
        "\n",
        "This shows a typical architecture for a Vision-Language Model (VLM), designed to process both image and text inputs for tasks that involve multimodal data. Here's a step-by-step breakdown of each component:\n",
        "\n",
        "1. **Image Input** and **Text Input**: These are the two types of input data—visual (image) and textual.\n",
        "\n",
        "2. **Image Encoder** and **Text Encoder**:\n",
        "   - The **Image Encoder** processes the image input and extracts features, typically using a convolutional neural network (CNN) or a vision transformer (ViT). It transforms the image data into a lower-dimensional representation.\n",
        "   - The **Text Encoder** processes the text input, usually with a transformer-based model like BERT, which encodes the text into a feature vector.\n",
        "\n",
        "3. **Joint Embedding Space**:\n",
        "   - The outputs from the Image Encoder and Text Encoder are projected into a **Joint Embedding Space** where both modalities are aligned. This is a common representation space for both image and text, allowing the model to compare, fuse, or relate features across modalities.\n",
        "   - The alignment in this space helps the model to understand the relationships between visual and textual data.\n",
        "\n",
        "4. **Fusion Layer**:\n",
        "   - This layer integrates the representations from the Joint Embedding Space. The fusion can be done using techniques like concatenation, attention mechanisms, or specialized fusion networks to combine and refine information from both image and text data.\n",
        "\n",
        "5. **Task-Specific Layers**:\n",
        "   - These layers are designed to handle the specific task the model is trained for, such as image captioning, visual question answering, or image-text retrieval.\n",
        "   - The layers may vary depending on the task but typically involve a few fully connected layers, transformers, or other task-appropriate structures.\n",
        "\n",
        "6. **Output**:\n",
        "   - The final output layer produces the result based on the task. For example, in an image captioning task, this might be a sequence of words. For a classification task, it might be a single label.\n",
        "\n",
        "This VLM architecture allows the model to take advantage of both image and text data, making it useful for applications that require an understanding of both modalities.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkFEkfkHaQtS"
      },
      "source": [
        "# Use cases\n",
        "\n",
        "Vision-Language Models (VLMs) offer a wide range of applications across various domains thanks to their ability to connect visual information with natural language. Some of the prominent use cases include:\n",
        "\n",
        "1. **Visual Question Answering (VQA)**: VLMs enable users to ask questions about an image and receive relevant answers. This task requires the model to correctly interpret the visual content and relate it to the textual query.\n",
        "\n",
        "2. **Image Captioning**: VLMs can generate descriptive captions for images, summarizing their content in natural language. This is particularly useful for accessibility applications, enabling visually impaired users to understand visual information.\n",
        "\n",
        "3. **Image-Text Retrieval**: VLMs facilitate searching for images using natural language descriptions and vice versa. This has applications in stock photography, content management, and improving the accessibility of visual information.\n",
        "\n",
        "4. **Document Understanding**: VLMs can help analyze and extract information from documents that combine text and images, automating tasks like form filling, data extraction, and summarizing content.\n",
        "\n",
        "5. **Automatic Content Moderation**: In social media and online platforms, VLMs can detect and classify inappropriate or harmful content in images and videos, enhancing user safety and content regulation.\n",
        "\n",
        "6. **Robotics**: VLMs can empower robots to understand visual instructions or describe their environment, leading to more advanced human-robot interaction and improved task automation.\n",
        "\n",
        "7. **Enhanced Product Discovery in E-commerce**: VLMs allow users to perform searches using images or complex descriptions, thus improving the shopping experience and facilitating better product recommendations.\n",
        "\n",
        "8. **Assistive Technologies**: VLMs can be integrated into applications aimed at helping people with disabilities by providing interactive and informative experiences that rely on visual and textual data.\n",
        "\n",
        "These use cases showcase the versatility and potential of VLMs in transforming how we interact with visual and textual information across several industries [[1](https://medium.com/@navendubrajesh/vision-language-models-use-cases-ee6d54b2c557)][[3](https://huggingface.co/blog/vision_language_pretraining)][[6](https://viso.ai/deep-learning/vision-language-models/)]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfyOEN7AbTa9"
      },
      "source": [
        "# **Interacting with VLMs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PnFSo8hRk3e"
      },
      "source": [
        "## **BLIP**\n",
        "\n",
        "\n",
        "**BLIP** stands for \"Bootstrapped Language-Image Pre-training.\" It's a framework designed to improve the way models understand and generate text and images together. BLIP is used for tasks that involve both visual and textual data, such as generating captions for images or answering questions about images.\n",
        "\n",
        "\n",
        "<img src='assets/blip-arch.png' width=500>\n",
        "\n",
        "The BLIP model consists of four main components: Image Encoder, Text Encoder, Image-Grounded Text Encoder, and Image-Grounded Text Decoder.\n",
        "* The Image Encoder processes the input image and extracts visual features.\n",
        "* The Text Encoder processes the input text and extracts textual features.\n",
        "* The Image-Grounded Text Encoder fuses the visual and textual features to create a joint representation.\n",
        "* The Image-Grounded Text Decoder generates text outputs based on the fused representation.\n",
        "* The BLIP model is trained in a self-supervised manner, learning from unlabeled data through Image-Text Matching, Image Captioning, and Text-Image Retrieval.\n",
        "\n",
        "\n",
        "**Key features of BLIP include:**\n",
        "\n",
        "* **Cross-modal Learning:** BLIP integrates visual and textual data to enhance comprehension and generation tasks that require understanding both modes simultaneously.\n",
        "\n",
        "* **Pre-training and Fine-tuning:** Similar to models like BERT or GPT for text, BLIP uses a pre-training phase where it learns from a large dataset of images and text. After this, it can be fine-tuned for specific tasks, which improves its effectiveness.\n",
        "\n",
        "* **Scalability and Efficiency:** BLIP is designed to be scalable across different tasks, allowing for efficient training times and performance improvements on various datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK1QyNLhuJbL"
      },
      "source": [
        "### **Experiments**\n",
        "\n",
        "We are going to perform the following experiments using task specific pretrained BLIP model:\n",
        "\n",
        "* Image captioning\n",
        "* Visual Question Answering\n",
        "* Image-Text Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXWx5SQi5NV2"
      },
      "source": [
        "#### **Image Captioning**\n",
        "\n",
        "BLIP Model for image captioning. The model consists of a vision encoder and a text decoder. One can optionally pass input_ids to the model, which serve as a text prompt, to make the text decoder continue the prompt. Otherwise, the decoder starts generating text from the [BOS] (beginning-of-sequence) token. will start generating the caption from the text input. If no text input is provided, the decoder will start with the [BOS] token only.\n",
        "\n",
        "\n",
        "```python\n",
        "class transformers.TFBlipForConditionalGeneration\n",
        "```\n",
        "**Parameters:**\n",
        "\n",
        "  * **pixel_values** (tf.Tensor of shape (batch_size, num_channels, height, width)) — Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using BlipImageProcessor. See BlipImageProcessor.call() for details.\n",
        "  * **output_attentions** (bool, optional) — Whether or not to return the attentions tensors of all attention layers. See attentions under returned tensors for more detail.\n",
        "  * **output_hidden_states** (bool, optional) — Whether or not to return the hidden states of all layers. See hidden_states under returned tensors for more detail.\n",
        "  * **return_dict** (bool, optional) — Whether or not to return a ModelOutput instead of a plain tuple.\n",
        "\n",
        "  * **loss** (tf.Tensor, optional, returned when labels is provided, tf.Tensor of shape (1,)) — Languge modeling loss from the text decoder.\n",
        "\n",
        "  * **logits** (tf.Tensor of shape (batch_size, sequence_length, config.vocab_size), optional) — Prediction scores of the language modeling head of the text decoder model.\n",
        "\n",
        "  * **image_embeds** (tf.Tensor of shape (batch_size, output_dim), optional) — The image embeddings obtained after applying the Vision Transformer model to the input image.\n",
        "\n",
        "  * **last_hidden_state** (tf.Tensor of shape (batch_size, sequence_length, hidden_size), optional) — Sequence of hidden-states at the output of the last layer of the model.\n",
        "\n",
        "  * **hidden_states** (tuple(tf.Tensor), optional, returned when output_hidden_states=True) — Tuple of tf.Tensor (one for the output of the embeddings, if the model has an embedding layer, + one for the output of each layer) of shape (batch_size, sequence_length, hidden_size).\n",
        "\n",
        "\n",
        "\n",
        "**Returns:**\n",
        "```python\n",
        "transformers.models.blip.modeling_tf_blip.TFBlipForConditionalGenerationModelOutput or tuple(tf.Tensor)\n",
        "```\n",
        "\n",
        "\n",
        "**Input image**\n",
        "\n",
        "<img src='assets/demo.jpg' width=500>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryirgsLAvcen"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from transformers import AutoProcessor, TFBlipForConditionalGeneration\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = TFBlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "image = Image.open(\"assets/demo.jpg\")\n",
        "text = \"a photography of\"\n",
        "\n",
        "# unconditional image captioning\n",
        "inputs = processor(images=image, text=text, return_tensors=\"tf\")\n",
        "\n",
        "outputs = model.generate(**inputs)\n",
        "print('========================== Outputs ==============================')\n",
        "print(processor.batch_decode(outputs, skip_special_tokens=True)[0])\n",
        "\n",
        "# unconditional image captioning\n",
        "inputs = processor(image, return_tensors=\"tf\")\n",
        "\n",
        "out = model.generate(**inputs)\n",
        "print(processor.decode(out[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g2m6izCxqfE"
      },
      "source": [
        "**Code Breakdown**\n",
        "\n",
        "1.  **Load the pre-trained model and processor**\n",
        "  * **Pretrained-model**\n",
        "      * Initially, we loaded the pre-trained BLIP base model that is specifically finetuned for image captioning.\n",
        "      * This means the model has already learned general image-text relationships from a large dataset. However, it's further fine-tuned specifically for image captioning, which makes it more effective at this task.\n",
        "  * **`AutoProcessor`**\n",
        "      * The `AutoProcessor` from transformers is a crucial component. It handles the following:\n",
        "\n",
        "        * **Image Preprocessing**: Resizing, normalization, and converting the image into a format suitable for the model.\n",
        "        * **Text Tokenization**: Breaking down the input text (if any) into individual tokens that the language model can understand.\n",
        "        * **Data Collation**: Combining the processed image and text into a single input format that the model expects.\n",
        "\n",
        "2. **Load the image**\n",
        "  * Next, we load the image from our local directory and open it using PIL library.\n",
        "\n",
        "3. **Conditional Image Captioning**\n",
        "  * You guide the caption generation process with a starting text prompt. This prompt biases the model towards specific aspects of the image or a desired style of caption.\n",
        "  * A starting text prompt (\"a photography of\") is provided.\n",
        "  * The processor prepares the image and text for the model.\n",
        "  * The model generates a caption based on the image and prompt.\n",
        "  * The generated caption is decoded and printed.\n",
        "\n",
        "4. **Unconditional Image Captioning**\n",
        "  * The model generates the caption solely based on the image content without any textual guidance.\n",
        "  * No text prompt is provided in this case.\n",
        "  * The model generates a caption based solely on the image.\n",
        "  * The generated caption is decoded and printed.\n",
        "\n",
        "5. **Caption Generation**\n",
        "\n",
        "  The core of the captioning process is the `model.generate()` function. This function uses the encoded image (and optionally the text prompt) to generate a sequence of tokens that form the caption. It involves complex internal mechanisms of the language model to predict the most likely words given the visual and textual context.\n",
        "\n",
        "6. **Decoding**\n",
        "\n",
        "  After generating the token sequence, processor.`batch_decode()` or `processor.decode()` is used to convert the tokens back into readable text, giving you the final caption. Special tokens are typically skipped during decoding to provide a cleaner output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t89f5kmQ5Zyb"
      },
      "source": [
        "#### **Visual Question Answering**\n",
        "\n",
        "```python\n",
        "class transformers.TFBlipForQuestionAnswering\n",
        "```\n",
        "\n",
        "Perform visual question answering using finetuned BLIP model.\n",
        "\n",
        "The model consists of a vision encoder, a text encoder as well as a text decoder. The vision encoder will encode the input image, the text encoder will encode the input question together with the encoding of the image, and the text decoder will output the answer to the question.\n",
        "\n",
        "**Parameters:**\n",
        "* Same as `TFBlipForConditionalGeneration` shown above.\n",
        "\n",
        "**Returns:**\n",
        "```python\n",
        "transformers.models.blip.modeling_tf_blip.TFBlipTextVisionModelOutput or tuple(tf.Tensor)\n",
        "```\n",
        "\n",
        "**Input Image**\n",
        "\n",
        "<img src='assets/cat_dog.jpg' width=500>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oF0PeMd5DXw"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor, TFBlipForQuestionAnswering\n",
        "\n",
        "model = TFBlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
        "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
        "\n",
        "image = Image.open('assets/cat_dog.jpg')\n",
        "\n",
        "# inference\n",
        "text = \"how many dogs are in the picture?\"\n",
        "inputs = processor(images=image, text=text, return_tensors=\"tf\")\n",
        "outputs = model.generate(**inputs)\n",
        "print('========================== Outputs ==============================')\n",
        "print(processor.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL-9tERi6FTj"
      },
      "source": [
        "**Code Breakdown**\n",
        "\n",
        "1. **Load the pre-trained model and processor**\n",
        "  \n",
        "  * **`TFBlipForQuestionAnswering`**\n",
        "  This is the BLIP model specifically fine-tuned for Visual Question Answering.\n",
        "  * **`AutoProcessor`**: This handles image preprocessing, text tokenization, and data collation for the model.\n",
        "2. **Load the image**\n",
        "\n",
        "  Next, load the image from the local directory and open it using PIL library.\n",
        "\n",
        "3. **Prepare the question**\n",
        "```\n",
        "text = \"how many dogs are in the picture?\":\n",
        "```\n",
        "This is the question you want to ask about the image.\n",
        "\n",
        "4. **Process the inputs**\n",
        "  ```\n",
        "  processor(images=image, text=text, return_tensors=\"tf\")\n",
        "  ```\n",
        "  The processor prepares the image and question by:\n",
        "    * Preprocessing the image (resizing, normalization).\n",
        "    * Tokenizing the question.\n",
        "    * Combining them into a format suitable for the model.\n",
        "    * Specifying return_tensors=\"tf\" ensures the output is a TensorFlow tensor.\n",
        "\n",
        "5. **Generate the answer**\n",
        "\n",
        "  outputs = model.generate(**inputs): This is where the magic happens. The model takes the processed image and question and generates an answer.\n",
        "  \n",
        "6. **Decode and print the answer**\n",
        "\n",
        "  print(processor.decode(outputs[0], skip_special_tokens=True)): This decodes the model's output (which is in token form) back into readable text and prints it. skip_special_tokens=True removes any unnecessary tokens from the output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLuGnD8ZtHmS"
      },
      "source": [
        "#### **Image-Text Retrieval**\n",
        "\n",
        "```python\n",
        "class transformers.TFBlipForImageTextRetrieval\n",
        "```\n",
        "\n",
        "BLIP Model with a vision and text projector, and a classification head on top. The model is used in the context of image-text retrieval. Given an image and a text, the model returns the probability of the text being relevant to the image.\n",
        "\n",
        "**Parameters:**\n",
        "* Same as `TFBlipForConditionalGeneration` shown above.\n",
        "\n",
        "**Returns:**\n",
        "```python\n",
        "transformers.models.blip.modeling_tf_blip.TFBlipImageTextMatchingModelOutput or tuple(tf.Tensor)\n",
        "```\n",
        "\n",
        "**Input Image**\n",
        "\n",
        "<img src='assets/cats.jpg' width=500>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESoYLOu5RnqF"
      },
      "outputs": [],
      "source": [
        "from transformers import BlipProcessor, TFBlipForImageTextRetrieval\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load model and processor\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-itm-base-coco\")\n",
        "model = TFBlipForImageTextRetrieval.from_pretrained(\"Salesforce/blip-itm-base-coco\")\n",
        "\n",
        "# Load image\n",
        "image = Image.open('assets/cats.jpg')\n",
        "text = \"an image of a cat\"\n",
        "\n",
        "# Process inputs\n",
        "inputs = processor(image, text, return_tensors=\"tf\")\n",
        "\n",
        "# Get similarity score\n",
        "outputs = model(**inputs)\n",
        "itm_score = tf.nn.sigmoid((outputs.itm_score[0][0])).numpy()\n",
        "print('========================== Outputs ==============================')\n",
        "print(f\"Image-Text Match Score: {itm_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwgvLl-TEHp0"
      },
      "source": [
        "**Code Breakdown**\n",
        "\n",
        "1. **Load the pre-trained model and processor:**\n",
        "\n",
        "  * `BlipProcessor`: Handles preprocessing of both image and text inputs.\n",
        "  * `TFBlipForImageTextRetrieval`: The BLIP model specifically fine-tuned for image-text retrieval.\n",
        "2. **Load the image:**\n",
        "\n",
        "  An image is loaded from the given image path using `PIL.Image`.\n",
        "3. **Prepare the text:**\n",
        "\n",
        "  A text description (\"an image of a cat\" in this case) is provided.\n",
        "\n",
        "4. **Process inputs:**\n",
        "\n",
        "  * The processor prepares the image and text by:\n",
        "  Preprocessing the image (resizing, normalization).\n",
        "  Tokenizing the text.\n",
        "  * Combining them into a format suitable for the model.\n",
        "  Specifying `return_tensors=\"tf\"` ensures the output is a TensorFlow tensor.\n",
        "\n",
        "5. **Get similarity score:**\n",
        "\n",
        "  The model takes the processed inputs and calculates an \"**image-text matching**\" (ITM) score.\n",
        "  `tf.nn.sigmoid` is applied to the score to convert it into a probability between 0 and 1.\n",
        "  \n",
        "6. **Print the score:**\n",
        "\n",
        "  The final ITM score, representing the similarity between the image and text, is printed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFfJ-vEVGD4f"
      },
      "source": [
        "# Important Points to remember\n",
        "\n",
        "1. **Hardware and Resources:**\n",
        "\n",
        "  * **GPU Usage**: VLMs often require significant computational resources, so using a GPU is recommended for faster inference.\n",
        "  * **Memory Management**: Be mindful of memory usage, especially with large models or datasets. Consider techniques like batch processing or gradient accumulation if needed.\n",
        "\n",
        "2. **Potential Issues:**\n",
        "\n",
        "  * **Bias and Fairness**: Be aware of potential biases in the pre-trained models and data, and take steps to mitigate them if necessary.\n",
        "  * **Hallucinations**: VLMs can sometimes generate incorrect or nonsensical outputs. Consider using grounding techniques or post-processing to improve reliability.\n",
        "\n",
        "3. **Context and Limitations:**\n",
        "\n",
        "  * **Domain Specificity:** Pretrained models may perform better on data similar to their training data. Consider fine-tuning for different domains.\n",
        "  * **Model Capabilities:** Understand the limitations of the specific VLM you are using. For example, some models might not be able to handle complex reasoning or fine-grained object recognition.\n",
        "\n",
        "4.  **Input Processing:**\n",
        "\n",
        "  * **Image Preprocessing:** Ensure images are preprocessed correctly using the corresponding processor (e.g., BlipProcessor). This includes resizing, normalization, and converting to the appropriate format.\n",
        "  * **Text Tokenization:** Text inputs, such as questions or prompts, need to be tokenized using the processor.\n",
        "  * **Data Collation:** Combine processed image and text data into the format expected by the model using `return_tensors=\"tf\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1EZUqWzEHdK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/icon.jpg)\n",
    "\n",
    "# Brief Recap\n",
    "\n",
    "**LlamaIndex** is an open source framework for building Context-Augmented LLM-powered agents (knowledge assistants) with LLMs and workflows (multi-step processes that combine one or more agents, data connectors, and other tools to combine a task). Context Augmentation makes data available to the LLM to solve the problem at hand. LlamaIndex provided tools to ingest, parse, index, and process your data and quickly implement complex workflows combining data access with LLM prompting. Some of the tools provided by LlamaIndex:\n",
    "\n",
    "1. Data connectors- APIs, PDFs, SQL, and many more to ingest existing data from their native source and format\n",
    "2. Data indexes- structure data in a representation that is easy for LLMs to consume\n",
    "3. Engines- Query Engines for question-answering and Chat Engines for \"back and forth\" interaction with data\n",
    "4. Agents- LLM powered knowledge workers augmented by tools\n",
    "5. Workflows- combine all the above into event-driven systems that can be deployed as production microservices\n",
    "\n",
    "# Architecture\n",
    "\n",
    "![](./assets/flow.png)\n",
    "\n",
    "Here's the architectutre and workflow of a LlamaIndex powered document processing and question answering system. Let me breakdown the ```managed indexed```:\n",
    "\n",
    "* Document Storage: getting your data from where it lives (PDF or a database)\n",
    "    \n",
    "    - The system starts with PDF files on the left side\n",
    "    \n",
    "    - These files get wrapped in a ```Document``` container\n",
    "\n",
    "* Chunk storage: storing and managing smaller segments of larger documents\n",
    "\n",
    "    - create smaller chunks based on text splitting strategies (fixed size splitting or semantic splitting)\n",
    "\n",
    "    - make the smaller chunks accessible for retrieval by assigning each chunk a unique identifier, metadata, and embeddings\n",
    "\n",
    "* Vector storage: storing vector representations of the given data\n",
    "\n",
    "    - generate embeddings from the documents\n",
    "\n",
    "    - system fetches the most relevant chunks based on similarity search or keyword-based retrieval\n",
    "\n",
    "    - assign an end-to-end flow to interact with the user (more on this later)\n",
    "\n",
    "# Use Cases\n",
    "\n",
    "* **Question Answering**\n",
    "\n",
    "    * Perform QA with LLMs through Retrieval Augmented Generation\n",
    "\n",
    "    * Perform QA using semantic search and summarization techniques over unstructured text such as text, PDFs, Notion, and Slack documents. LlamaParse allows to parse complex documents having text, tables, charts, images, footers\n",
    "\n",
    "    * Query data in a SQL database, CSV file, or other structured formats. This includes text-to-sql and text-to-pandas\n",
    "\n",
    "* **Chatbots**\n",
    "\n",
    "    * Knowledge management and enterprise search\n",
    "\n",
    "    * Health care and customer support services\n",
    "\n",
    "    * Virtual assistants for e-commerce and retail\n",
    "\n",
    "* **Document Understanding and Data Extraction**\n",
    "\n",
    "    * Read natural language and identify semantically important details such as names, dates, addresses and return them in a consistent format\n",
    "\n",
    "    * Create source materials such as chat logs and conversation transcripts\n",
    "\n",
    "* **Autonomous Agents**\n",
    "\n",
    "    * Generate a multimodal report using a multi-agent researcher, writer workflow, and LlamaParse\n",
    "\n",
    "    * A \"text-to-SQL assistant\" that can interact with a structured database\n",
    "\n",
    "    * Agentic RAG- build a context-augmented research assistant over your data that not only answers simple questions, but complex research tasks\n",
    "\n",
    "    * Build a coding assistant that can operate over code\n",
    "\n",
    "* **Multi-modal applications**\n",
    "\n",
    "    * All the core RAG concepts: indexing, retrieval, and synthesis, can be extended into the image setting\n",
    "\n",
    "    * You can generate a structured output with the new OpenAI GPT4V via LlamaIndex. The user just needs to specify a Pydantic object to define the structure of the output\n",
    "\n",
    "    * Retrieval-Augmented Image Captioning- first caption the image with a multi-modal model, then refine the caption by retrieving it from a text corpus\n",
    "\n",
    "* **Fine-tuning**: LlamaIndex allows fine-tuning Llama2, cross-encoders, and GPT-3.5 to distill GPT-4. It has multiple use cases such as:\n",
    "\n",
    "    * Multilingual Applications- supporting users in multiple languages or dialects\n",
    "\n",
    "    * Domain-Specific Knowledge Retrieval- legal, medical, or technical fields where accurate and context-sensitive answers are critical\n",
    "\n",
    "    * Personalized Financial Advisory- chatbots for investment firms providing tailored portfolio suggestions\n",
    "\n",
    "    * Healthcare and Diagnostics- assisting clinicians or patients with medical information and diagnostics\n",
    "\n",
    "# Components\n",
    "\n",
    "* Documents: a container around a data source that stores some text along with other attributes- i) metadata (dictionary of annotations), ii) relationships (dictionary containing relationships to other Documents/Nodes)\n",
    "\n",
    "* VectorStoreIndexes: builds an index on a list of Node objects\n",
    "\n",
    "* Agents: It is a software powered by an LLM that executes a series of steps towards solving a task with the help of a given set of tools.\n",
    "\n",
    "* Query engines: end-to-end flow that takes in a natural language query and returns a response along with reference context retrieved and passed to the LLM\n",
    "\n",
    "* Chat engines: end-to-end flow for having a conversation with your data\n",
    "\n",
    "# Implementation\n",
    "\n",
    "## **Initial Setup**\n",
    "\n",
    "Firstly we are going to pip install llamaindex: `pip install llama-index`\n",
    "\n",
    "This is a starter bundle of packages and installs the following openai packages:\n",
    "\n",
    "1. `llama-index-llms-openai`\n",
    "2. `llama-index-embeddings-openai`\n",
    "3. `llama-index-program-openai`\n",
    "4. `llama-index-question-gen-openai`\n",
    "5. `llama-index-agent-openai`\n",
    "6. `llama-index-multi-modal-llms-openai`\n",
    "\n",
    "By default, we would be using OpenAI **gpt-3.5-turbo for text generation** and **text-embedding-ada-002 for retrieval and embeddings**.\n",
    "\n",
    "## **API Keys**\n",
    "\n",
    "Sign up and gather your API keys from [OpenAI website](https://platform.openai.com/docs/overview)\n",
    "\n",
    "## Understanding Documents and Nodes\n",
    "\n",
    "Run the cells below to understand the structure of nodes and documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# creating documents with data loader\n",
    "documents = SimpleDirectoryReader(\"YOUR DATA PATH\").load_data()\n",
    "nodes = SentenceSplitter().get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also customise documents with your own custom metadata. Steps can be found [here](https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_documents/)\n",
    "\n",
    "# Creating Question Answering system using LlamaIndex and OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your API keys in an env variable\n",
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=\"YOUR API KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"YOUR DATA PATH\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index is loaded in memory as a series of vector embeddings. As a better practice it is better to save to a disk (make it persistent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "PERSIST_DIR = \"YOUR-DIRECTORY\"\n",
    "# check if storage already exists\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    # store it for later\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An improvement on the existing approach can be putting nodes in a docstore. This allows to define multiple indices over the same underlying docstore instead of duplicating data across indices. Implementiing docstores is outside the scope of this lab, but you can follow this [guide](https://docs.llamaindex.ai/en/stable/examples/docstore/DynamoDBDocstoreDemo/)\n",
    "\n",
    "## Create Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Some question about the data should go here\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream response\n",
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Chat Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = index.as_chat_engine()\n",
    "response = chat_engine.chat(\"Some question about the data should go here\")\n",
    "for token in response.response_gen:\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customize chat engine\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n",
    "chat_engine = index.as_chat_engine(chat_mode=\"openai\", llm=llm, verbose=True)\n",
    "response = chat_engine.chat(\"Some question about the data should go here\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in response.response_gen:\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Force chat engine to query an index\n",
    "\n",
    "- make use of the ```query_engine_tool``` under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_engine.chat(\n",
    "    \"Some question about the data should go here\", tool_choice=\"query_engine_tool\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents\n",
    "\n",
    "Structure of basic agent: ```agent = ReActAgent.from_tools([multiply_tool, add_tool], llm=llm, verbose=True)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.tools import FunctionTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create basic tools\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers and returns the product\"\"\"\n",
    "    return a * b\n",
    "\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Add two numbers and returns the sum\"\"\"\n",
    "    return a + b\n",
    "\n",
    "# define tool to multiply two numbers\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
    "# define tool to sum two numbers\n",
    "add_tool = FunctionTool.from_defaults(fn=add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define llm\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the agent\n",
    "agent = ReActAgent.from_tools([multiply_tool, add_tool], llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain a response\n",
    "response = agent.chat(\"What is 20+(2*4)? Use a tool to calculate every step.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

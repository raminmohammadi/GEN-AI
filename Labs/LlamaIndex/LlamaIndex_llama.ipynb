{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/icon.jpg)\n",
    "\n",
    "# Brief Recap\n",
    "\n",
    "**LlamaIndex** is an open source framework for building Context-Augmented LLM-powered agents (knowledge assistants) with LLMs and workflows (multi-step processes that combine one or more agents, data connectors, and other tools to combine a task). Context Augmentation makes data available to the LLM to solve the problem at hand. LlamaIndex provided tools to ingest, parse, index, and process your data and quickly implement complex workflows combining data access with LLM prompting. Some of the tools provided by LlamaIndex:\n",
    "\n",
    "1. Data connectors- APIs, PDFs, SQL, and many more to ingest existing data from their native source and format\n",
    "2. Data indexes- structure data in a representation that is easy for LLMs to consume\n",
    "3. Engines- Query Engines for question-answering and Chat Engines for \"back and forth\" interaction with data\n",
    "4. Agents- LLM powered knowledge workers augmented by tools\n",
    "5. Workflows- combine all the above into event-driven systems that can be deployed as production microservices\n",
    "\n",
    "# Use Cases\n",
    "\n",
    "* **Question Answering**\n",
    "\n",
    "    * Perform QA with LLMs through Retrieval Augmented Generation\n",
    "\n",
    "    * Perform QA using semantic search and summarization techniques over unstructured text such as text, PDFs, Notion, and Slack documents. LlamaParse allows to parse complex documents having text, tables, charts, images, footers\n",
    "\n",
    "    * Query data in a SQL database, CSV file, or other structured formats. This includes text-to-sql and text-to-pandas\n",
    "\n",
    "* **Chatbots**\n",
    "\n",
    "    * Knowledge management and enterprise search\n",
    "\n",
    "    * Health care and customer support services\n",
    "\n",
    "    * Virtual assistants for e-commerce and retail\n",
    "\n",
    "* **Document Understanding and Data Extraction**\n",
    "\n",
    "    * Read natural language and identify semantically important details such as names, dates, addresses and return them in a consistent format\n",
    "\n",
    "    * Create source materials such as chat logs and conversation transcripts\n",
    "\n",
    "* **Autonomous Agents**\n",
    "\n",
    "    * Generate a multimodal report using a multi-agent researcher, writer workflow, and LlamaParse\n",
    "\n",
    "    * A \"text-to-SQL assistant\" that can interact with a structured database\n",
    "\n",
    "    * Agentic RAG- build a context-augmented research assistant over your data that not only answers simple questions, but complex research tasks\n",
    "\n",
    "    * Build a coding assistant that can operate over code\n",
    "\n",
    "* **Multi-modal applications**\n",
    "\n",
    "    * All the core RAG concepts: indexing, retrieval, and synthesis, can be extended into the image setting\n",
    "\n",
    "    * You can generate a structured output with the new OpenAI GPT4V via LlamaIndex. The user just needs to specify a Pydantic object to define the structure of the output\n",
    "\n",
    "    * Retrieval-Augmented Image Captioning- first caption the image with a multi-modal model, then refine the caption by retrieving it from a text corpus\n",
    "\n",
    "* **Fine-tuning**: LlamaIndex allows fine-tuning Llama2, cross-encoders, and GPT-3.5 to distill GPT-4. It has multiple use cases such as:\n",
    "\n",
    "    * Multilingual Applications- supporting users in multiple languages or dialects\n",
    "\n",
    "    * Domain-Specific Knowledge Retrieval- legal, medical, or technical fields where accurate and context-sensitive answers are critical\n",
    "\n",
    "    * Personalized Financial Advisory- chatbots for investment firms providing tailored portfolio suggestions\n",
    "\n",
    "    * Healthcare and Diagnostics- assisting clinicians or patients with medical information and diagnostics\n",
    "\n",
    "# Initial Setup\n",
    "\n",
    "Firstly we are going to download ```ollama``` from [here](https://ollama.com)\n",
    "\n",
    "Then,\n",
    "\n",
    "```ollama pull llama3.2```\n",
    "\n",
    "Now pip install llamaindex: `pip install llama-index` \n",
    "\n",
    "and ollama for llamaindex: `pip install llama-index-llms-ollama`\n",
    "\n",
    "## LLM Wrapper\n",
    "\n",
    "In this section, we're going to see how LlamaIndex allows us to interact with LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run basic query with Ollama wrapper\n",
    "\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3.1:latest\", request_timeout=120.0)\n",
    "resp = llm.complete(\"Who is Paul Graham?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It takes in a \"llama3.1:latest\" LLM\n",
    "* The output is something similar to when you run it by the OpenAI API directly\n",
    "\n",
    "Ollama supports a JSON mode which ensures all responses are valid JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3.1:latest\", request_timeout=120.0, json_mode=True)\n",
    "response = llm.complete(\n",
    "    \"Who is Paul Graham? Output as a structured JSON object.\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using Ollama as a LLM for a chat model\n",
    "\n",
    "**Breakdown**\n",
    "\n",
    "Initializing the Ollama instance:\n",
    "\n",
    "```llm = Ollama(model=\"llama3.1:latest\", request_timeout=120.0)```\n",
    "\n",
    "* This creates an instance of the Ollama class, which is a wrapper for interacting with Meta's chat models that have been setup locally.\n",
    "* model_name=\"llama3.1:latest\" specifies that we're using the latest version of the \"llama3.1\" model.\n",
    "\n",
    "**Defining the messages**:\n",
    "\n",
    "```\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role = \"system\", content=\"You are an expert data scientist\"\n",
    "    ),\n",
    "    ChatMessage(\n",
    "        role=\"user\", content=\"Write a Python script that trains a neural network on simulated data.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "```\n",
    "\n",
    "This creates a list of ```ChatMessage``` objects messages that will be sent to the chat model.\n",
    "\n",
    "The role defines whom the instruction is meant for. \n",
    "\n",
    "- ```system``` provides overall instructions or context to the model. Here, it's instructing the model to act as an \"expert data scientist.\"\n",
    "- ```user``` represents the user's input or query. In this case, it's asking the model to \"Write a Python script that trains a neural network on simulated data.\"\n",
    "\n",
    "**In essence**,\n",
    "\n",
    "This sets up a conversation with the \"llama3.1\" chat model, provides it with a system prompt and a user query, and then prints the model's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "# list of messages to send to the chat model\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are an expert data scientist\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"Write a Python script that trains a neural network on simulated data.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3.1:latest\", request_timeout=120.0)\n",
    "resp = llm.chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In essence**,\n",
    "\n",
    "This sets up a conversation with the \"llama3.1\" chat model, provides it with a system prompt and a user query, and then prints the model's response.\n",
    "\n",
    "## Structured Outputs\n",
    "\n",
    "Ollama has builtin structured output capabilities which allows attaching a Pydantic class to the LLM. Now we see how to structure a LLM generated response as a dictionary without the need of generating the response as a valid JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.bridge.pydantic import BaseModel\n",
    "\n",
    "# define the Pydantic class\n",
    "class Song(BaseModel):\n",
    "    \"\"\"A song with name and artist.\"\"\"\n",
    "\n",
    "    name: str\n",
    "    artist: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3.1:latest\", request_timeout=120.0)\n",
    "sllm = llm.as_structured_llm(Song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "response = sllm.chat([ChatMessage(role=\"user\", content=\"Name a random song!\")])\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In essence**,\n",
    "\n",
    "we generate a response and structure it as a dictionary where the keys have the same name as the Pydantic class variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUgRVUfXQKl4"
      },
      "source": [
        "<h1>\n",
        "Diffusion Models\n",
        "</h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDUpAOnYQQSo"
      },
      "source": [
        "# Brief Recap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxHI6AdUQRdh"
      },
      "source": [
        "**Diffusion models** are a class of generative models used primarily for generating data such as images. They work by gradually transforming a simple noise distribution into a data distribution through a series of steps. Essentially, they reverse a diffusion process, which is a stochastic process where data is incrementally degraded by adding noise.\n",
        "\n",
        "\n",
        "They were initially introduced in a broader form of stochastic processes, but their specific application to generative modeling was popularized by the work of Sohl-Dickstein et al. with the paper \"Deep Unsupervised Learning using Nonequilibrium Thermodynamics\" in 2015. This work laid the foundation for understanding how to use a diffusion process in both forward and reverse directions to generate data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWQGPeAmYLMq"
      },
      "source": [
        "# Architecture\n",
        "\n",
        "<img src='assets/architecture.png' width=450>\n",
        "\n",
        "This diagram illustrates the architecture of a diffusion model, highlighting both the forward and reverse diffusion processes.\n",
        "\n",
        "1. **Input Data**: This is the original data that you want to model or generate, such as an image.\n",
        "\n",
        "2. **Noise Schedule**: A predefined schedule that determines how much noise is added at each step in the forward diffusion process. This schedule controls the transformation from the original data to noise.\n",
        "\n",
        "3. **Forward Diffusion Process**: In this phase, noise is progressively added to the input data according to the noise schedule, transforming it into noisy data. This simulates the degradation of data.\n",
        "\n",
        "4. **Noisy Data**: The result of the forward diffusion process, where the original data has been converted into a noisy version.\n",
        "\n",
        "5. **Neural Network**: A model trained to predict and reverse the noise added in the forward process. It learns to generate the original data from the noisy data.\n",
        "\n",
        "6. **Reverse Diffusion Process**: Guided by the neural network and using the noisy data as a starting point, this process iteratively removes noise, working backwards to recreate a denoised version of the original data.\n",
        "\n",
        "7. **Denoised Output**: The final output generated by the reverse diffusion process, ideally resembling the initial input data.\n",
        "\n",
        "Overall, this architecture shows how diffusion models use a combination of noise addition and removal to generate new data that mimics the original input.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkFEkfkHaQtS"
      },
      "source": [
        "# Use cases\n",
        "\n",
        "Diffusion models have a variety of use cases, particularly in generative tasks. Here are some key applications:\n",
        "\n",
        "1. **Image Generation**: They are used to create realistic images from noise, often producing high-quality results. This includes art creation, synthetic media, and designing textures.\n",
        "\n",
        "2. **Image Editing**: Diffusion models can be employed for tasks like inpainting (filling in missing parts of an image), super-resolution (increasing image resolution), and style transfer.\n",
        "\n",
        "3. **Video Generation**: These models can be adapted to generate or enhance video content, maintaining temporal consistency between frames.\n",
        "\n",
        "4. **3D Model Creation**: By extending their capabilities, diffusion models can assist in generating 3D content, used in gaming, simulations, and VR applications.\n",
        "\n",
        "5. **Text and Language Modeling**: They can be used for text-based tasks, such as generating coherent paragraphs or dialogue.\n",
        "\n",
        "6. **Audio Synthesis**: Generating and manipulating audio signals, such as creating music or sound effects that mimic real-world sounds.\n",
        "\n",
        "7. **Data Augmentation**: Enhancing datasets by generating additional synthetic examples for training machine learning models.\n",
        "\n",
        "These diverse applications illustrate the versatility and potential of diffusion models in various fields."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfyOEN7AbTa9"
      },
      "source": [
        "# **Diffusion Model Implementation**\n",
        "\n",
        "### **Key Components**\n",
        "* **Data Preprocessing:** Prepare the dataset, typically normalizing images to ensure they fit within the model's expected range.\n",
        "\n",
        "* **Diffusion Process:**\n",
        "\n",
        "  * **Forward Diffusion:** Incrementally adds noise to the data.\n",
        "  * **Reverse Diffusion:** The model learns to remove this noise to reconstruct the data.\n",
        "* **Noise Schedule:** Determines how noise is added at each step of the forward process.\n",
        "\n",
        "* **Neural Network Architecture:** Usually a U-Net, which is effective for capturing spatial information and reconstructing images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbrPUXtVtXlv"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example U-Net like architecture\n",
        "def get_unet_model(input_shape):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "\n",
        "    # Encoding path\n",
        "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
        "    x = layers.MaxPooling2D(2)(x)\n",
        "    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D(2)(x)\n",
        "\n",
        "    # Bottom\n",
        "    x = layers.Conv2D(256, 3, activation='relu', padding='same')(x)\n",
        "\n",
        "    # Decoding path\n",
        "    x = layers.UpSampling2D(2)(x)\n",
        "    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
        "    x = layers.UpSampling2D(2)(x)\n",
        "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
        "\n",
        "    outputs = layers.Conv2D(3, 1, activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Cf-iyMyt3S1"
      },
      "source": [
        "### **Explanation**\n",
        "\n",
        "**U-Net Architecture:**\n",
        "\n",
        "<img src='assets/unet.jpg' width=500>\n",
        "\n",
        "Utilizes encoding and decoding paths with skip connections to capture and reconstruct image features effectively.\n",
        "  * **Training Process:** The model is optimized using a mean squared error loss, attempting to learn the denoising process.\n",
        "  * **Epochs:** Set to a low number for quick demonstration; more epochs would be needed for real data.\n",
        "\n",
        "**Use Cases**\n",
        "\n",
        "This basic setup can be extended for advanced applications like super-resolution, style transfer, or generative tasks by adjusting the architecture and training protocols. For real implementations, consider using additional noise schedules and more complex architectures tailored to specific tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjamLERTtYg4"
      },
      "outputs": [],
      "source": [
        "# Instantiate and compile the model\n",
        "image_size = (128, 128, 3)\n",
        "model = get_unet_model(input_shape=image_size)\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Example dummy data for training\n",
        "import numpy as np\n",
        "dummy_images = np.random.rand(100, 128, 128, 3)\n",
        "\n",
        "# Train the model\n",
        "model.fit(dummy_images, dummy_images, epochs=5, batch_size=10)\n",
        "\n",
        "# Display some outputs\n",
        "sample_output = model.predict(dummy_images[:5])\n",
        "for img in sample_output:\n",
        "    plt.imshow(img)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNNcguv4uW6F"
      },
      "source": [
        "# **Denoising Images using Diffusion**\n",
        "\n",
        "\n",
        "### **Idea**\n",
        "<img src='assets/denoise.jpg' width=500>\n",
        "\n",
        "The idea is to see how a diffusion model deconstructs an image into noise and reversely constructs it back to a recognizable form, displaying the effectiveness of learned noise removal in generating high-quality images.\n",
        "\n",
        "* **Forward Diffusion (Top Row):**\n",
        "\n",
        "  Starts with a clear image and progressively adds noise over several steps, making the image noisier.\n",
        "\n",
        "* **Reverse Diffusion (Bottom Row):**\n",
        "\n",
        "  The model learns to gradually remove noise, starting from a noisy image and reconstructing it back into a clear image.\n",
        "  Arrows indicate how the model refines the image at each step, effectively demonstrating its capability to denoise and generate the original content.\n",
        "\n",
        "\n",
        "\n",
        "### **Reverse process**\n",
        "\n",
        "<img src='assets/denoise_flow.jpg' width=500>\n",
        "\n",
        "The reverse process of a diffusion model is illustrated above:\n",
        "\n",
        "1. **Noise Initialization** (First Input \\( t = 0 \\)):\n",
        "   - The model begins with an entirely noisy image.\n",
        "\n",
        "2. **Model Processing**:\n",
        "   - The model takes the noisy image and predicts a slightly denoised version.\n",
        "   - This process iteratively refines the image by reducing noise step by step.\n",
        "\n",
        "3. **Output and Feedback Loop**:\n",
        "   - The output image, with reduced noise $( \\text{noise} - 1 )$, becomes the input for the next iteration (\\( t > 0 \\)).\n",
        "   - This feedback loop continues until the image is sufficiently denoised, progressively reconstructing the original content.\n",
        "\n",
        "Essentially, the model uses iterative predictions to transform noise into a coherent image through multiple steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjS7bE76ughp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from tqdm.auto import trange, tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clgwSrDigerH"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ifo1nrQ3FDzh"
      },
      "outputs": [],
      "source": [
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "X_train = X_train[y_train.squeeze() == 1]\n",
        "X_train = (X_train / 127.5) - 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNEekOGYFDxl"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 32     # input image size, CIFAR-10 is 32x32\n",
        "BATCH_SIZE = 128  # for training batch size\n",
        "timesteps = 16    # how many steps for a noisy image into clear\n",
        "time_bar = 1 - np.linspace(0, 1.0, timesteps + 1) # linspace for timesteps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6v15PLm4FDvx"
      },
      "outputs": [],
      "source": [
        "plt.plot(time_bar, label='Noise')\n",
        "plt.plot(1 - time_bar, label='Clarity')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTyPGFKogpBm"
      },
      "source": [
        "## Image Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HwkhJ4_FDt7"
      },
      "outputs": [],
      "source": [
        "def cvtImg(img):\n",
        "    img = img - img.min()\n",
        "    img = (img / img.max())\n",
        "    return img.astype(np.float32)\n",
        "\n",
        "def show_examples(x):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i in range(25):\n",
        "        plt.subplot(5, 5, i+1)\n",
        "        img = cvtImg(x[i])\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "\n",
        "show_examples(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fml0NRFkbEi"
      },
      "source": [
        "**Key Components**\n",
        "\n",
        "* **Normalization**: This function normalizes the input image (img) to a range between 0 and 1.\n",
        "\n",
        "* **Data Type:** It converts the image data type to np.float32, which is often preferred for numerical computations in machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKb6NYQpgsrQ"
      },
      "source": [
        "## Noise Generation\n",
        "\n",
        "We are going to simulate a diffusion process where an image is gradually degraded by adding noise over multiple steps. This process is essential for training diffusion models, as they learn to reverse this noise addition to generate images.\n",
        "\n",
        "\n",
        "The `forward_noise` function takes an image and a timestep as input and applies Gaussian noise according to a pre-defined schedule. By repeating this process, you can progressively degrade the clarity of an image. This technique is fundamental to how diffusion models learn to generate images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMABZYe_FDr6"
      },
      "outputs": [],
      "source": [
        "def forward_noise(x, t):\n",
        "    a = time_bar[t]        # base noise level\n",
        "    b = time_bar[t + 1]    # next noise level\n",
        "\n",
        "    noise = np.random.normal(size=x.shape)\n",
        "    # Reshape 'a' and 'b' to add dimensions compatible with 'x'\n",
        "    a = a[:, np.newaxis, np.newaxis, np.newaxis]  # Add 3 new axes\n",
        "    b = b[:, np.newaxis, np.newaxis, np.newaxis]  # Add 3 new axes\n",
        "\n",
        "    img_a = x * (1 - a) + noise * a\n",
        "    img_b = x * (1 - b) + noise * b\n",
        "    return img_a, img_b\n",
        "\n",
        "def generate_ts(num):\n",
        "    return np.random.randint(0, timesteps, size=num)\n",
        "\n",
        "# t = np.full((25,), timesteps - 1) # if you want see clarity\n",
        "# t = np.full((25,), 0)             # if you want see noisy\n",
        "t = generate_ts(25)             # random for training data\n",
        "a, b = forward_noise(X_train[:25], t)\n",
        "show_examples(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqWrkKfPj7j9"
      },
      "source": [
        "**Key Components**\n",
        "\n",
        "* **Input:** It takes an image (x) and a timestep (t) as input.\n",
        "* **Noise Levels:** It determines the noise levels for the current timestep (a) and the next timestep (b) using the time_bar array. time_bar contains pre-calculated noise levels that decrease over time. This is the \"noise schedule\" you mentioned earlier.\n",
        "* **Adding Noise:** Gaussian noise is added to the input image based on the noise level:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NnKFFudgvMa"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbvti6zplWBd"
      },
      "source": [
        "**Block Function**\n",
        "\n",
        "\n",
        "This function defines a basic building block of the U-Net, consisting of convolutional layers, activation functions, and layer normalization. It processes both the image features (x_img) and timestep information (x_ts) to learn how noise affects the image at different stages of the diffusion process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nD_7PHI3FDp1"
      },
      "outputs": [],
      "source": [
        "def block(x_img, x_ts):\n",
        "    x_parameter = layers.Conv2D(128, kernel_size=3, padding='same')(x_img)\n",
        "    x_parameter = layers.Activation('relu')(x_parameter)\n",
        "\n",
        "    time_parameter = layers.Dense(128)(x_ts)\n",
        "    time_parameter = layers.Activation('relu')(time_parameter)\n",
        "    time_parameter = layers.Reshape((1, 1, 128))(time_parameter)\n",
        "    x_parameter = x_parameter * time_parameter\n",
        "\n",
        "    # -----\n",
        "    x_out = layers.Conv2D(128, kernel_size=3, padding='same')(x_img)\n",
        "    x_out = x_out + x_parameter\n",
        "    x_out = layers.LayerNormalization()(x_out)\n",
        "    x_out = layers.Activation('relu')(x_out)\n",
        "\n",
        "    return x_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCV0Vm3hlaVA"
      },
      "source": [
        "**`make_model` Function**\n",
        "\n",
        "This function assembles the complete U-Net model:\n",
        "\n",
        "* **Input Layers:** It creates input layers for the image (x_input) and the timestep (x_ts_input).\n",
        "* **Timestep Embedding:** The timestep is embedded into a higher-dimensional representation to provide more context to the model about the diffusion stage.\n",
        "* **Encoding Path:** The image is processed through a series of convolutional blocks, gradually downsampling the feature maps to capture larger-scale patterns.\n",
        "* **Bottleneck:** A central block processes the most compressed representation of the image.\n",
        "* **Decoding Path:** The feature maps are upsampled and combined with features from the encoding path using skip connections to reconstruct fine details.\n",
        "* **Output Layer:** A final convolutional layer produces the denoised image.\n",
        "* **Model Creation:** The function returns a Keras model object that takes the imag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1XoKCAnFDj4"
      },
      "outputs": [],
      "source": [
        "def make_model():\n",
        "    x = x_input = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3), name='x_input')\n",
        "\n",
        "    x_ts = x_ts_input = layers.Input(shape=(1,), name='x_ts_input')\n",
        "    x_ts = layers.Dense(192)(x_ts)\n",
        "    x_ts = layers.LayerNormalization()(x_ts)\n",
        "    x_ts = layers.Activation('relu')(x_ts)\n",
        "\n",
        "    # ----- left ( down ) -----\n",
        "    x = x32 = block(x, x_ts)\n",
        "    x = layers.MaxPool2D(2)(x)\n",
        "\n",
        "    x = x16 = block(x, x_ts)\n",
        "    x = layers.MaxPool2D(2)(x)\n",
        "\n",
        "    x = x8 = block(x, x_ts)\n",
        "    x = layers.MaxPool2D(2)(x)\n",
        "\n",
        "    x = x4 = block(x, x_ts)\n",
        "\n",
        "    # ----- MLP -----\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Concatenate()([x, x_ts])\n",
        "    x = layers.Dense(128)(x)\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    x = layers.Dense(4 * 4 * 32)(x)\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.Reshape((4, 4, 32))(x)\n",
        "\n",
        "    # ----- right ( up ) -----\n",
        "    x = layers.Concatenate()([x, x4])\n",
        "    x = block(x, x_ts)\n",
        "    x = layers.UpSampling2D(2)(x)\n",
        "\n",
        "    x = layers.Concatenate()([x, x8])\n",
        "    x = block(x, x_ts)\n",
        "    x = layers.UpSampling2D(2)(x)\n",
        "\n",
        "    x = layers.Concatenate()([x, x16])\n",
        "    x = block(x, x_ts)\n",
        "    x = layers.UpSampling2D(2)(x)\n",
        "\n",
        "    x = layers.Concatenate()([x, x32])\n",
        "    x = block(x, x_ts)\n",
        "\n",
        "    # ----- output -----\n",
        "    x = layers.Conv2D(3, kernel_size=1, padding='same')(x)\n",
        "    model = tf.keras.models.Model([x_input, x_ts_input], x)\n",
        "    return model\n",
        "\n",
        "\n",
        "model = make_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MisjfR6HlxZy"
      },
      "source": [
        "The U-Net architecture is characterized by its symmetrical encoding and decoding paths, resembling a \"U\" shape. Skip connections between corresponding layers in the encoding and decoding paths allow the model to preserve fine-grained details during the denoising process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Am59LJCg7dm"
      },
      "source": [
        "## Model Training\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTKr1u5CmRKw"
      },
      "source": [
        "#### **Compilation**\n",
        "\n",
        "\n",
        "The model is compiled with an Adam optimizer and a mean absolute error (MAE) loss function. During training, it learns to predict the noise added to the image at each timestep. By iteratively denoising images, it learns to reverse the diffusion process and generate new images from noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJvScoPvGmNb"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0008)\n",
        "loss_func = tf.keras.losses.MeanAbsoluteError()\n",
        "model.compile(loss=loss_func, optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mbFjw83GmLi"
      },
      "outputs": [],
      "source": [
        "def predict(x_idx=None):\n",
        "    x = np.random.normal(size=(32, IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "    for i in trange(timesteps):\n",
        "        t = i\n",
        "        x = model.predict([x, np.full((32), t)], verbose=0)\n",
        "    show_examples(x)\n",
        "\n",
        "predict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74EdexeCGmJh"
      },
      "outputs": [],
      "source": [
        "def train_one(x_img):\n",
        "    x_ts = generate_ts(len(x_img))\n",
        "    x_a, x_b = forward_noise(x_img, x_ts)\n",
        "    loss = model.train_on_batch([x_a, x_ts], x_b)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OV_8H7QOGmDN"
      },
      "outputs": [],
      "source": [
        "def train(R=50):\n",
        "    bar = trange(R)\n",
        "    total = 100\n",
        "    for i in bar:\n",
        "        for j in range(total):\n",
        "            x_img = X_train[np.random.randint(len(X_train), size=BATCH_SIZE)]\n",
        "            loss = train_one(x_img)\n",
        "            pg = (j / total) * 100\n",
        "            if j % 5 == 0:\n",
        "                bar.set_description(f'loss: {loss:.5f}, p: {pg:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdhEl-MpmNUF"
      },
      "source": [
        "#### **Training**\n",
        "\n",
        "The training process involves repeatedly presenting the model with noisy images and their corresponding timesteps, and guiding it to predict less noisy versions. This process allows the model to learn the underlying patterns in the data and effectively reverse the diffusion process to generate high-quality images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBIImkmlGmBJ"
      },
      "outputs": [],
      "source": [
        "for _ in range(10):\n",
        "    train()\n",
        "    # reduce learning rate for next training\n",
        "    model.optimizer.learning_rate = max(0.000001, model.optimizer.learning_rate * 0.9)\n",
        "\n",
        "    # show result\n",
        "    predict()\n",
        "    predict_step()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdvId2pO5xpn"
      },
      "source": [
        "Diffusion Models are a conceptually simple and elegant approach to the problem of generating data. Their State-of-the-Art results combined with non-adversarial training has propelled them to great heights, and further improvements can be expected in the coming years given their nascent status. In particular, Diffusion Models have been found to be essential to the performance of cutting-edge models like DALL-E 2.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zo2B5pV0ucWC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJqAJq6pucUM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkhISyboucRj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olkzIyKHucO_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1EZUqWzEHdK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

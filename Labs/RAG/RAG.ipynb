{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Retrieval-Augmented Generation (RAG)</h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief Recap of Retrieval-Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you're having a conversation with someone who has read thousands of books but can only remember general concepts, not specific details. Now imagine giving this person access to a library where they can quickly look up exact information while talking to you. This is essentially what RAG does for artificial intelligence systems.\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) combines two powerful capabilities: the ability to retrieve specific information from a knowledge base and the ability to generate human-like text responses. Think of it as giving an AI system both a powerful memory (retrieval) and the ability to explain things in its own words (generation).\n",
    "\n",
    "Let's understand this through an example. If you ask a standard language model \"What were the key points in the company's Q3 2023 earnings report?\", it might generate a plausible but potentially incorrect response based on its training data. However, a RAG system would first retrieve the actual earnings report, find the relevant sections, and then generate a response based on that specific, accurate information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"static/image1.gif\" alt=\"RAG\" style=\"width:50%;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Traditional Approaches Are Challenging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Language Model Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use traditional language models without retrieval capabilities, we encounter several significant challenges:\n",
    "\n",
    "1. **Knowledge Cutoff**: Language models are trained on data up to a certain date. Imagine trying to discuss current events using only information from a year ago. This creates a fundamental limitation in handling recent or updated information.\n",
    "\n",
    "2. **Hallucination Problems**: Without access to specific reference material, language models might generate plausible-sounding but incorrect information. It's like asking someone to recall the details of a book they read years ago – they might mix up details or fill in gaps with assumptions.\n",
    "\n",
    "3. **Specialization Difficulties**: Language models struggle with highly specialized or technical information. While they might understand general medical concepts, they wouldn't have access to a specific patient's medical history or the latest research in a particular field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional Search Engine Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional search and retrieval systems also have their own set of challenges:\n",
    "\n",
    "1. **Literal Matching**: Most search engines work by matching exact words or close variants. This means they might miss relevant information expressed in different terms. For example, a search for \"heart attack symptoms\" might not return documents discussing \"myocardial infarction indicators.\"\n",
    "\n",
    "2. **Context Understanding**: Search engines struggle to understand the broader context of a query. They might return documents containing the right keywords but missing the actual intent of the question.\n",
    "\n",
    "3. **Response Generation**: Even if they find relevant information, traditional search engines can't synthesize or explain it – they can only show you the original text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding RAG Architecture: Core Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG systems are built on a carefully designed architecture that combines several sophisticated components. Let's break this down using an analogy of a highly efficient library system:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Document Processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of this as the library's cataloging system. Just as a library needs to process new books – categorizing them, creating summaries, and deciding where they belong – the document processing pipeline in RAG:\n",
    "- Breaks down large documents into manageable chunks\n",
    "- Cleans and standardizes the text\n",
    "- Extracts key information and metadata\n",
    "- Prepares the content for efficient storage and retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Embedding System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This component is like creating a detailed index of every concept in your library. But instead of just keywords, it creates rich mathematical representations that capture the meaning of the text. For example:\n",
    "- The phrase \"the heart pumps blood\" and \"the cardiac muscle circulates fluid\" would be recognized as similar concepts\n",
    "- These mathematical representations help find relevant information even when the exact words don't match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Retrieval Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works like an extremely intelligent librarian who:\n",
    "- Understands the true meaning behind your questions\n",
    "- Knows exactly where to find relevant information\n",
    "- Can quickly assess which sources are most relevant\n",
    "- Brings back not just exact matches, but contextually relevant information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is like having an expert who:\n",
    "- Reads through all the retrieved information\n",
    "- Understands how different pieces of information relate to each other\n",
    "- Synthesizes the information into a coherent response\n",
    "- Explains complex concepts in a way that matches your level of understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"static/image2.png\" alt=\"RAG\" style=\"width:50%;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Advantages of RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Accuracy and Reliability\n",
    "RAG systems significantly improve the accuracy of AI responses by:\n",
    "- Grounding answers in specific, verifiable sources\n",
    "- Reducing hallucinations and speculation\n",
    "- Providing up-to-date information\n",
    "- Maintaining factual consistency\n",
    "\n",
    "2. Transparency and Explainability\n",
    "Unlike traditional \"black box\" AI systems, RAG offers:\n",
    "- Clear sources for information\n",
    "- Traceable reasoning paths\n",
    "- Verifiable references\n",
    "- The ability to audit and validate responses\n",
    "\n",
    "3. Adaptability and Maintenance\n",
    "RAG systems excel in staying current because:\n",
    "- New information can be added without retraining\n",
    "- Knowledge bases can be updated in real-time\n",
    "- Different sources can be used for different contexts\n",
    "- Content can be easily modified or removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing RAG Core Components From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll build each core component of a RAG system from the ground up. We'll start with simple implementations and progressively add sophistication, explaining each step along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Document Processing Component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The document processor is our first building block. It handles the crucial task of breaking down documents into manageable, meaningful chunks while preserving context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional\n",
    "import re\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, \n",
    "                 chunk_size: int = 500,\n",
    "                 chunk_overlap: int = 50,\n",
    "                 min_chunk_size: int = 100):\n",
    "        \"\"\"\n",
    "        Initialize the document processor with configurable parameters.\n",
    "        \n",
    "        Parameters:\n",
    "            chunk_size: Target size for each text chunk\n",
    "            chunk_overlap: Number of characters to overlap between chunks\n",
    "            min_chunk_size: Minimum size for a valid chunk\n",
    "        \"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.min_chunk_size = min_chunk_size\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and normalize text content for better processing.\n",
    "        \n",
    "        This method handles common text issues like extra whitespace,\n",
    "        special characters, and formatting inconsistencies.\n",
    "        \"\"\"\n",
    "        # Remove extra whitespace and normalize line endings\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'\\n+', '\\n', text)\n",
    "        \n",
    "        # Remove special characters while preserving essential punctuation\n",
    "        text = re.sub(r'[^\\w\\s.,!?;:()\\[\\]{}\"\\'`-]', '', text)\n",
    "        \n",
    "        # Normalize sentence endings to help with chunking\n",
    "        text = re.sub(r'([.!?])\\s*', r'\\1\\n', text)\n",
    "        \n",
    "        return text.strip()\n",
    "\n",
    "    def create_chunks(self, \n",
    "                     document: Dict[str, str], \n",
    "                     respect_sentences: bool = True) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Break document into chunks while trying to preserve semantic meaning.\n",
    "        \n",
    "        Parameters:\n",
    "            document: Dictionary containing document text and metadata\n",
    "            respect_sentences: Whether to avoid breaking mid-sentence\n",
    "            \n",
    "        Returns:\n",
    "            List of chunk dictionaries with text and metadata\n",
    "        \"\"\"\n",
    "        text = self.clean_text(document['text'])\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        \n",
    "        # Split into sentences if we want to respect sentence boundaries\n",
    "        sentences = text.split('\\n') if respect_sentences else [text]\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "                \n",
    "            sentence_length = len(sentence)\n",
    "            \n",
    "            # If adding this sentence exceeds chunk size\n",
    "            if current_length + sentence_length > self.chunk_size:\n",
    "                # Save current chunk if it's large enough\n",
    "                if current_length >= self.min_chunk_size:\n",
    "                    chunk_text = ' '.join(current_chunk)\n",
    "                    chunks.append({\n",
    "                        'text': chunk_text,\n",
    "                        'document_id': document.get('id', 'unknown'),\n",
    "                        'chunk_size': len(chunk_text),\n",
    "                        'position': len(chunks)\n",
    "                    })\n",
    "                \n",
    "                # Start new chunk, including overlap from previous chunk\n",
    "                if current_chunk and self.chunk_overlap > 0:\n",
    "                    # Calculate overlap text from previous chunk\n",
    "                    overlap_text = ' '.join(current_chunk[-2:])  # Keep last 2 sentences for context\n",
    "                    current_chunk = [overlap_text, sentence]\n",
    "                    current_length = len(overlap_text) + sentence_length\n",
    "                else:\n",
    "                    current_chunk = [sentence]\n",
    "                    current_length = sentence_length\n",
    "            else:\n",
    "                current_chunk.append(sentence)\n",
    "                current_length += sentence_length\n",
    "        \n",
    "        # Add any remaining text as the final chunk\n",
    "        if current_chunk and current_length >= self.min_chunk_size:\n",
    "            chunk_text = ' '.join(current_chunk)\n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'document_id': document.get('id', 'unknown'),\n",
    "                'chunk_size': len(chunk_text),\n",
    "                'position': len(chunks)\n",
    "            })\n",
    "            \n",
    "        return chunks\n",
    "\n",
    "    def process_document(self, document: Dict[str, str]) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Process a document from start to finish.\n",
    "        \n",
    "        This is the main method that orchestrates the entire document\n",
    "        processing pipeline.\n",
    "        \"\"\"\n",
    "        # Validate document\n",
    "        if not document.get('text'):\n",
    "            raise ValueError(\"Document must contain 'text' field\")\n",
    "            \n",
    "        # Process the document\n",
    "        chunks = self.create_chunks(document)\n",
    "        \n",
    "        # Add metadata to each chunk\n",
    "        for chunk in chunks:\n",
    "            chunk.update({\n",
    "                'source': document.get('source', 'unknown'),\n",
    "                'title': document.get('title', 'unknown'),\n",
    "                'timestamp': document.get('timestamp', None)\n",
    "            })\n",
    "            \n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the key aspects of our document processor:\n",
    "\n",
    "1. **Text Cleaning**: The `clean_text` method handles common text issues:\n",
    "   - Normalizes whitespace and line endings\n",
    "   - Removes problematic special characters\n",
    "   - Standardizes sentence endings for better chunking\n",
    "   - This creates a consistent format for further processing\n",
    "\n",
    "2. **Chunking Strategy**: The `create_chunks` method uses a sophisticated approach:\n",
    "   - Respects sentence boundaries to maintain readability\n",
    "   - Implements overlap between chunks to preserve context\n",
    "   - Maintains minimum chunk sizes to ensure meaningful content\n",
    "   - Tracks document metadata throughout the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embedding System Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding system creates vector representations of text that capture semantic meaning. Here's our implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from hashlib import md5\n",
    "from typing import Dict, List, Union, Optional\n",
    "\n",
    "class EmbeddingEngine:\n",
    "    def __init__(self, dimensions: int = 768, cache_size: int = 10000):\n",
    "        \"\"\"\n",
    "        Initialize the embedding engine.\n",
    "        \n",
    "        Parameters:\n",
    "            dimensions: Size of the embedding vectors\n",
    "            cache_size: Maximum number of embeddings to cache\n",
    "        \"\"\"\n",
    "        self.dimensions = dimensions\n",
    "        self.cache_size = cache_size\n",
    "        self.cache = {}\n",
    "        self.word_vectors = {}\n",
    "        \n",
    "    def _create_word_vector(self, word: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create a deterministic vector for a word using a hashing approach.\n",
    "        This is a simplified version - in practice, you'd use a proper\n",
    "        embedding model.\n",
    "        \"\"\"\n",
    "        \n",
    "        # If we've already created a vector for this word, return it\n",
    "        if word in self.word_vectors:\n",
    "            return self.word_vectors[word]\n",
    "            \n",
    "        # Create a deterministic seed from the word that fits within NumPy's limits\n",
    "        # We'll take the first 8 characters of the hash and convert to int\n",
    "        # This ensures our seed is within the valid range (0 to 2^32 - 1)\n",
    "        word_hash = int(md5(word.encode()).hexdigest()[:8], 16)\n",
    "        \n",
    "        # Set the random seed for reproducibility\n",
    "        np.random.seed(word_hash)\n",
    "        \n",
    "        # Create a unit vector for the word\n",
    "        vector = np.random.randn(self.dimensions)\n",
    "        vector = vector / np.linalg.norm(vector)\n",
    "        \n",
    "        # Cache the vector\n",
    "        self.word_vectors[word] = vector\n",
    "        return vector\n",
    "        \n",
    "    def compute_embedding(self, \n",
    "                         text: str, \n",
    "                         use_cache: bool = True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert text into a vector representation.\n",
    "        \n",
    "        This implementation uses a bag-of-words approach with position\n",
    "        weighting and IDF-like scaling.\n",
    "        \"\"\"\n",
    "        # Check cache first\n",
    "        if use_cache and text in self.cache:\n",
    "            return self.cache[text]\n",
    "            \n",
    "        # Preprocess text\n",
    "        words = text.lower().split()\n",
    "        if not words:\n",
    "            return np.zeros(self.dimensions)\n",
    "            \n",
    "        # Initialize embedding\n",
    "        embedding = np.zeros(self.dimensions)\n",
    "        word_count = {}\n",
    "        \n",
    "        # Compute word vectors with position weighting\n",
    "        for position, word in enumerate(words):\n",
    "            # Count word frequency\n",
    "            word_count[word] = word_count.get(word, 0) + 1\n",
    "            \n",
    "            # Get word vector\n",
    "            word_vector = self._create_word_vector(word)\n",
    "            \n",
    "            # Apply position-based weighting (words earlier in the text get slightly more weight)\n",
    "            position_weight = 1.0 / (1 + np.log1p(position))\n",
    "            \n",
    "            # Add to embedding\n",
    "            embedding += word_vector * position_weight\n",
    "            \n",
    "        # Apply IDF-like scaling based on word frequency\n",
    "        for word, count in word_count.items():\n",
    "            # Reduce impact of very frequent words\n",
    "            scaling = 1.0 / np.sqrt(count)\n",
    "            word_vector = self._create_word_vector(word)\n",
    "            embedding += word_vector * scaling\n",
    "            \n",
    "        # Normalize the final embedding\n",
    "        embedding_norm = np.linalg.norm(embedding)\n",
    "        if embedding_norm > 0:\n",
    "            embedding = embedding / embedding_norm\n",
    "            \n",
    "        # Cache the result if caching is enabled\n",
    "        if use_cache:\n",
    "            if len(self.cache) >= self.cache_size:\n",
    "                # Remove oldest entry if cache is full\n",
    "                self.cache.pop(next(iter(self.cache)))\n",
    "            self.cache[text] = embedding\n",
    "            \n",
    "        return embedding\n",
    "        \n",
    "    def compute_similarity(self, \n",
    "                          vector1: np.ndarray, \n",
    "                          vector2: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Compute cosine similarity between two vectors.\n",
    "        \"\"\"\n",
    "        if vector1.shape != vector2.shape:\n",
    "            raise ValueError(\"Vectors must have the same dimensions\")\n",
    "            \n",
    "        dot_product = np.dot(vector1, vector2)\n",
    "        return float(dot_product)  # Vectors are already normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our embedding system includes several sophisticated features:\n",
    "\n",
    "1. **Position Weighting**: Words earlier in the text get slightly more weight\n",
    "2. **Frequency Scaling**: Reduces the impact of very frequent words\n",
    "3. **Caching System**: Maintains a fixed-size cache of computed embeddings\n",
    "4. **Deterministic Word Vectors**: Creates consistent vectors for words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retrieval System Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retrieval system finds relevant chunks based on semantic similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from heapq import nlargest\n",
    "\n",
    "class RetrievalSystem:\n",
    "    def __init__(self, \n",
    "                 embedding_engine: EmbeddingEngine,\n",
    "                 top_k: int = 5,\n",
    "                 similarity_threshold: float = 0.2):  # Lowered threshold for better recall\n",
    "        \"\"\"\n",
    "        Initialize the retrieval system with more lenient similarity matching.\n",
    "        \n",
    "        Parameters:\n",
    "            embedding_engine: Instance of EmbeddingEngine for vector operations\n",
    "            top_k: Number of chunks to retrieve\n",
    "            similarity_threshold: Minimum similarity score to consider (lowered from 0.5)\n",
    "        \"\"\"\n",
    "        self.embedding_engine = embedding_engine\n",
    "        self.top_k = top_k\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.chunk_vectors = []\n",
    "        self.chunks = []\n",
    "        \n",
    "    def add_chunk(self, \n",
    "                  chunk: Dict[str, Any],\n",
    "                  vector: Optional[np.ndarray] = None) -> None:\n",
    "        \"\"\"\n",
    "        Add a chunk and its vector to the retrieval system.\n",
    "        Prints confirmation for debugging purposes.\n",
    "        \"\"\"\n",
    "        if vector is None:\n",
    "            vector = self.embedding_engine.compute_embedding(chunk['text'])\n",
    "            \n",
    "        self.chunk_vectors.append(vector)\n",
    "        self.chunks.append(chunk)\n",
    "        print(f\"Added chunk with id: {chunk.get('id', 'unknown')}\")  # Debug print\n",
    "        \n",
    "    def search(self, \n",
    "               query: str,\n",
    "               filters: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Search for relevant chunks based on query with improved logging.\n",
    "        \n",
    "        Parameters:\n",
    "            query: Search query text\n",
    "            filters: Optional metadata filters to apply\n",
    "            \n",
    "        Returns:\n",
    "            List of relevant chunks with similarity scores\n",
    "        \"\"\"\n",
    "        # Compute query vector\n",
    "        query_vector = self.embedding_engine.compute_embedding(query)\n",
    "        \n",
    "        # Initialize results\n",
    "        results = []\n",
    "        \n",
    "        # Compare with all chunks\n",
    "        for idx, (chunk_vector, chunk) in enumerate(zip(self.chunk_vectors, self.chunks)):\n",
    "            # Check filters if provided\n",
    "            if filters and not self._apply_filters(chunk, filters):\n",
    "                continue\n",
    "                \n",
    "            # Compute similarity\n",
    "            similarity = self.embedding_engine.compute_similarity(\n",
    "                query_vector, chunk_vector\n",
    "            )\n",
    "            \n",
    "            print(f\"Chunk {idx} similarity: {similarity}\")  # Debug print\n",
    "            \n",
    "            # Add to results if above threshold\n",
    "            if similarity >= self.similarity_threshold:\n",
    "                results.append({\n",
    "                    'chunk': chunk,\n",
    "                    'similarity': float(similarity),  # Ensure similarity is regular float\n",
    "                    'index': idx\n",
    "                })\n",
    "                \n",
    "        # Sort by similarity and get top k\n",
    "        top_results = sorted(\n",
    "            results, \n",
    "            key=lambda x: x['similarity'],\n",
    "            reverse=True\n",
    "        )[:self.top_k]\n",
    "        \n",
    "        return top_results\n",
    "        \n",
    "    def _apply_filters(self, \n",
    "                      chunk: Dict[str, Any], \n",
    "                      filters: Dict[str, Any]) -> bool:\n",
    "        \"\"\"\n",
    "        Apply metadata filters to a chunk.\n",
    "        \"\"\"\n",
    "        for key, value in filters.items():\n",
    "            chunk_value = chunk.get(key)\n",
    "            if chunk_value != value:\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retrieval system includes several important features:\n",
    "\n",
    "1. **Efficient Search**: Uses vector similarity for semantic matching\n",
    "2. **Filtering**: Supports metadata-based filtering of results\n",
    "3. **Threshold-based Selection**: Only returns sufficiently similar results\n",
    "4. **Flexible Ranking**: Sorts results by similarity score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Complete RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's put everything together in a complete RAG pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from typing import List, Dict\n",
    "import os\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(self, openai_api_key: str):\n",
    "        self.embedding_engine = EmbeddingEngine()\n",
    "        self.retrieval_system = RetrievalSystem(self.embedding_engine)\n",
    "        self.client = OpenAI(api_key=openai_api_key)\n",
    "        \n",
    "        # Enhanced system prompt for better context utilization\n",
    "        self.system_prompt = \"\"\"You are a helpful assistant that answers questions based on the provided context. \n",
    "        Your responses should:\n",
    "        1. Use specific information from the provided contexts\n",
    "        2. Acknowledge when information is or isn't available in the context\n",
    "        3. Synthesize information from multiple contexts when relevant\n",
    "        4. Stay focused on answering the specific question asked\n",
    "        \n",
    "        If the context contains relevant information, use it to provide a detailed answer.\n",
    "        If the context doesn't contain enough information, clearly state what information is missing.\"\"\"\n",
    "    \n",
    "    def add_document(self, document: Dict[str, str]) -> None:\n",
    "        \"\"\"\n",
    "        Add a document to our knowledge base with improved error handling.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            chunk = {\n",
    "                'text': document['text'].strip(),  # Clean whitespace\n",
    "                'source': document.get('source', 'unknown'),\n",
    "                'id': document.get('id', 'unknown')\n",
    "            }\n",
    "            \n",
    "            vector = self.embedding_engine.compute_embedding(chunk['text'])\n",
    "            self.retrieval_system.add_chunk(chunk, vector)\n",
    "            print(f\"Successfully added document {chunk['id']}\")  # Debug print\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding document: {str(e)}\")\n",
    "    \n",
    "    def query(self, question: str, temperature: float = 0.7) -> Dict:\n",
    "        \"\"\"\n",
    "        Process a question with improved context handling and response generation.\n",
    "        \"\"\"\n",
    "        # Retrieve relevant contexts\n",
    "        relevant_contexts = self.retrieval_system.search(question)\n",
    "        \n",
    "        if not relevant_contexts:\n",
    "            return {\n",
    "                'answer': \"I apologize, but I couldn't find any relevant information in the provided contexts to answer your question.\",\n",
    "                'contexts': [],\n",
    "                'model': 'gpt-3.5-turbo'\n",
    "            }\n",
    "        \n",
    "        # Format contexts with more structure\n",
    "        formatted_contexts = \"\\n\\n\".join([\n",
    "            f\"Context {i+1} (Relevance Score: {ctx['similarity']:.3f}):\\n{ctx['chunk']['text']}\"\n",
    "            for i, ctx in enumerate(relevant_contexts)\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"\"\"Please answer this question based on the following contexts:\n",
    "                    \n",
    "                    {formatted_contexts}\n",
    "                    \n",
    "                    Question: {question}\n",
    "                    \n",
    "                    Remember to use specific information from the contexts and acknowledge if any relevant information is missing.\"\"\"}\n",
    "                ],\n",
    "                temperature=temperature\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'answer': response.choices[0].message.content,\n",
    "                'contexts': relevant_contexts,\n",
    "                'model': 'gpt-3.5-turbo'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'error': f\"Error generating response: {str(e)}\",\n",
    "                'contexts': relevant_contexts\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your actual API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_rag(api_key: str):\n",
    "    \"\"\"\n",
    "    Demonstrate the RAG system's capabilities with example documents and questions.\n",
    "    \n",
    "    This function shows how the system:\n",
    "    1. Processes and stores documents\n",
    "    2. Retrieves relevant information\n",
    "    3. Generates context-aware answers\n",
    "    \"\"\"\n",
    "    # Initialize our RAG system\n",
    "    rag = RAGPipeline(api_key)\n",
    "    \n",
    "    # Our sample documents about the solar system and Earth\n",
    "    documents = [\n",
    "        {\n",
    "            'id': '1',\n",
    "            'text': '''\n",
    "            The solar system consists of the Sun and all the objects that orbit around it. \n",
    "            The Sun contains 99.8% of all the mass in our solar system. The planets that \n",
    "            orbit the Sun are Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and \n",
    "            Neptune. Each planet has unique characteristics and conditions.\n",
    "            ''',\n",
    "            'source': 'astronomy_basics'\n",
    "        },\n",
    "        {\n",
    "            'id': '2',\n",
    "            'text': '''\n",
    "            Earth is the third planet from the Sun and the only known planet to support life. \n",
    "            It has one natural satellite, the Moon, which influences Earth's tides. The Earth's \n",
    "            atmosphere is composed mainly of nitrogen (78%) and oxygen (21%), creating conditions \n",
    "            perfect for life as we know it.\n",
    "            ''',\n",
    "            'source': 'earth_science'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"Adding documents to the RAG system...\")\n",
    "    for document in documents:\n",
    "        rag.add_document(document)\n",
    "    \n",
    "    print(\"\\nDemonstrating RAG System Question-Answering:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test questions that explore different aspects of our documents\n",
    "    questions = [\n",
    "        \"What is unique about Earth compared to other planets?\",\n",
    "        \"How does the Moon affect Earth?\",\n",
    "        \"What are the main components of Earth's atmosphere?\"\n",
    "    ]\n",
    "    \n",
    "    for question in questions:\n",
    "        print(f\"\\nQuestion: {question}\")\n",
    "        result = rag.query(question)\n",
    "        \n",
    "        if 'error' not in result:\n",
    "            print(\"\\nAnswer:\")\n",
    "            print(result['answer'])\n",
    "            print(\"\\nBased on these contexts:\")\n",
    "            for i, ctx in enumerate(result['contexts'], 1):\n",
    "                print(f\"\\nContext {i} (Relevance: {ctx['similarity']:.3f}):\")\n",
    "                print(ctx['chunk']['text'])\n",
    "        else:\n",
    "            print(\"\\nError:\", result['error'])\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the demonstration using the environment variable for the API key\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "demonstrate_rag(os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above demonstration, it can be observed that the relevance score of Context1 which uses the RAG implementation is higher than of the Context2 which does not uses the RAG."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

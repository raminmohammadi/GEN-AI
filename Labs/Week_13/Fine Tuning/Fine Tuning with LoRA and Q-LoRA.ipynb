{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Fine Tuning</h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief Recap of Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning techniques are specialized methods for adapting pre-trained language models to specific tasks or domains. These techniques have revolutionized NLP by making it more efficient and accessible to fine-tune large language models with limited computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Traditional Fine-tuning is Challenging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **High Computational Cost**: Fine-tuning the entire model requires significant computational resources, as all parameters are updated during training.\n",
    "  \n",
    "2. **Large Storage Requirement**: Each fine-tuned model copy occupies substantial storage, which scales poorly with the number of tasks or datasets.\n",
    "\n",
    "3. **Catastrophic Forgetting**: Updating all parameters can lead to the loss of knowledge from the pre-trained model, making it less effective on tasks outside the fine-tuning domain.\n",
    "\n",
    "4. **Inefficiency for Large Models**: For large-scale models like GPT or LLaMA, fine-tuning is resource-intensive, requiring extensive GPU/TPU memory.\n",
    "\n",
    "5. **Limited Adaptability**: Fine-tuned models are specialized for a single task, making reuse for other tasks less feasible without further fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"static/image1.gif\" alt=\"Fine Tuning\" style=\"width:50%;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding LoRA (Low-Rank Adaptation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"static/image2.gif\" alt=\"Fine Tuning with LoRA\" style=\"width:50%;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem LoRA Solves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Resource Intensity**\n",
    "   - Full fine-tuning requires updating all parameters\n",
    "   - High memory requirements (2-3x model size)\n",
    "   - Expensive computational resources needed\n",
    "\n",
    "2. **Storage Overhead**\n",
    "   - Each fine-tuned version needs full model storage\n",
    "   - Multiple task adaptations become impractical\n",
    "   - Version management becomes complex\n",
    "\n",
    "3. **Training Efficiency**\n",
    "   - Long training times\n",
    "   - High energy consumption\n",
    "   - Limited parallel adaptations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Novel Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Low-Rank Decomposition**\n",
    "   - Represents weight updates as low-rank matrices\n",
    "   - Uses matrix factorization for efficiency\n",
    "   - Minimizes parameter count while maintaining performance\n",
    "\n",
    "2. **Frozen Weights**\n",
    "   - Original model weights remain unchanged\n",
    "   - Only train small adaptation matrices\n",
    "   - Preserves pre-trained knowledge\n",
    "\n",
    "3. **Parameter-Efficient Updates**\n",
    "   - Updates through small matrices (A and B)\n",
    "   - Rank determines compression ratio\n",
    "   - Trainable parameters reduced significantly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Efficiency**\n",
    "   - Typically <1% of original parameters\n",
    "   - Fast training convergence\n",
    "   - Minimal memory overhead\n",
    "\n",
    "2. **Adaptability**\n",
    "   - Task-specific adaptations\n",
    "   - Multiple adaptations can coexist\n",
    "   - Easy to switch between tasks\n",
    "\n",
    "3. **Performance**\n",
    "   - Comparable to full fine-tuning\n",
    "   - Stable training dynamics\n",
    "   - Good generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How LoRA Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Weight Update Decomposition**:\n",
    "    ```\n",
    "    ΔW = BA\n",
    "    where:\n",
    "    - ΔW ∈ ℝᵐˣⁿ (weight update)\n",
    "    - B ∈ ℝᵐˣʳ (first adaptation matrix)\n",
    "    - A ∈ ℝʳˣⁿ (second adaptation matrix)\n",
    "    - r is the rank (typically 8, 16, or 32)\n",
    "    ```\n",
    "\n",
    "2. **Forward Pass Computation**:\n",
    "    ```\n",
    "    Y = XW + α(X(BA))\n",
    "    where:\n",
    "    - X is input\n",
    "    - W is original weights\n",
    "    - α is scaling factor\n",
    "    - BA is LoRA update\n",
    "    ```\n",
    "\n",
    "3. **Parameter Reduction**:\n",
    "    ```\n",
    "    Original parameters: m × n\n",
    "    LoRA parameters: r × (m + n)\n",
    "    Reduction ratio: (r × (m + n)) / (m × n)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Implementation Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAConfig:\n",
    "    def __init__(self,\n",
    "                 rank=8,\n",
    "                 alpha=32,\n",
    "                 target_modules=None,\n",
    "                 dropout=0.1):\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.target_modules = target_modules or ['query', 'key', 'value']\n",
    "        self.dropout = dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the above code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose**\n",
    "\n",
    "This class serves as a configuration container for LoRA hyperparameters and settings. It centralizes all LoRA-specific parameters in one place for easy management and modification.\n",
    "\n",
    "**Parameters Explained:**\n",
    "\n",
    "1. **rank (default=8)**\n",
    "   - Defines the dimension of low-rank matrices\n",
    "   - Controls compression ratio and memory savings\n",
    "   - Lower rank = more compression but potentially less capacity\n",
    "   - Common values: 8, 16, 32\n",
    "   - Formula: compression ≈ 2r/(d_in + d_out)\n",
    "\n",
    "2. **alpha (default=32)**\n",
    "   - Scaling factor for LoRA updates\n",
    "   - Controls the magnitude of adaptations\n",
    "   - Usually set to match or be larger than rank\n",
    "   - Helps stabilize training\n",
    "   - Formula: output = original + (alpha * LoRA_output)\n",
    "\n",
    "3. **target_modules (default=['query', 'key', 'value'])**\n",
    "   - Specifies which layers to apply LoRA to\n",
    "   - Defaults to attention mechanism components\n",
    "   - Can be customized for different architectures\n",
    "   - Common targets:\n",
    "     - query: Query projection in attention\n",
    "     - key: Key projection in attention\n",
    "     - value: Value projection in attention\n",
    "\n",
    "4. **dropout (default=0.1)**\n",
    "   - Dropout rate for LoRA layers\n",
    "   - Helps prevent overfitting\n",
    "   - Applied only to LoRA path, not base model\n",
    "   - Standard range: 0.0-0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Initialization Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAInitialization:\n",
    "    @staticmethod\n",
    "    def init_weights_a(shape, rank):\n",
    "        # Kaiming/He initialization scaled by rank\n",
    "        std = np.sqrt(2.0 / float(shape[0])) / rank\n",
    "        return tf.random.normal(shape, stddev=std)\n",
    "    \n",
    "    @staticmethod\n",
    "    def init_weights_b(shape):\n",
    "        # Zero initialization for stability\n",
    "        return tf.zeros(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the above code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose**\n",
    "\n",
    "This class handles the initialization strategies for the two LoRA matrices (A and B). It uses different initialization approaches for each matrix to ensure stable training and good convergence.\n",
    "\n",
    "**Methods Explained:**\n",
    "\n",
    "1. **init_weights_a**\n",
    "    ```python\n",
    "    @staticmethod\n",
    "    def init_weights_a(shape, rank):\n",
    "        std = np.sqrt(2.0 / float(shape[0])) / rank\n",
    "        return tf.random.normal(shape, stddev=std)\n",
    "    ```\n",
    "      \n",
    "    - **Purpose**: Initializes the first LoRA matrix (A)\n",
    "    - **Uses Kaiming/He Initialization**:\n",
    "      - Designed for ReLU-based networks\n",
    "      - Helps maintain variance across layers\n",
    "      - Scaled by rank for stability\n",
    "    - **Parameters**:\n",
    "      - shape: Dimensions of matrix A\n",
    "      - rank: LoRA rank parameter\n",
    "    - **Formula Breakdown**:\n",
    "      - `2.0 / float(shape[0])`: He initialization base\n",
    "      - `/rank`: Additional scaling for LoRA stability\n",
    "      - Result used as standard deviation for normal distribution\n",
    "\n",
    "2. **init_weights_b**\n",
    "    ```python\n",
    "    @staticmethod\n",
    "    def init_weights_b(shape):\n",
    "        return tf.zeros(shape)\n",
    "    ```\n",
    "\n",
    "    - **Purpose**: Initializes the second LoRA matrix (B)\n",
    "    - **Uses Zero Initialization**:\n",
    "      - Ensures LoRA starts with no initial impact\n",
    "      - Allows gradual learning of adaptations\n",
    "      - Promotes stability in early training\n",
    "    - **Parameters**:\n",
    "      - shape: Dimensions of matrix B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing LoRA in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LoRA Layer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, \n",
    "                 original_layer,\n",
    "                 rank=8,\n",
    "                 alpha=32,\n",
    "                 dropout_rate=0.1,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.original_layer = original_layer\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # Initialize shapes\n",
    "        self.original_shape = original_layer.get_weights()[0].shape\n",
    "        \n",
    "        # Create LoRA matrices\n",
    "        self.lora_a = self._create_lora_matrix(\"a\")\n",
    "        self.lora_b = self._create_lora_matrix(\"b\")\n",
    "        \n",
    "        # Create dropout layer\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        # Freeze original weights\n",
    "        self.original_layer.trainable = False\n",
    "        \n",
    "    def _create_lora_matrix(self, name):\n",
    "        if name == \"a\":\n",
    "            shape = (self.original_shape[0], self.rank)\n",
    "            initializer = LoRAInitialization.init_weights_a\n",
    "        else:\n",
    "            shape = (self.rank, self.original_shape[1])\n",
    "            initializer = LoRAInitialization.init_weights_b\n",
    "            \n",
    "        return self.add_weight(\n",
    "            name=f\"lora_{name}\",\n",
    "            shape=shape,\n",
    "            initializer=initializer,\n",
    "            trainable=True\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        # Original transformation\n",
    "        original_output = self.original_layer(inputs)\n",
    "        \n",
    "        # LoRA path with dropout\n",
    "        lora_input = inputs\n",
    "        if training:\n",
    "            lora_input = self.dropout(lora_input, training=training)\n",
    "        \n",
    "        # LoRA transformation\n",
    "        lora_output = tf.matmul(\n",
    "            tf.matmul(lora_input, self.lora_a),\n",
    "            self.lora_b\n",
    "        )\n",
    "        \n",
    "        # Combine with scaling\n",
    "        return original_output + (self.alpha * lora_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the above code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Class Initialization\n",
    "    ```python\n",
    "    def __init__(self, \n",
    "                original_layer,\n",
    "                rank=8,\n",
    "                alpha=32,\n",
    "                dropout_rate=0.1,\n",
    "                **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    ```\n",
    "    - Inherits from TensorFlow's base Layer class\n",
    "    - Takes original layer and LoRA parameters\n",
    "    - Parameters:\n",
    "    - original_layer: Base layer to adapt\n",
    "    - rank: Dimension of low-rank matrices\n",
    "    - alpha: Scaling factor\n",
    "    - dropout_rate: Regularization strength\n",
    "\n",
    "2. Setup and Initialization\n",
    "    ```python\n",
    "    # Store parameters\n",
    "    self.original_layer = original_layer\n",
    "    self.rank = rank\n",
    "    self.alpha = alpha\n",
    "    self.dropout_rate = dropout_rate\n",
    "\n",
    "    # Get shape from original layer\n",
    "    self.original_shape = original_layer.get_weights()[0].shape\n",
    "    ```\n",
    "    - Stores configuration parameters\n",
    "    - Extracts shape from original layer weights\n",
    "    - Prepares for LoRA matrix creation\n",
    "\n",
    "3. Matrix Creation Helper\n",
    "    ```python\n",
    "    def _create_lora_matrix(self, name):\n",
    "        if name == \"a\":\n",
    "            shape = (self.original_shape[0], self.rank)\n",
    "            initializer = LoRAInitialization.init_weights_a\n",
    "        else:\n",
    "            shape = (self.rank, self.original_shape[1])\n",
    "            initializer = LoRAInitialization.init_weights_b\n",
    "    ```\n",
    "    - Creates LoRA matrices A and B\n",
    "    - Matrix A: input_dim × rank\n",
    "    - Matrix B: rank × output_dim\n",
    "    - Uses different initializations for each matrix\n",
    "\n",
    "4. Forward Pass Implementation\n",
    "    ```python\n",
    "    def call(self, inputs, training=None):\n",
    "        # Original transformation\n",
    "        original_output = self.original_layer(inputs)\n",
    "        \n",
    "        # LoRA path with dropout\n",
    "        lora_input = inputs\n",
    "        if training:\n",
    "            lora_input = self.dropout(lora_input, training=training)\n",
    "        \n",
    "        # LoRA transformation\n",
    "        lora_output = tf.matmul(\n",
    "            tf.matmul(lora_input, self.lora_a),\n",
    "            self.lora_b\n",
    "        )\n",
    "    ```\n",
    "    - Implements forward pass computation\n",
    "    - Steps:\n",
    "    1. Compute original layer output\n",
    "    2. Apply dropout during training\n",
    "    3. Compute LoRA transformation\n",
    "    4. Combine results with scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Components:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Original Layer Handling\n",
    "    ```python\n",
    "    self.original_layer = original_layer\n",
    "    self.original_layer.trainable = False\n",
    "    ```\n",
    "    - Stores original layer\n",
    "    - Freezes original weights\n",
    "\n",
    "2. LoRA Matrices\n",
    "    ```python\n",
    "    self.lora_a = self._create_lora_matrix(\"a\")\n",
    "    self.lora_b = self._create_lora_matrix(\"b\")\n",
    "    ```\n",
    "    - Creates two trainable matrices\n",
    "    - Different initialization strategies\n",
    "    - Shapes determined by original layer\n",
    "\n",
    "3. Dropout Implementation\n",
    "    ```python\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    ```\n",
    "    - Adds regularization\n",
    "    - Only applied during training\n",
    "    - Applied to LoRA path only\n",
    "\n",
    "4. Forward Pass Logic\n",
    "    ```python\n",
    "    return original_output + (self.alpha * lora_output)\n",
    "    ```\n",
    "    - Combines original and LoRA paths\n",
    "    - Scales LoRA contribution\n",
    "    - Maintains original layer behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Parameter Efficiency**: Only trains LoRA matrices\n",
    "2. **Original Preservation**: Base model unchanged\n",
    "3. **Regularization**: Dropout for stability\n",
    "4. **Flexibility**: Adaptable to any dense layer\n",
    "5. **Training Focus**: Only updates LoRA parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model Adapter Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAModelAdapter:\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 config: LoRAConfig):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.lora_layers = []\n",
    "        \n",
    "    def adapt_layer(self, layer):\n",
    "        \"\"\"Apply LoRA adaptation to a single layer\"\"\"\n",
    "        if isinstance(layer, tf.keras.layers.Dense):\n",
    "            return LoRALayer(\n",
    "                layer,\n",
    "                rank=self.config.rank,\n",
    "                alpha=self.config.alpha,\n",
    "                dropout_rate=self.config.dropout\n",
    "            )\n",
    "        return layer\n",
    "    \n",
    "    def create_adapted_model(self):\n",
    "        \"\"\"Create a new model with LoRA adaptations\"\"\"\n",
    "        def clone_function(layer):\n",
    "            if any(name in layer.name \n",
    "                  for name in self.config.target_modules):\n",
    "                adapted_layer = self.adapt_layer(layer)\n",
    "                if isinstance(adapted_layer, LoRALayer):\n",
    "                    self.lora_layers.append(adapted_layer)\n",
    "                return adapted_layer\n",
    "            return layer\n",
    "        \n",
    "        adapted_model = tf.keras.models.clone_model(\n",
    "            self.model,\n",
    "            clone_function=clone_function\n",
    "        )\n",
    "        \n",
    "        return adapted_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the above code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Class Initialization\n",
    "    ```python\n",
    "    def __init__(self, model, config: LoRAConfig):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.lora_layers = []\n",
    "    ```\n",
    "    - **Purpose**: Initializes the adapter with:\n",
    "    - model: Original model to adapt\n",
    "    - config: LoRA configuration settings\n",
    "    - lora_layers: Tracks created LoRA layers\n",
    "    - **Type Hint**: Expects LoRAConfig object for configuration\n",
    "\n",
    "2. Layer Adaptation Method\n",
    "    ```python\n",
    "    def adapt_layer(self, layer):\n",
    "        \"\"\"Apply LoRA adaptation to a single layer\"\"\"\n",
    "        if isinstance(layer, tf.keras.layers.Dense):\n",
    "            return LoRALayer(\n",
    "                layer,\n",
    "                rank=self.config.rank,\n",
    "                alpha=self.config.alpha,\n",
    "                dropout_rate=self.config.dropout\n",
    "            )\n",
    "        return layer\n",
    "    ```\n",
    "    - **Purpose**: Converts single layer to LoRA version\n",
    "    - **Process**:\n",
    "    1. Checks if layer is Dense type\n",
    "    2. Creates LoRA version if applicable\n",
    "    3. Returns original layer if not Dense\n",
    "    - **Parameters**: Uses configuration values for:\n",
    "    - rank\n",
    "    - alpha\n",
    "    - dropout_rate\n",
    "\n",
    "3. Model Adaptation Method\n",
    "    ```python\n",
    "    def create_adapted_model(self):\n",
    "        \"\"\"Create a new model with LoRA adaptations\"\"\"\n",
    "        def clone_function(layer):\n",
    "            if any(name in layer.name \n",
    "                for name in self.config.target_modules):\n",
    "                adapted_layer = self.adapt_layer(layer)\n",
    "                if isinstance(adapted_layer, LoRALayer):\n",
    "                    self.lora_layers.append(adapted_layer)\n",
    "                return adapted_layer\n",
    "            return layer\n",
    "        \n",
    "        adapted_model = tf.keras.models.clone_model(\n",
    "            self.model,\n",
    "            clone_function=clone_function\n",
    "        )\n",
    "        \n",
    "        return adapted_model\n",
    "    ```\n",
    "    - **Purpose**: Creates complete LoRA-adapted model\n",
    "    - **Process**:\n",
    "    1. Defines clone function for layer handling\n",
    "    2. Checks layer names against target modules\n",
    "    3. Adapts matching layers\n",
    "    4. Tracks created LoRA layers\n",
    "    5. Clones entire model with adaptations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Selective Adaptation\n",
    "    ```python\n",
    "    if any(name in layer.name for name in self.config.target_modules)\n",
    "    ```\n",
    "    - Only adapts specified layers\n",
    "    - Maintains original architecture\n",
    "    - Configurable targeting\n",
    "\n",
    "2. Layer Tracking\n",
    "    ```python\n",
    "    self.lora_layers.append(adapted_layer)\n",
    "    ```\n",
    "    - Keeps record of LoRA layers\n",
    "    - Enables monitoring\n",
    "    - Facilitates management\n",
    "\n",
    "3. Model Preservation\n",
    "    ```python\n",
    "    adapted_model = tf.keras.models.clone_model(...)\n",
    "    ```\n",
    "    - Creates new model instance\n",
    "    - Preserves original model\n",
    "    - Safe adaptation process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Points:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Modularity**: \n",
    "   - Clean separation of concerns\n",
    "   - Reusable components\n",
    "   - Configurable behavior\n",
    "\n",
    "2. **Safety**:\n",
    "   - Non-destructive adaptation\n",
    "   - Original model preserved\n",
    "   - Controlled modifications\n",
    "\n",
    "3. **Flexibility**:\n",
    "   - Configurable targeting\n",
    "   - Adaptable to different architectures\n",
    "   - Easy to extend\n",
    "\n",
    "4. **Management**:\n",
    "   - Tracks adaptations\n",
    "   - Organized structure\n",
    "   - Easy monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRATrainingManager:\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 learning_rate=1e-4,\n",
    "                 weight_decay=0.01):\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        self.optimizer = self._create_optimizer()\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name='loss')\n",
    "        \n",
    "    def _create_optimizer(self):\n",
    "        return tf.keras.optimizers.AdamW(\n",
    "            learning_rate=self.learning_rate,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, inputs, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            predictions = self.model(inputs, training=True)\n",
    "            # Calculate loss\n",
    "            loss = self.compute_loss(labels, predictions)\n",
    "            \n",
    "        # Get trainable variables (only LoRA parameters)\n",
    "        trainable_vars = [var for var in self.model.trainable_variables\n",
    "                         if 'lora_' in var.name]\n",
    "        \n",
    "        # Compute gradients\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        \n",
    "        # Apply gradients\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        \n",
    "        # Update metrics\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": self.loss_tracker.result()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the above code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Class Initialization\n",
    "    ```python\n",
    "    def __init__(self,\n",
    "                model,\n",
    "                learning_rate=1e-4,\n",
    "                weight_decay=0.01):\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        self.optimizer = self._create_optimizer()\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name='loss')\n",
    "    ```\n",
    "    - **Purpose**: Sets up training environment\n",
    "    - **Parameters**:\n",
    "    - model: LoRA-adapted model\n",
    "    - learning_rate: Training rate (default: 0.0001)\n",
    "    - weight_decay: L2 regularization (default: 0.01)\n",
    "    - **Components**:\n",
    "    - Creates optimizer\n",
    "    - Initializes loss tracking\n",
    "\n",
    "2. Optimizer Creation\n",
    "    ```python\n",
    "    def _create_optimizer(self):\n",
    "        return tf.keras.optimizers.AdamW(\n",
    "            learning_rate=self.learning_rate,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "    ```\n",
    "    - **Purpose**: Initializes AdamW optimizer\n",
    "    - **Features**:\n",
    "    - Adaptive learning rates\n",
    "    - Weight decay regularization\n",
    "    - Momentum-based updates\n",
    "\n",
    "3. Training Step Implementation\n",
    "    ```python\n",
    "    @tf.function  # Compiler decorator for performance\n",
    "    def train_step(self, inputs, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            predictions = self.model(inputs, training=True)\n",
    "            # Calculate loss\n",
    "            loss = self.compute_loss(labels, predictions)\n",
    "    ```\n",
    "    - **Purpose**: Executes single training iteration\n",
    "    - **Process**:\n",
    "    1. Records operations for gradient computation\n",
    "    2. Performs forward pass\n",
    "    3. Calculates loss\n",
    "\n",
    "4. Gradient Computation and Application\n",
    "    ```python\n",
    "    # Get trainable variables (only LoRA parameters)\n",
    "    trainable_vars = [var for var in self.model.trainable_variables\n",
    "                    if 'lora_' in var.name]\n",
    "\n",
    "    # Compute gradients\n",
    "    gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "    # Apply gradients\n",
    "    self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "    ```\n",
    "    - **Purpose**: Updates LoRA parameters\n",
    "    - **Features**:\n",
    "    - Selects only LoRA variables\n",
    "    - Computes gradients\n",
    "    - Applies updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Components:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Loss Tracking\n",
    "    ```python\n",
    "    self.loss_tracker = tf.keras.metrics.Mean(name='loss')\n",
    "    self.loss_tracker.update_state(loss)\n",
    "    ```\n",
    "    - Maintains running average of loss\n",
    "    - Tracks training progress\n",
    "    - Returns current metrics\n",
    "\n",
    "2. LoRA Parameter Selection\n",
    "    ```python\n",
    "    trainable_vars = [var for var in self.model.trainable_variables\n",
    "                    if 'lora_' in var.name]\n",
    "    ```\n",
    "    - Filters for LoRA parameters\n",
    "    - Ignores frozen base model\n",
    "    - Efficient update process\n",
    "\n",
    "3. Gradient Management\n",
    "    ```python\n",
    "    gradients = tape.gradient(loss, trainable_vars)\n",
    "    self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "    ```\n",
    "    - Computes parameter updates\n",
    "    - Applies optimization steps\n",
    "    - Manages learning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Points:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Efficiency**:\n",
    "   - Only updates LoRA parameters\n",
    "   - Optimized with @tf.function\n",
    "   - Efficient memory usage\n",
    "\n",
    "2. **Organization**:\n",
    "   - Clear training workflow\n",
    "   - Centralized management\n",
    "   - Easy monitoring\n",
    "\n",
    "3. **Flexibility**:\n",
    "   - Configurable parameters\n",
    "   - Adaptable to different tasks\n",
    "   - Easy to extend\n",
    "\n",
    "4. **Performance**:\n",
    "   - Gradient computation optimization\n",
    "   - Efficient parameter updates\n",
    "   - Progress tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Complete Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_lora(\n",
    "    base_model,\n",
    "    train_dataset,\n",
    "    validation_dataset,\n",
    "    config: LoRAConfig\n",
    "):\n",
    "    # Create LoRA adapter\n",
    "    adapter = LoRAModelAdapter(base_model, config)\n",
    "    adapted_model = adapter.create_adapted_model()\n",
    "    \n",
    "    # Initialize training manager\n",
    "    trainer = LoRATrainingManager(adapted_model)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(config.epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{config.epochs}\")\n",
    "        \n",
    "        # Train\n",
    "        for batch in train_dataset:\n",
    "            metrics = trainer.train_step(\n",
    "                batch['input_ids'],\n",
    "                batch['labels']\n",
    "            )\n",
    "            \n",
    "        # Validate\n",
    "        val_metrics = trainer.evaluate(validation_dataset)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Training loss: {metrics['loss']:.4f}\")\n",
    "        print(f\"Validation loss: {val_metrics['loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Function Definition\n",
    "    ```python\n",
    "    def train_with_lora(\n",
    "        base_model,\n",
    "        train_dataset,\n",
    "        validation_dataset,\n",
    "        config: LoRAConfig\n",
    "    ):\n",
    "    ```\n",
    "    - **Purpose**: Main training pipeline for LoRA\n",
    "    - **Parameters**:\n",
    "    - base_model: Original model to adapt\n",
    "    - train_dataset: Training data\n",
    "    - validation_dataset: Validation data\n",
    "    - config: LoRA configuration settings\n",
    "\n",
    "2. Model Adaptation\n",
    "    ```python\n",
    "    # Create LoRA adapter\n",
    "    adapter = LoRAModelAdapter(base_model, config)\n",
    "    adapted_model = adapter.create_adapted_model()\n",
    "    ```\n",
    "    - **Purpose**: Sets up LoRA-adapted model\n",
    "    - **Process**:\n",
    "    1. Creates adapter instance\n",
    "    2. Applies LoRA to specified layers\n",
    "    3. Returns adapted model\n",
    "\n",
    "3. Training Setup\n",
    "    ```python\n",
    "    # Initialize training manager\n",
    "    trainer = LoRATrainingManager(adapted_model)\n",
    "    ```\n",
    "    - **Purpose**: Prepares training environment\n",
    "    - **Features**:\n",
    "    - Sets up optimizer\n",
    "    - Initializes loss tracking\n",
    "    - Manages training state\n",
    "\n",
    "4. Training Loop\n",
    "    ```python\n",
    "    # Training loop\n",
    "    for epoch in range(config.epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{config.epochs}\")\n",
    "        \n",
    "        # Train\n",
    "        for batch in train_dataset:\n",
    "            metrics = trainer.train_step(\n",
    "                batch['input_ids'],\n",
    "                batch['labels']\n",
    "            )\n",
    "    ```\n",
    "    - **Purpose**: Executes training process\n",
    "    - **Components**:\n",
    "    - Epoch iteration\n",
    "    - Batch processing\n",
    "    - Metrics collection\n",
    "\n",
    "5. Validation\n",
    "    ```python\n",
    "    # Validate\n",
    "    val_metrics = trainer.evaluate(validation_dataset)\n",
    "    ```\n",
    "    - **Purpose**: Evaluates model performance\n",
    "    - **Process**:\n",
    "    - Runs validation data\n",
    "    - Computes metrics\n",
    "    - Tracks progress\n",
    "\n",
    "6. Progress Reporting\n",
    "    ```python\n",
    "    # Print metrics\n",
    "    print(f\"Training loss: {metrics['loss']:.4f}\")\n",
    "    print(f\"Validation loss: {val_metrics['loss']:.4f}\")\n",
    "    ```\n",
    "    - **Purpose**: Monitors training progress\n",
    "    - **Output**:\n",
    "    - Training loss\n",
    "    - Validation loss\n",
    "    - Formatted metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Organization**:\n",
    "   - Clear workflow\n",
    "   - Modular components\n",
    "   - Structured training\n",
    "\n",
    "2. **Monitoring**:\n",
    "   - Regular progress updates\n",
    "   - Loss tracking\n",
    "   - Validation checks\n",
    "\n",
    "3. **Flexibility**:\n",
    "   - Configurable training\n",
    "   - Adaptable to different models\n",
    "   - Customizable metrics\n",
    "\n",
    "4. **Efficiency**:\n",
    "   - Batch processing\n",
    "   - Optimized training\n",
    "   - Resource management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Flow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Model Preparation:\n",
    "   - Load base model\n",
    "   - Apply LoRA adaptations\n",
    "   - Setup training environment\n",
    "\n",
    "2. Training Execution:\n",
    "   - Iterate through epochs\n",
    "   - Process batches\n",
    "   - Update parameters\n",
    "\n",
    "3. Validation:\n",
    "   - Evaluate performance\n",
    "   - Track metrics\n",
    "   - Monitor progress\n",
    "\n",
    "4. Reporting:\n",
    "   - Display metrics\n",
    "   - Track progress\n",
    "   - Monitor convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. **Memory Efficiency**\n",
    "   - Reduced parameter count (<1% of original)\n",
    "   - Lower memory requirements\n",
    "   - Efficient storage of adaptations\n",
    "\n",
    "2. **Training Efficiency**\n",
    "   - Faster convergence\n",
    "   - Lower computational requirements\n",
    "   - Reduced energy consumption\n",
    "\n",
    "3. **Modularity**\n",
    "   - Task-specific adaptations\n",
    "   - Easy switching between tasks\n",
    "   - Simple version management\n",
    "\n",
    "4. **Performance**\n",
    "   - Comparable to full fine-tuning\n",
    "   - Good generalization\n",
    "   - Stable training dynamics\n",
    "\n",
    "5. **Practical Benefits**\n",
    "   - Reduced infrastructure costs\n",
    "   - Faster deployment\n",
    "   - Easier maintenance\n",
    "\n",
    "6. **Technical Benefits**\n",
    "   - Gradient flow optimization\n",
    "   - Stable numerical properties\n",
    "   - Efficient backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Q-LoRA: Quantized Low-Rank Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"static/image3.gif\" alt=\"Q-LoRA\" style=\"width:50%;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem Q-LoRA Solves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Memory Constraints**\n",
    "   - Even LoRA requires full-precision model weights\n",
    "   - 16-bit models still consume significant memory\n",
    "   - Limited by GPU VRAM during training\n",
    "\n",
    "2. **Hardware Limitations**\n",
    "   - Most consumer GPUs can't handle large models\n",
    "   - Training requires expensive specialized hardware\n",
    "   - Multiple GPUs often needed for fine-tuning\n",
    "\n",
    "3. **Accessibility Issues**\n",
    "   - Research limited by hardware requirements\n",
    "   - High computational costs\n",
    "   - Resource-intensive deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Novel Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **4-bit Quantization**\n",
    "   - Reduces model precision from 16/32-bit to 4-bit\n",
    "   - Uses special NormalFloat (NF4) format\n",
    "   - Maintains model quality despite compression\n",
    "\n",
    "2. **Double Quantization**\n",
    "   - Quantizes both weights and quantization constants\n",
    "   - Further reduces memory footprint\n",
    "   - Minimal impact on model performance\n",
    "\n",
    "3. **Paged Attention**\n",
    "   - Efficient memory management\n",
    "   - CPU offloading for attention computations\n",
    "   - Dynamic memory allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Memory Efficiency**\n",
    "   - 85% memory reduction compared to full fine-tuning\n",
    "   - Enables training on consumer GPUs\n",
    "   - Supports larger context windows\n",
    "\n",
    "2. **Quality Preservation**\n",
    "   - Maintains model performance\n",
    "   - Comparable results to full fine-tuning\n",
    "   - Stable training process\n",
    "\n",
    "3. **Accessibility**\n",
    "   - Works on single GPU setups\n",
    "   - Reduces hardware requirements\n",
    "   - Enables broader research participation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Q-LoRA Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **NormalFloat (NF4) Quantization**:\n",
    "    ```\n",
    "    Q(x) = s * round(clamp(x/s, -1, 1) * (2^b - 1)) / (2^b - 1)\n",
    "    where:\n",
    "    - x is the original value\n",
    "    - s is the scaling factor\n",
    "    - b is bits (4 for NF4)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Double Quantization**:\n",
    "    ```\n",
    "    First level: W_q = Q1(W, s1)\n",
    "    Second level: s_q = Q2(s1, s2)\n",
    "    where:\n",
    "    - W is original weights\n",
    "    - Q1, Q2 are quantization functions\n",
    "    - s1, s2 are scaling factors\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Gradient Computation**:\n",
    "    ```\n",
    "    ∂L/∂W = (∂L/∂W_q) * (∂W_q/∂W)\n",
    "    where:\n",
    "    - L is loss function\n",
    "    - W_q is quantized weights\n",
    "    - Straight-through estimator for gradients\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-LoRA Implementation Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLoRAConfig:\n",
    "    def __init__(self,\n",
    "                 bits=4,\n",
    "                 group_size=128,\n",
    "                 double_quant=True,\n",
    "                 quant_type=\"nf4\"):\n",
    "        self.bits = bits\n",
    "        self.group_size = group_size\n",
    "        self.double_quant = double_quant\n",
    "        self.quant_type = quant_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NF4Quantizer:\n",
    "    def __init__(self):\n",
    "        # NF4 quantization levels\n",
    "        self.levels = np.array([\n",
    "            -1.0, -0.72, -0.34, -0.11, \n",
    "            0.0, 0.11, 0.34, 0.72, 1.0\n",
    "        ])\n",
    "    \n",
    "    def quantize(self, x):\n",
    "        # Find nearest quantization level\n",
    "        indices = np.digitize(x, self.levels) - 1\n",
    "        return self.levels[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Q-LoRA in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantized Layer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLoRALayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, \n",
    "                 original_layer,\n",
    "                 rank=8,\n",
    "                 alpha=32,\n",
    "                 bits=4,\n",
    "                 group_size=128,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.original_layer = original_layer\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.bits = bits\n",
    "        self.group_size = group_size\n",
    "        \n",
    "        # Initialize quantization\n",
    "        self.quantizer = self._create_quantizer()\n",
    "        \n",
    "        # Get original shapes\n",
    "        self.original_shape = original_layer.get_weights()[0].shape\n",
    "        \n",
    "        # Initialize LoRA matrices\n",
    "        self.lora_a = self._create_lora_weights(\"a\")\n",
    "        self.lora_b = self._create_lora_weights(\"b\")\n",
    "        \n",
    "        # Freeze original weights\n",
    "        self.original_layer.trainable = False\n",
    "\n",
    "    def _create_quantizer(self):\n",
    "        return {\n",
    "            'scale': tf.Variable(1.0, trainable=False),\n",
    "            'zero_point': tf.Variable(0.0, trainable=False)\n",
    "        }\n",
    "    \n",
    "    def _create_lora_weights(self, name):\n",
    "        if name == \"a\":\n",
    "            shape = (self.original_shape[0], self.rank)\n",
    "        else:\n",
    "            shape = (self.rank, self.original_shape[1])\n",
    "            \n",
    "        return self.add_weight(\n",
    "            name=f\"lora_{name}\",\n",
    "            shape=shape,\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True\n",
    "        )\n",
    "    \n",
    "    def quantize(self, x):\n",
    "        # Apply quantization\n",
    "        scale = self.quantizer['scale']\n",
    "        zero_point = self.quantizer['zero_point']\n",
    "        \n",
    "        # Quantize to specified bits\n",
    "        range_float = 2.0 ** self.bits - 1.0\n",
    "        x_scaled = tf.clip_by_value(x / scale, -1.0, 1.0)\n",
    "        x_scaled_q = tf.round(x_scaled * range_float)\n",
    "        return (x_scaled_q - zero_point) * scale\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Quantize original layer weights\n",
    "        q_weights = self.quantize(self.original_layer.weights[0])\n",
    "        \n",
    "        # Original transformation with quantized weights\n",
    "        original_output = tf.matmul(inputs, q_weights)\n",
    "        \n",
    "        # LoRA transformation\n",
    "        lora_output = tf.matmul(\n",
    "            tf.matmul(inputs, self.lora_a),\n",
    "            self.lora_b\n",
    "        )\n",
    "        \n",
    "        # Combine outputs\n",
    "        return original_output + (self.alpha * lora_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Class Initialization\n",
    "    ```python\n",
    "    def __init__(self, \n",
    "                original_layer,\n",
    "                rank=8,\n",
    "                alpha=32,\n",
    "                bits=4,\n",
    "                group_size=128,\n",
    "                **kwargs):\n",
    "    ```\n",
    "    - **Purpose**: Initializes quantized LoRA layer\n",
    "    - **Parameters**:\n",
    "    - original_layer: Base layer to adapt\n",
    "    - rank: LoRA rank dimension\n",
    "    - alpha: Scaling factor\n",
    "    - bits: Quantization precision (default 4-bit)\n",
    "    - group_size: Quantization group size\n",
    "\n",
    "2. Quantizer Creation\n",
    "    ```python\n",
    "    def _create_quantizer(self):\n",
    "        return {\n",
    "            'scale': tf.Variable(1.0, trainable=False),\n",
    "            'zero_point': tf.Variable(0.0, trainable=False)\n",
    "        }\n",
    "    ```\n",
    "    - **Purpose**: Sets up quantization parameters\n",
    "    - **Components**:\n",
    "    - scale: Scaling factor for quantization\n",
    "    - zero_point: Offset for quantization\n",
    "    - **Features**: Non-trainable variables\n",
    "\n",
    "3. Weight Creation\n",
    "    ```python\n",
    "    def _create_lora_weights(self, name):\n",
    "        if name == \"a\":\n",
    "            shape = (self.original_shape[0], self.rank)\n",
    "        else:\n",
    "            shape = (self.rank, self.original_shape[1])\n",
    "            \n",
    "        return self.add_weight(\n",
    "            name=f\"lora_{name}\",\n",
    "            shape=shape,\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True\n",
    "        )\n",
    "    ```\n",
    "    - **Purpose**: Creates LoRA matrices\n",
    "    - **Features**:\n",
    "    - Matrix A: input_dim × rank\n",
    "    - Matrix B: rank × output_dim\n",
    "    - Trainable parameters\n",
    "\n",
    "4. Quantization Implementation\n",
    "    ```python\n",
    "    def quantize(self, x):\n",
    "        scale = self.quantizer['scale']\n",
    "        zero_point = self.quantizer['zero_point']\n",
    "        \n",
    "        range_float = 2.0 ** self.bits - 1.0\n",
    "        x_scaled = tf.clip_by_value(x / scale, -1.0, 1.0)\n",
    "        x_scaled_q = tf.round(x_scaled * range_float)\n",
    "        return (x_scaled_q - zero_point) * scale\n",
    "    ```\n",
    "    - **Purpose**: Implements weight quantization\n",
    "    - **Process**:\n",
    "    1. Scale input values\n",
    "    2. Clip to range [-1, 1]\n",
    "    3. Quantize to specified bits\n",
    "    4. Rescale to original range\n",
    "\n",
    "5. Forward Pass Implementation\n",
    "    ```python\n",
    "    def call(self, inputs):\n",
    "        # Quantize original weights\n",
    "        q_weights = self.quantize(self.original_layer.weights[0])\n",
    "        \n",
    "        # Original path with quantized weights\n",
    "        original_output = tf.matmul(inputs, q_weights)\n",
    "        \n",
    "        # LoRA path\n",
    "        lora_output = tf.matmul(\n",
    "            tf.matmul(inputs, self.lora_a),\n",
    "            self.lora_b\n",
    "        )\n",
    "        \n",
    "        # Combine outputs\n",
    "        return original_output + (self.alpha * lora_output)\n",
    "    ```\n",
    "    - **Purpose**: Executes forward pass\n",
    "    - **Steps**:\n",
    "    1. Quantize original weights\n",
    "    2. Compute original path\n",
    "    3. Compute LoRA path\n",
    "    4. Combine results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Quantization\n",
    "- 4-bit precision by default\n",
    "- Scale and zero-point tracking\n",
    "- Linear quantization scheme\n",
    "\n",
    "2. Memory Efficiency\n",
    "- Reduced precision storage\n",
    "- Efficient computation\n",
    "- Memory-aware design\n",
    "\n",
    "3. LoRA Integration\n",
    "- Low-rank adaptation\n",
    "- Trainable components\n",
    "- Original weight preservation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mathematical Operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Quantization**:\n",
    "    ```\n",
    "    q(x) = round(clip(x/s, -1, 1) * (2^bits - 1)) * s\n",
    "    where:\n",
    "    - s is scale\n",
    "    - bits is precision\n",
    "    ```\n",
    "\n",
    "2. **Forward Pass**:\n",
    "    ```\n",
    "    output = (input × q(W_original)) + α(input × A × B)\n",
    "    where:\n",
    "    - q() is quantization function\n",
    "    - A, B are LoRA matrices\n",
    "    - α is scaling factor\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Points:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Memory Savings**:\n",
    "   - 4-bit quantization\n",
    "   - Efficient parameter storage\n",
    "   - Reduced memory footprint\n",
    "\n",
    "2. **Computation Efficiency**:\n",
    "   - Quantized operations\n",
    "   - Low-rank updates\n",
    "   - Optimized processing\n",
    "\n",
    "3. **Adaptation Quality**:\n",
    "   - Preserved model behavior\n",
    "   - Fine-tuning capability\n",
    "   - Controlled updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Management Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PagedAttention:\n",
    "    def __init__(self, max_memory=None):\n",
    "        self.max_memory = max_memory\n",
    "        self.cache = {}\n",
    "        \n",
    "    def compute(self, query, key, value):\n",
    "        batch_size = tf.shape(query)[0]\n",
    "        \n",
    "        # Split computation into manageable chunks\n",
    "        chunk_size = self._calculate_chunk_size(query)\n",
    "        num_chunks = tf.shape(query)[1] // chunk_size\n",
    "        \n",
    "        outputs = []\n",
    "        for i in range(num_chunks):\n",
    "            start_idx = i * chunk_size\n",
    "            end_idx = (i + 1) * chunk_size\n",
    "            \n",
    "            # Process chunk\n",
    "            chunk_output = self._process_chunk(\n",
    "                query[:, start_idx:end_idx],\n",
    "                key,\n",
    "                value\n",
    "            )\n",
    "            outputs.append(chunk_output)\n",
    "        \n",
    "        return tf.concat(outputs, axis=1)\n",
    "    \n",
    "    def _calculate_chunk_size(self, tensor):\n",
    "        # Calculate optimal chunk size based on memory\n",
    "        element_size = tensor.dtype.size\n",
    "        return min(\n",
    "            tf.shape(tensor)[1],\n",
    "            self.max_memory // (element_size * tf.shape(tensor)[2])\n",
    "        )\n",
    "    \n",
    "    def _process_chunk(self, query_chunk, key, value):\n",
    "        # Compute attention for chunk\n",
    "        scores = tf.matmul(query_chunk, key, transpose_b=True)\n",
    "        scores = scores / tf.sqrt(tf.cast(tf.shape(key)[-1], tf.float32))\n",
    "        attention = tf.nn.softmax(scores, axis=-1)\n",
    "        return tf.matmul(attention, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Class Initialization\n",
    "    ```python\n",
    "    def __init__(self, max_memory=None):\n",
    "        self.max_memory = max_memory\n",
    "        self.cache = {}\n",
    "    ```\n",
    "    - **Purpose**: Initializes paged attention system\n",
    "    - **Parameters**:\n",
    "    - max_memory: Memory limit for chunks\n",
    "    - **Features**: \n",
    "    - Caching mechanism\n",
    "    - Memory management\n",
    "\n",
    "2. Main Computation Method\n",
    "    ```python\n",
    "    def compute(self, query, key, value):\n",
    "        batch_size = tf.shape(query)[0]\n",
    "        \n",
    "        # Split computation into manageable chunks\n",
    "        chunk_size = self._calculate_chunk_size(query)\n",
    "        num_chunks = tf.shape(query)[1] // chunk_size\n",
    "    ```\n",
    "    - **Purpose**: Manages chunked attention computation\n",
    "    - **Process**:\n",
    "    1. Determines chunk size\n",
    "    2. Calculates number of chunks\n",
    "    3. Processes each chunk separately\n",
    "\n",
    "3. Chunk Size Calculation\n",
    "    ```python\n",
    "    def _calculate_chunk_size(self, tensor):\n",
    "        element_size = tensor.dtype.size\n",
    "        return min(\n",
    "            tf.shape(tensor)[1],\n",
    "            self.max_memory // (element_size * tf.shape(tensor)[2])\n",
    "        )\n",
    "    ```\n",
    "    - **Purpose**: Determines optimal chunk size\n",
    "    - **Factors**:\n",
    "    - Memory limit\n",
    "    - Element size\n",
    "    - Tensor dimensions\n",
    "\n",
    "4. Chunk Processing\n",
    "    ```python\n",
    "    def _process_chunk(self, query_chunk, key, value):\n",
    "        # Compute attention for chunk\n",
    "        scores = tf.matmul(query_chunk, key, transpose_b=True)\n",
    "        scores = scores / tf.sqrt(tf.cast(tf.shape(key)[-1], tf.float32))\n",
    "        attention = tf.nn.softmax(scores, axis=-1)\n",
    "        return tf.matmul(attention, value)\n",
    "    ```\n",
    "    - **Purpose**: Processes individual attention chunks\n",
    "    - **Steps**:\n",
    "    1. Compute attention scores\n",
    "    2. Apply scaling factor\n",
    "    3. Calculate softmax\n",
    "    4. Compute final values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Components:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Memory Management\n",
    "    ```python\n",
    "    chunk_size = self._calculate_chunk_size(query)\n",
    "    num_chunks = tf.shape(query)[1] // chunk_size\n",
    "    ```\n",
    "    - Manages memory usage\n",
    "    - Prevents OOM errors\n",
    "    - Optimizes chunk size\n",
    "\n",
    "2. Chunked Processing\n",
    "    ```python\n",
    "    outputs = []\n",
    "    for i in range(num_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = (i + 1) * chunk_size\n",
    "        \n",
    "        chunk_output = self._process_chunk(\n",
    "            query[:, start_idx:end_idx],\n",
    "            key,\n",
    "            value\n",
    "        )\n",
    "        outputs.append(chunk_output)\n",
    "    ```\n",
    "    - Processes in manageable chunks\n",
    "    - Maintains sequence order\n",
    "    - Accumulates results\n",
    "\n",
    "3. Attention Computation\n",
    "    ```python\n",
    "    scores = tf.matmul(query_chunk, key, transpose_b=True)\n",
    "    scores = scores / tf.sqrt(tf.cast(tf.shape(key)[-1], tf.float32))\n",
    "    attention = tf.nn.softmax(scores, axis=-1)\n",
    "    ```\n",
    "    - Standard attention mechanism\n",
    "    - Scaled dot-product attention\n",
    "    - Memory-efficient implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mathematical Operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Chunk Size Calculation**:\n",
    "    ```\n",
    "    chunk_size = min(\n",
    "        sequence_length,\n",
    "        max_memory / (element_size * hidden_dim)\n",
    "    )\n",
    "    ```\n",
    "\n",
    "2. **Attention Computation**:\n",
    "    ```\n",
    "    Attention(Q,K,V) = softmax(QK^T/√d_k)V\n",
    "    Computed in chunks for memory efficiency\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Benefits:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Memory Efficiency**:\n",
    "   - Controlled memory usage\n",
    "   - Prevents OOM errors\n",
    "   - Scalable to large sequences\n",
    "\n",
    "2. **Performance**:\n",
    "   - Optimized chunk processing\n",
    "   - Efficient attention computation\n",
    "   - GPU memory management\n",
    "\n",
    "3. **Flexibility**:\n",
    "   - Adaptable chunk sizes\n",
    "   - Memory-aware processing\n",
    "   - Dynamic adjustment\n",
    "\n",
    "4. **Scalability**:\n",
    "   - Handles long sequences\n",
    "   - Memory-constrained environments\n",
    "   - Large model support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Wrapper Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLoRAModelWrapper:\n",
    "    def __init__(self,\n",
    "                 base_model,\n",
    "                 rank=8,\n",
    "                 alpha=32,\n",
    "                 bits=4,\n",
    "                 group_size=128):\n",
    "        self.base_model = base_model\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.bits = bits\n",
    "        self.group_size = group_size\n",
    "        self.qlora_layers = []\n",
    "        \n",
    "    def apply_qlora(self, layer_names=None):\n",
    "        if layer_names is None:\n",
    "            layer_names = ['query', 'key', 'value']\n",
    "            \n",
    "        def replace_layer(layer):\n",
    "            if any(name in layer.name for name in layer_names):\n",
    "                if isinstance(layer, tf.keras.layers.Dense):\n",
    "                    qlora_layer = QLoRALayer(\n",
    "                        layer,\n",
    "                        rank=self.rank,\n",
    "                        alpha=self.alpha,\n",
    "                        bits=self.bits,\n",
    "                        group_size=self.group_size\n",
    "                    )\n",
    "                    self.qlora_layers.append(qlora_layer)\n",
    "                    return qlora_layer\n",
    "            return layer\n",
    "        \n",
    "        # Clone and modify model\n",
    "        new_model = tf.keras.models.clone_model(\n",
    "            self.base_model,\n",
    "            clone_function=replace_layer\n",
    "        )\n",
    "        \n",
    "        return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Class Initialization\n",
    "    ```python\n",
    "    def __init__(self,\n",
    "                base_model,\n",
    "                rank=8,\n",
    "                alpha=32,\n",
    "                bits=4,\n",
    "                group_size=128):\n",
    "        self.base_model = base_model\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.bits = bits\n",
    "        self.group_size = group_size\n",
    "        self.qlora_layers = []\n",
    "    ```\n",
    "    - **Purpose**: Initializes Q-LoRA wrapper\n",
    "    - **Parameters**:\n",
    "    - base_model: Original model to adapt\n",
    "    - rank: LoRA rank dimension\n",
    "    - alpha: Scaling factor\n",
    "    - bits: Quantization precision\n",
    "    - group_size: Quantization group size\n",
    "    - **Storage**: Tracks modified layers\n",
    "\n",
    "2. Layer Replacement Method\n",
    "    ```python\n",
    "    def apply_qlora(self, layer_names=None):\n",
    "        if layer_names is None:\n",
    "            layer_names = ['query', 'key', 'value']\n",
    "    ```\n",
    "    - **Purpose**: Applies Q-LoRA to specified layers\n",
    "    - **Default Targets**: \n",
    "    - query layers\n",
    "    - key layers\n",
    "    - value layers\n",
    "\n",
    "3. Layer Replacement Function\n",
    "    ```python\n",
    "    def replace_layer(layer):\n",
    "        if any(name in layer.name for name in layer_names):\n",
    "            if isinstance(layer, tf.keras.layers.Dense):\n",
    "                qlora_layer = QLoRALayer(\n",
    "                    layer,\n",
    "                    rank=self.rank,\n",
    "                    alpha=self.alpha,\n",
    "                    bits=self.bits,\n",
    "                    group_size=self.group_size\n",
    "                )\n",
    "                self.qlora_layers.append(qlora_layer)\n",
    "                return qlora_layer\n",
    "        return layer\n",
    "    ```\n",
    "    - **Purpose**: Handles individual layer replacement\n",
    "    - **Process**:\n",
    "    1. Checks layer name match\n",
    "    2. Verifies layer type\n",
    "    3. Creates Q-LoRA layer\n",
    "    4. Tracks modifications\n",
    "\n",
    "4. Model Modification\n",
    "    ```python\n",
    "    # Clone and modify model\n",
    "    new_model = tf.keras.models.clone_model(\n",
    "        self.base_model,\n",
    "        clone_function=replace_layer\n",
    "    )\n",
    "    ```\n",
    "    - **Purpose**: Creates adapted model\n",
    "    - **Features**:\n",
    "    - Non-destructive modification\n",
    "    - Preserves original model\n",
    "    - Selective adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Configuration Management\n",
    "   ```python\n",
    "   self.rank = rank\n",
    "   self.alpha = alpha\n",
    "   self.bits = bits\n",
    "   self.group_size = group_size\n",
    "   ```\n",
    "   - Centralized parameter storage\n",
    "   - Consistent configuration\n",
    "   - Easy modification\n",
    "\n",
    "2. Layer Tracking\n",
    "   ```python\n",
    "   self.qlora_layers.append(qlora_layer)\n",
    "   ```\n",
    "   - Maintains layer registry\n",
    "   - Enables monitoring\n",
    "   - Facilitates management\n",
    "\n",
    "3. Selective Adaptation\n",
    "   ```python\n",
    "   if any(name in layer.name for name in layer_names):\n",
    "   ```\n",
    "   - Targeted modifications\n",
    "   - Flexible layer selection\n",
    "   - Controlled adaptation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLoRATrainer:\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 learning_rate=1e-4,\n",
    "                 max_memory=None):\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.paged_attention = PagedAttention(max_memory)\n",
    "        \n",
    "    @tf.function\n",
    "    def train_step(self, inputs, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass with memory-efficient attention\n",
    "            predictions = self.model(\n",
    "                inputs,\n",
    "                attention_implementation=self.paged_attention\n",
    "            )\n",
    "            loss = self.compute_loss(labels, predictions)\n",
    "            \n",
    "        # Compute gradients\n",
    "        gradients = tape.gradient(\n",
    "            loss,\n",
    "            self.model.trainable_variables\n",
    "        )\n",
    "        \n",
    "        # Apply gradients\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Class Initialization\n",
    "    ```python\n",
    "    def __init__(self,\n",
    "                model,\n",
    "                learning_rate=1e-4,\n",
    "                max_memory=None):\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.paged_attention = PagedAttention(max_memory)\n",
    "    ```\n",
    "    - **Purpose**: Sets up Q-LoRA training environment\n",
    "    - **Parameters**:\n",
    "    - model: Q-LoRA adapted model\n",
    "    - learning_rate: Training rate\n",
    "    - max_memory: Memory limit for attention\n",
    "    - **Features**: \n",
    "    - Memory-efficient attention\n",
    "    - Configurable learning rate\n",
    "\n",
    "2. Training Step Method\n",
    "    ```python\n",
    "    @tf.function  # TensorFlow optimization decorator\n",
    "    def train_step(self, inputs, labels):\n",
    "    ```\n",
    "    - **Purpose**: Executes single training iteration\n",
    "    - **Optimization**: Graph mode execution\n",
    "    - **Parameters**:\n",
    "    - inputs: Training data\n",
    "    - labels: Target values\n",
    "\n",
    "3. Forward Pass\n",
    "    ```python\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass with memory-efficient attention\n",
    "        predictions = self.model(\n",
    "            inputs,\n",
    "            attention_implementation=self.paged_attention\n",
    "        )\n",
    "        loss = self.compute_loss(labels, predictions)\n",
    "    ```\n",
    "    - **Purpose**: Computes model predictions\n",
    "    - **Features**:\n",
    "    - Gradient tracking\n",
    "    - Paged attention usage\n",
    "    - Loss computation\n",
    "\n",
    "4. Gradient Computation and Application\n",
    "    ```python\n",
    "    # Compute gradients\n",
    "    gradients = tape.gradient(\n",
    "        loss,\n",
    "        self.model.trainable_variables\n",
    "    )\n",
    "\n",
    "    # Apply gradients\n",
    "    self.optimizer.apply_gradients(\n",
    "        zip(gradients, self.model.trainable_variables)\n",
    "    )\n",
    "    ```\n",
    "    - **Purpose**: Updates model parameters\n",
    "    - **Process**:\n",
    "    1. Compute gradients\n",
    "    2. Apply updates\n",
    "    3. Return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Components:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Memory Management\n",
    "   ```python\n",
    "   self.paged_attention = PagedAttention(max_memory)\n",
    "   ```\n",
    "   - Efficient attention computation\n",
    "   - Memory-aware processing\n",
    "   - Controlled resource usage\n",
    "\n",
    "2. Optimization\n",
    "   ```python\n",
    "   @tf.function\n",
    "   def train_step(self, inputs, labels):\n",
    "   ```\n",
    "   - Graph compilation\n",
    "   - Performance optimization\n",
    "   - Efficient execution\n",
    "\n",
    "3. Gradient Management\n",
    "   ```python\n",
    "   gradients = tape.gradient(\n",
    "      loss,\n",
    "      self.model.trainable_variables\n",
    "   )\n",
    "   ```\n",
    "   - Automatic differentiation\n",
    "   - Parameter updates\n",
    "   - Training optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Memory Efficiency**\n",
    "   - 85% reduction in memory usage\n",
    "   - Enables training on consumer GPUs\n",
    "   - Supports larger batch sizes\n",
    "\n",
    "2. **Cost Effectiveness**\n",
    "   - Reduced hardware requirements\n",
    "   - Lower energy consumption\n",
    "   - More accessible deployment\n",
    "\n",
    "3. **Performance**\n",
    "   - Comparable results to full fine-tuning\n",
    "   - Stable training process\n",
    "   - Maintained model quality\n",
    "\n",
    "4. **Scalability**\n",
    "   - Supports larger models\n",
    "   - Efficient multi-task adaptation\n",
    "   - Better resource utilization\n",
    "\n",
    "5. **Accessibility**\n",
    "   - Enables broader research participation\n",
    "   - Reduces entry barriers\n",
    "   - Supports democratization of AI\n",
    "\n",
    "6. **Technical Benefits**\n",
    "   - Efficient gradient computation\n",
    "   - Stable numerical operations\n",
    "   - Reduced precision loss"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

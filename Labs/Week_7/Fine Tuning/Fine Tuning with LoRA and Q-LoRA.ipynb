{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Fine Tuning</h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief Recap of Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning techniques are specialized methods for adapting pre-trained language models to specific tasks or domains. These techniques have revolutionized NLP by making it more efficient and accessible to fine-tune large language models with limited computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Traditional Fine-tuning is Challenging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **High Computational Cost**: Fine-tuning the entire model requires significant computational resources, as all parameters are updated during training.\n",
    "  \n",
    "2. **Large Storage Requirement**: Each fine-tuned model copy occupies substantial storage, which scales poorly with the number of tasks or datasets.\n",
    "\n",
    "3. **Catastrophic Forgetting**: Updating all parameters can lead to the loss of knowledge from the pre-trained model, making it less effective on tasks outside the fine-tuning domain.\n",
    "\n",
    "4. **Inefficiency for Large Models**: For large-scale models like GPT or LLaMA, fine-tuning is resource-intensive, requiring extensive GPU/TPU memory.\n",
    "\n",
    "5. **Limited Adaptability**: Fine-tuned models are specialized for a single task, making reuse for other tasks less feasible without further fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"static/image1.gif\" alt=\"Fine Tuning\" style=\"width:50%;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding LoRA (Low-Rank Adaptation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"static/image2.gif\" alt=\"Fine Tuning with LoRA\" style=\"width:50%;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem LoRA Solves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Resource Intensity**\n",
    "   - Full fine-tuning requires updating all parameters\n",
    "   - High memory requirements (2-3x model size)\n",
    "   - Expensive computational resources needed\n",
    "\n",
    "2. **Storage Overhead**\n",
    "   - Each fine-tuned version needs full model storage\n",
    "   - Multiple task adaptations become impractical\n",
    "   - Version management becomes complex\n",
    "\n",
    "3. **Training Efficiency**\n",
    "   - Long training times\n",
    "   - High energy consumption\n",
    "   - Limited parallel adaptations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Novel Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Low-Rank Decomposition**\n",
    "   - Represents weight updates as low-rank matrices\n",
    "   - Uses matrix factorization for efficiency\n",
    "   - Minimizes parameter count while maintaining performance\n",
    "\n",
    "2. **Frozen Weights**\n",
    "   - Original model weights remain unchanged\n",
    "   - Only train small adaptation matrices\n",
    "   - Preserves pre-trained knowledge\n",
    "\n",
    "3. **Parameter-Efficient Updates**\n",
    "   - Updates through small matrices (A and B)\n",
    "   - Rank determines compression ratio\n",
    "   - Trainable parameters reduced significantly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How LoRA Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Weight Update Decomposition**:\n",
    "    ```\n",
    "    ΔW = BA\n",
    "    where:\n",
    "    - ΔW ∈ ℝᵐˣⁿ (weight update)\n",
    "    - B ∈ ℝᵐˣʳ (first adaptation matrix)\n",
    "    - A ∈ ℝʳˣⁿ (second adaptation matrix)\n",
    "    - r is the rank (typically 8, 16, or 32)\n",
    "    ```\n",
    "\n",
    "2. **Forward Pass Computation**:\n",
    "    ```\n",
    "    Y = XW + α(X(BA))\n",
    "    where:\n",
    "    - X is input\n",
    "    - W is original weights\n",
    "    - α is scaling factor\n",
    "    - BA is LoRA update\n",
    "    ```\n",
    "\n",
    "3. **Parameter Reduction**:\n",
    "    ```\n",
    "    Original parameters: m × n\n",
    "    LoRA parameters: r × (m + n)\n",
    "    Reduction ratio: (r × (m + n)) / (m × n)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Implementation Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAConfig:\n",
    "    \"\"\"\n",
    "    Configuration class for Low-Rank Adaptation (LoRA).\n",
    "\n",
    "    LoRA is a technique that reduces the number of trainable parameters by\n",
    "    injecting low-rank matrices into existing layers while keeping the base\n",
    "    model weights frozen.\n",
    "\n",
    "    Attributes:\n",
    "        rank (int): The rank of the LoRA decomposition (controls adaptation capacity).\n",
    "        alpha (int): Scaling factor for LoRA updates.\n",
    "        target_modules (list): The list of module names to apply LoRA to (e.g., attention layers).\n",
    "        dropout (float): Dropout rate applied to LoRA layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 rank=8,\n",
    "                 alpha=32,\n",
    "                 target_modules=None,\n",
    "                 dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the LoRA configuration.\n",
    "\n",
    "        Args:\n",
    "            rank (int, optional): The rank for LoRA decomposition. Default is 8.\n",
    "            alpha (int, optional): The scaling factor for LoRA updates. Default is 32.\n",
    "            target_modules (list, optional): List of module names where LoRA should be applied.\n",
    "                                             Default: ['query', 'key', 'value'].\n",
    "            dropout (float, optional): Dropout rate applied to LoRA layers. Default is 0.1.\n",
    "        \"\"\"\n",
    "        self.rank = rank  # Defines the rank of the low-rank decomposition\n",
    "        self.alpha = alpha  # Scaling factor for LoRA updates\n",
    "        self.target_modules = target_modules or ['query', 'key', 'value']  # Apply LoRA to these layers\n",
    "        self.dropout = dropout  # Dropout applied to LoRA layers (helps prevent overfitting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the above code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose**\n",
    "\n",
    "This class serves as a configuration container for LoRA hyperparameters and settings. It centralizes all LoRA-specific parameters in one place for easy management and modification.\n",
    "\n",
    "**Parameters Explained:**\n",
    "\n",
    "1. **rank (default=8)**\n",
    "   - Defines the dimension of low-rank matrices\n",
    "   - Controls compression ratio and memory savings\n",
    "   - Lower rank = more compression but potentially less capacity\n",
    "   - Common values: 8, 16, 32\n",
    "   - Formula: compression ≈ 2r/(d_in + d_out)\n",
    "\n",
    "2. **alpha (default=32)**\n",
    "   - Scaling factor for LoRA updates\n",
    "   - Controls the magnitude of adaptations\n",
    "   - Usually set to match or be larger than rank\n",
    "   - Helps stabilize training\n",
    "   - Formula: output = original + (alpha * LoRA_output)\n",
    "\n",
    "3. **target_modules (default=['query', 'key', 'value'])**\n",
    "   - Specifies which layers to apply LoRA to\n",
    "   - Defaults to attention mechanism components\n",
    "   - Can be customized for different architectures\n",
    "   - Common targets:\n",
    "     - query: Query projection in attention\n",
    "     - key: Key projection in attention\n",
    "     - value: Value projection in attention\n",
    "\n",
    "4. **dropout (default=0.1)**\n",
    "   - Dropout rate for LoRA layers\n",
    "   - Helps prevent overfitting\n",
    "   - Applied only to LoRA path, not base model\n",
    "   - Standard range: 0.0-0.5\n",
    "\n",
    "5. **std - Random weight initialization**\n",
    "   $$\n",
    "   \\frac{\\sqrt{\\frac{2.0}{\\text{float}(\\text{shape}[0])}}}{\\text{rank}}\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Initialization Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAInitialization:\n",
    "    \"\"\"\n",
    "    Utility class for initializing LoRA (Low-Rank Adaptation) weight matrices.\n",
    "\n",
    "    LoRA uses low-rank matrices (A and B) to adapt frozen model weights,\n",
    "    requiring careful initialization for stability and efficiency.\n",
    "\n",
    "    Methods:\n",
    "        init_weights_a(shape, rank): Initializes matrix A using scaled Kaiming/He initialization.\n",
    "        init_weights_b(shape): Initializes matrix B with zeros for stability.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights_a(shape, rank):\n",
    "        \"\"\"\n",
    "        Initializes the LoRA A matrix using Kaiming/He initialization.\n",
    "\n",
    "        This ensures that the weight distribution is properly scaled based on \n",
    "        the number of input features, helping prevent vanishing/exploding gradients.\n",
    "\n",
    "        Args:\n",
    "            shape (tuple): Shape of the weight matrix (input_dim, rank).\n",
    "            rank (int): The rank for low-rank adaptation.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Initialized weight matrix for LoRA A.\n",
    "        \"\"\"\n",
    "        std = np.sqrt(2.0 / float(shape[0])) / rank  # Scale variance by rank\n",
    "        return tf.random.normal(shape, stddev=std)\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights_b(shape):\n",
    "        \"\"\"\n",
    "        Initializes the LoRA B matrix with zeros for stability.\n",
    "\n",
    "        A zero-initialized B matrix ensures that at the start of training,\n",
    "        the LoRA adaptation does not interfere with the frozen model weights.\n",
    "\n",
    "        Args:\n",
    "            shape (tuple): Shape of the weight matrix (rank, output_dim).\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Zero-initialized weight matrix for LoRA B.\n",
    "        \"\"\"\n",
    "        return tf.zeros(shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the above code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose**\n",
    "\n",
    "This class handles the initialization strategies for the two LoRA matrices (A and B). It uses different initialization approaches for each matrix to ensure stable training and good convergence.\n",
    "\n",
    "**Methods Explained:**\n",
    "\n",
    "1. **init_weights_a**\n",
    "    ```python\n",
    "    @staticmethod\n",
    "    def init_weights_a(shape, rank):\n",
    "        std = np.sqrt(2.0 / float(shape[0])) / rank\n",
    "        return tf.random.normal(shape, stddev=std)\n",
    "    ```\n",
    "      \n",
    "    - **Purpose**: Initializes the first LoRA matrix (A)\n",
    "    - **Uses Kaiming/He Initialization**:\n",
    "      - Designed for ReLU-based networks\n",
    "      - Helps maintain variance across layers\n",
    "      - Scaled by rank for stability\n",
    "    - **Parameters**:\n",
    "      - shape: Dimensions of matrix A\n",
    "      - rank: LoRA rank parameter\n",
    "    - **Formula Breakdown**:\n",
    "      - `2.0 / float(shape[0])`: He initialization base\n",
    "      - `/rank`: Additional scaling for LoRA stability\n",
    "      - Result used as standard deviation for normal distribution\n",
    "\n",
    "2. **init_weights_b**\n",
    "    ```python\n",
    "    @staticmethod\n",
    "    def init_weights_b(shape):\n",
    "        return tf.zeros(shape)\n",
    "    ```\n",
    "\n",
    "    - **Purpose**: Initializes the second LoRA matrix (B)\n",
    "    - **Uses Zero Initialization**:\n",
    "      - Ensures LoRA starts with no initial impact\n",
    "      - Allows gradual learning of adaptations\n",
    "      - Promotes stability in early training\n",
    "    - **Parameters**:\n",
    "      - shape: Dimensions of matrix B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing LoRA in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LoRA Layer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Implements a Low-Rank Adaptation (LoRA) layer for fine-tuning large models efficiently.\n",
    "\n",
    "    LoRA injects trainable low-rank matrices into a frozen layer, allowing adaptation\n",
    "    without modifying the original pre-trained weights.\n",
    "\n",
    "    Attributes:\n",
    "        original_layer (tf.keras.layers.Layer): The frozen base layer to be adapted.\n",
    "        rank (int): The rank of the LoRA decomposition (controls adaptation flexibility).\n",
    "        alpha (int): The scaling factor applied to the LoRA output.\n",
    "        dropout_rate (float): Dropout rate applied before LoRA transformation.\n",
    "        lora_a (tf.Variable): Trainable low-rank matrix A.\n",
    "        lora_b (tf.Variable): Trainable low-rank matrix B.\n",
    "        dropout (tf.keras.layers.Dropout): Dropout layer applied to LoRA inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 original_layer,\n",
    "                 rank=8,\n",
    "                 alpha=32,\n",
    "                 dropout_rate=0.1,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the LoRA layer.\n",
    "\n",
    "        Args:\n",
    "            original_layer (tf.keras.layers.Layer): The frozen base layer being adapted.\n",
    "            rank (int, optional): The rank for LoRA decomposition. Default is 8.\n",
    "            alpha (int, optional): Scaling factor for LoRA updates. Default is 32.\n",
    "            dropout_rate (float, optional): Dropout rate applied before LoRA transformation. Default is 0.1.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.original_layer = original_layer  # Store the frozen base layer\n",
    "        self.rank = rank  # LoRA rank for low-rank decomposition\n",
    "        self.alpha = alpha  # Scaling factor for LoRA updates\n",
    "        self.dropout_rate = dropout_rate  # Dropout rate applied before LoRA updates\n",
    "        \n",
    "        # Get original weight shape\n",
    "        self.original_shape = original_layer.get_weights()[0].shape\n",
    "        \n",
    "        # Create LoRA matrices (trainable parameters)\n",
    "        self.lora_a = self._create_lora_matrix(\"a\")\n",
    "        self.lora_b = self._create_lora_matrix(\"b\")\n",
    "        \n",
    "        # Dropout applied before LoRA updates\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        # Freeze original model weights\n",
    "        self.original_layer.trainable = False\n",
    "\n",
    "    def _create_lora_matrix(self, name):\n",
    "        \"\"\"\n",
    "        Creates a trainable LoRA matrix (A or B) using predefined initialization.\n",
    "\n",
    "        Args:\n",
    "            name (str): \"a\" or \"b\", indicating which LoRA matrix to create.\n",
    "\n",
    "        Returns:\n",
    "            tf.Variable: Trainable weight matrix for LoRA adaptation.\n",
    "        \"\"\"\n",
    "        if name == \"a\":\n",
    "            shape = (self.original_shape[0], self.rank)  # A: (input_dim, rank)\n",
    "            initializer = LoRAInitialization.init_weights_a\n",
    "        else:\n",
    "            shape = (self.rank, self.original_shape[1])  # B: (rank, output_dim)\n",
    "            initializer = LoRAInitialization.init_weights_b\n",
    "            \n",
    "        return self.add_weight(\n",
    "            name=f\"lora_{name}\",\n",
    "            shape=shape,\n",
    "            initializer=initializer,\n",
    "            trainable=True\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the LoRA layer.\n",
    "\n",
    "        Args:\n",
    "            inputs (tf.Tensor): The input tensor.\n",
    "            training (bool, optional): Whether the model is in training mode.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Output tensor after applying LoRA adaptation.\n",
    "        \"\"\"\n",
    "        # Compute original output using frozen weights\n",
    "        original_output = self.original_layer(inputs)\n",
    "        \n",
    "        # Apply dropout before LoRA transformation (only in training mode)\n",
    "        lora_input = self.dropout(inputs, training=training) if training else inputs\n",
    "        \n",
    "        # Compute the LoRA adaptation\n",
    "        lora_output = tf.matmul(\n",
    "            tf.matmul(lora_input, self.lora_a),  # First projection (low-rank A)\n",
    "            self.lora_b  # Second projection (low-rank B)\n",
    "        )\n",
    "        \n",
    "        # Combine the original output with LoRA adaptation (scaled by alpha)\n",
    "        return original_output + (self.alpha * lora_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the above code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Class Initialization\n",
    "    ```python\n",
    "    def __init__(self, \n",
    "                original_layer,\n",
    "                rank=8,\n",
    "                alpha=32,\n",
    "                dropout_rate=0.1,\n",
    "                **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    ```\n",
    "    - Inherits from TensorFlow's base Layer class\n",
    "    - Takes original layer and LoRA parameters\n",
    "    - Parameters:\n",
    "    - original_layer: Base layer to adapt\n",
    "    - rank: Dimension of low-rank matrices\n",
    "    - alpha: Scaling factor\n",
    "    - dropout_rate: Regularization strength\n",
    "\n",
    "2. Setup and Initialization\n",
    "    ```python\n",
    "    # Store parameters\n",
    "    self.original_layer = original_layer\n",
    "    self.rank = rank\n",
    "    self.alpha = alpha\n",
    "    self.dropout_rate = dropout_rate\n",
    "\n",
    "    # Get shape from original layer\n",
    "    self.original_shape = original_layer.get_weights()[0].shape\n",
    "    ```\n",
    "    - Stores configuration parameters\n",
    "    - Extracts shape from original layer weights\n",
    "    - Prepares for LoRA matrix creation\n",
    "\n",
    "3. Matrix Creation Helper\n",
    "    ```python\n",
    "    def _create_lora_matrix(self, name):\n",
    "        if name == \"a\":\n",
    "            shape = (self.original_shape[0], self.rank)\n",
    "            initializer = LoRAInitialization.init_weights_a\n",
    "        else:\n",
    "            shape = (self.rank, self.original_shape[1])\n",
    "            initializer = LoRAInitialization.init_weights_b\n",
    "    ```\n",
    "    - Creates LoRA matrices A and B\n",
    "    - Matrix A: input_dim × rank\n",
    "    - Matrix B: rank × output_dim\n",
    "    - Uses different initializations for each matrix\n",
    "\n",
    "4. Forward Pass Implementation\n",
    "    ```python\n",
    "    def call(self, inputs, training=None):\n",
    "        # Original transformation\n",
    "        original_output = self.original_layer(inputs)\n",
    "        \n",
    "        # LoRA path with dropout\n",
    "        lora_input = inputs\n",
    "        if training:\n",
    "            lora_input = self.dropout(lora_input, training=training)\n",
    "        \n",
    "        # LoRA transformation\n",
    "        lora_output = tf.matmul(\n",
    "            tf.matmul(lora_input, self.lora_a),\n",
    "            self.lora_b\n",
    "        )\n",
    "    ```\n",
    "    - Implements forward pass computation\n",
    "    - Steps:\n",
    "    1. Compute original layer output\n",
    "    2. Apply dropout during training\n",
    "    3. Compute LoRA transformation\n",
    "    4. Combine results with scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Components:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Original Layer Handling\n",
    "    ```python\n",
    "    self.original_layer = original_layer\n",
    "    self.original_layer.trainable = False\n",
    "    ```\n",
    "    - Stores original layer\n",
    "    - Freezes original weights\n",
    "\n",
    "2. LoRA Matrices\n",
    "    ```python\n",
    "    self.lora_a = self._create_lora_matrix(\"a\")\n",
    "    self.lora_b = self._create_lora_matrix(\"b\")\n",
    "    ```\n",
    "    - Creates two trainable matrices\n",
    "    - Different initialization strategies\n",
    "    - Shapes determined by original layer\n",
    "\n",
    "3. Dropout Implementation\n",
    "    ```python\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    ```\n",
    "    - Adds regularization\n",
    "    - Only applied during training\n",
    "    - Applied to LoRA path only\n",
    "\n",
    "4. Forward Pass Logic\n",
    "    ```python\n",
    "    return original_output + (self.alpha * lora_output)\n",
    "    ```\n",
    "    - Combines original and LoRA paths\n",
    "    - Scales LoRA contribution\n",
    "    - Maintains original layer behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model Adapter Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAModelAdapter:\n",
    "    \"\"\"\n",
    "    A utility class for adapting a pre-trained model with LoRA (Low-Rank Adaptation).\n",
    "\n",
    "    This adapter replaces specific layers (e.g., attention layers) with LoRA-modified\n",
    "    versions while keeping the base model frozen. It enables efficient fine-tuning\n",
    "    without updating the full model parameters.\n",
    "\n",
    "    Attributes:\n",
    "        model (tf.keras.Model): The original pre-trained model to be adapted.\n",
    "        config (LoRAConfig): Configuration object specifying LoRA parameters.\n",
    "        lora_layers (list): A list storing all applied LoRA layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 config: LoRAConfig):\n",
    "        \"\"\"\n",
    "        Initializes the LoRAModelAdapter.\n",
    "\n",
    "        Args:\n",
    "            model (tf.keras.Model): The original model to be adapted with LoRA.\n",
    "            config (LoRAConfig): Configuration object specifying LoRA parameters.\n",
    "        \"\"\"\n",
    "        self.model = model  # Store the base model\n",
    "        self.config = config  # Store the LoRA configuration\n",
    "        self.lora_layers = []  # Track all applied LoRA layers\n",
    "\n",
    "    def adapt_layer(self, layer):\n",
    "        \"\"\"\n",
    "        Applies LoRA adaptation to a single layer if it matches the criteria.\n",
    "\n",
    "        Args:\n",
    "            layer (tf.keras.layers.Layer): The layer to check and adapt.\n",
    "\n",
    "        Returns:\n",
    "            tf.keras.layers.Layer: The original or LoRA-modified layer.\n",
    "        \"\"\"\n",
    "        if isinstance(layer, tf.keras.layers.Dense):  # Only apply LoRA to Dense layers\n",
    "            return LoRALayer(\n",
    "                layer,\n",
    "                rank=self.config.rank,\n",
    "                alpha=self.config.alpha,\n",
    "                dropout_rate=self.config.dropout\n",
    "            )\n",
    "        return layer  # Return unchanged if the layer is not adapted\n",
    "\n",
    "    def create_adapted_model(self):\n",
    "        \"\"\"\n",
    "        Creates a new model with LoRA adaptations applied to target layers.\n",
    "\n",
    "        The function clones the original model and replaces specified layers\n",
    "        (e.g., 'query', 'key', 'value') with LoRA-modified versions.\n",
    "\n",
    "        Returns:\n",
    "            tf.keras.Model: A new model with LoRA adaptations applied.\n",
    "        \"\"\"\n",
    "        def clone_function(layer):\n",
    "            \"\"\"\n",
    "            Function used to modify layers during model cloning.\n",
    "\n",
    "            Args:\n",
    "                layer (tf.keras.layers.Layer): The layer being cloned.\n",
    "\n",
    "            Returns:\n",
    "                tf.keras.layers.Layer: The modified or unchanged layer.\n",
    "            \"\"\"\n",
    "            if any(name in layer.name for name in self.config.target_modules):\n",
    "                adapted_layer = self.adapt_layer(layer)\n",
    "                if isinstance(adapted_layer, LoRALayer):\n",
    "                    self.lora_layers.append(adapted_layer)  # Track adapted layers\n",
    "                return adapted_layer\n",
    "            return layer  # Return unchanged layer if not modified\n",
    "        \n",
    "        # Clone the model while replacing specified layers with LoRA layers\n",
    "        adapted_model = tf.keras.models.clone_model(\n",
    "            self.model,\n",
    "            clone_function=clone_function\n",
    "        )\n",
    "        \n",
    "        return adapted_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the above code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Class Initialization\n",
    "    ```python\n",
    "    def __init__(self, model, config: LoRAConfig):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.lora_layers = []\n",
    "    ```\n",
    "    - **Purpose**: Initializes the adapter with:\n",
    "    - model: Original model to adapt\n",
    "    - config: LoRA configuration settings\n",
    "    - lora_layers: Tracks created LoRA layers\n",
    "    - **Type Hint**: Expects LoRAConfig object for configuration\n",
    "\n",
    "2. Layer Adaptation Method\n",
    "    ```python\n",
    "    def adapt_layer(self, layer):\n",
    "        \"\"\"Apply LoRA adaptation to a single layer\"\"\"\n",
    "        if isinstance(layer, tf.keras.layers.Dense):\n",
    "            return LoRALayer(\n",
    "                layer,\n",
    "                rank=self.config.rank,\n",
    "                alpha=self.config.alpha,\n",
    "                dropout_rate=self.config.dropout\n",
    "            )\n",
    "        return layer\n",
    "    ```\n",
    "    - **Purpose**: Converts single layer to LoRA version\n",
    "    - **Process**:\n",
    "    1. Checks if layer is Dense type\n",
    "    2. Creates LoRA version if applicable\n",
    "    3. Returns original layer if not Dense\n",
    "    - **Parameters**: Uses configuration values for:\n",
    "    - rank\n",
    "    - alpha\n",
    "    - dropout_rate\n",
    "\n",
    "3. Model Adaptation Method\n",
    "    ```python\n",
    "    def create_adapted_model(self):\n",
    "        \"\"\"Create a new model with LoRA adaptations\"\"\"\n",
    "        def clone_function(layer):\n",
    "            if any(name in layer.name \n",
    "                for name in self.config.target_modules):\n",
    "                adapted_layer = self.adapt_layer(layer)\n",
    "                if isinstance(adapted_layer, LoRALayer):\n",
    "                    self.lora_layers.append(adapted_layer)\n",
    "                return adapted_layer\n",
    "            return layer\n",
    "        \n",
    "        adapted_model = tf.keras.models.clone_model(\n",
    "            self.model,\n",
    "            clone_function=clone_function\n",
    "        )\n",
    "        \n",
    "        return adapted_model\n",
    "    ```\n",
    "    - **Purpose**: Creates complete LoRA-adapted model\n",
    "    - **Process**:\n",
    "    1. Defines clone function for layer handling\n",
    "    2. Checks layer names against target modules\n",
    "    3. Adapts matching layers\n",
    "    4. Tracks created LoRA layers\n",
    "    5. Clones entire model with adaptations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Selective Adaptation\n",
    "    ```python\n",
    "    if any(name in layer.name for name in self.config.target_modules)\n",
    "    ```\n",
    "    - Only adapts specified layers\n",
    "    - Maintains original architecture\n",
    "    - Configurable targeting\n",
    "\n",
    "2. Layer Tracking\n",
    "    ```python\n",
    "    self.lora_layers.append(adapted_layer)\n",
    "    ```\n",
    "    - Keeps record of LoRA layers\n",
    "    - Enables monitoring\n",
    "    - Facilitates management\n",
    "\n",
    "3. Model Preservation\n",
    "    ```python\n",
    "    adapted_model = tf.keras.models.clone_model(...)\n",
    "    ```\n",
    "    - Creates new model instance\n",
    "    - Preserves original model\n",
    "    - Safe adaptation process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRATrainingManager:\n",
    "    \"\"\"\n",
    "    A training manager for fine-tuning models with LoRA (Low-Rank Adaptation).\n",
    "\n",
    "    This class manages optimization, loss tracking, and gradient updates while ensuring\n",
    "    that only LoRA parameters are updated, keeping the original model weights frozen.\n",
    "\n",
    "    Attributes:\n",
    "        model (tf.keras.Model): The LoRA-adapted model to be trained.\n",
    "        learning_rate (float): The learning rate for the optimizer.\n",
    "        weight_decay (float): The weight decay coefficient for AdamW.\n",
    "        optimizer (tf.keras.optimizers.AdamW): Optimizer for updating LoRA parameters.\n",
    "        loss_tracker (tf.keras.metrics.Mean): Tracks loss values across training steps.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 learning_rate=1e-4,\n",
    "                 weight_decay=0.01):\n",
    "        \"\"\"\n",
    "        Initializes the LoRATrainingManager.\n",
    "\n",
    "        Args:\n",
    "            model (tf.keras.Model): The model to be trained with LoRA.\n",
    "            learning_rate (float, optional): The learning rate for the optimizer. Default is 1e-4.\n",
    "            weight_decay (float, optional): Weight decay factor for AdamW. Default is 0.01.\n",
    "        \"\"\"\n",
    "        self.model = model  # Store the LoRA-adapted model\n",
    "        self.learning_rate = learning_rate  # Learning rate for optimizer\n",
    "        self.weight_decay = weight_decay  # Weight decay to prevent overfitting\n",
    "        \n",
    "        # Create an optimizer with weight decay\n",
    "        self.optimizer = self._create_optimizer()\n",
    "        \n",
    "        # Loss tracker to monitor training progress\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name='loss')\n",
    "\n",
    "    def _create_optimizer(self):\n",
    "        \"\"\"\n",
    "        Creates an AdamW optimizer for training LoRA parameters.\n",
    "\n",
    "        Returns:\n",
    "            tf.keras.optimizers.AdamW: Optimizer configured with learning rate and weight decay.\n",
    "        \"\"\"\n",
    "        return tf.keras.optimizers.AdamW(\n",
    "            learning_rate=self.learning_rate,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, inputs, labels):\n",
    "        \"\"\"\n",
    "        Performs a single training step, updating only LoRA parameters.\n",
    "\n",
    "        Args:\n",
    "            inputs (tf.Tensor): Input tensor (e.g., tokenized sequences).\n",
    "            labels (tf.Tensor): Target tensor (e.g., ground-truth labels).\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the loss value for tracking.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            predictions = self.model(inputs, training=True)\n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(labels, predictions)\n",
    "            \n",
    "        # Get trainable variables (only LoRA parameters)\n",
    "        trainable_vars = [var for var in self.model.trainable_variables\n",
    "                          if 'lora_' in var.name]  # Ensures we only update LoRA layers\n",
    "        \n",
    "        # Compute gradients\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        \n",
    "        # Apply gradients using AdamW optimizer\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        \n",
    "        # Update loss tracking metric\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": self.loss_tracker.result()\n",
    "        }\n",
    "\n",
    "    def compute_loss(self, labels, predictions):\n",
    "        \"\"\"\n",
    "        Computes the loss function for training.\n",
    "\n",
    "        Args:\n",
    "            labels (tf.Tensor): Ground-truth labels.\n",
    "            predictions (tf.Tensor): Model predictions.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Computed loss value.\n",
    "        \"\"\"\n",
    "        return tf.keras.losses.sparse_categorical_crossentropy(labels, predictions, from_logits=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the above code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Class Initialization\n",
    "    ```python\n",
    "    def __init__(self,\n",
    "                model,\n",
    "                learning_rate=1e-4,\n",
    "                weight_decay=0.01):\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        self.optimizer = self._create_optimizer()\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name='loss')\n",
    "    ```\n",
    "    - **Purpose**: Sets up training environment\n",
    "    - **Parameters**:\n",
    "    - model: LoRA-adapted model\n",
    "    - learning_rate: Training rate (default: 0.0001)\n",
    "    - weight_decay: L2 regularization (default: 0.01)\n",
    "    - **Components**:\n",
    "    - Creates optimizer\n",
    "    - Initializes loss tracking\n",
    "\n",
    "2. Optimizer Creation\n",
    "    ```python\n",
    "    def _create_optimizer(self):\n",
    "        return tf.keras.optimizers.AdamW(\n",
    "            learning_rate=self.learning_rate,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "    ```\n",
    "    - **Purpose**: Initializes AdamW optimizer\n",
    "    - **Features**:\n",
    "    - Adaptive learning rates\n",
    "    - Weight decay regularization\n",
    "    - Momentum-based updates\n",
    "\n",
    "3. Training Step Implementation\n",
    "    ```python\n",
    "    @tf.function  # Compiler decorator for performance\n",
    "    def train_step(self, inputs, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            predictions = self.model(inputs, training=True)\n",
    "            # Calculate loss\n",
    "            loss = self.compute_loss(labels, predictions)\n",
    "    ```\n",
    "    - **Purpose**: Executes single training iteration\n",
    "    - **Process**:\n",
    "    1. Records operations for gradient computation\n",
    "    2. Performs forward pass\n",
    "    3. Calculates loss\n",
    "\n",
    "4. Gradient Computation and Application\n",
    "    ```python\n",
    "    # Get trainable variables (only LoRA parameters)\n",
    "    trainable_vars = [var for var in self.model.trainable_variables\n",
    "                    if 'lora_' in var.name]\n",
    "\n",
    "    # Compute gradients\n",
    "    gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "    # Apply gradients\n",
    "    self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "    ```\n",
    "    - **Purpose**: Updates LoRA parameters\n",
    "    - **Features**:\n",
    "    - Selects only LoRA variables\n",
    "    - Computes gradients\n",
    "    - Applies updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Components:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Loss Tracking\n",
    "    ```python\n",
    "    self.loss_tracker = tf.keras.metrics.Mean(name='loss')\n",
    "    self.loss_tracker.update_state(loss)\n",
    "    ```\n",
    "    - Maintains running average of loss\n",
    "    - Tracks training progress\n",
    "    - Returns current metrics\n",
    "\n",
    "2. LoRA Parameter Selection\n",
    "    ```python\n",
    "    trainable_vars = [var for var in self.model.trainable_variables\n",
    "                    if 'lora_' in var.name]\n",
    "    ```\n",
    "    - Filters for LoRA parameters\n",
    "    - Ignores frozen base model\n",
    "    - Efficient update process\n",
    "\n",
    "3. Gradient Management\n",
    "    ```python\n",
    "    gradients = tape.gradient(loss, trainable_vars)\n",
    "    self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "    ```\n",
    "    - Computes parameter updates\n",
    "    - Applies optimization steps\n",
    "    - Manages learning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Complete Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_lora(\n",
    "    base_model,\n",
    "    train_dataset,\n",
    "    validation_dataset,\n",
    "    config: LoRAConfig\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a model using LoRA (Low-Rank Adaptation) with a given dataset.\n",
    "\n",
    "    This function wraps a base model with LoRA, fine-tunes only the LoRA parameters,\n",
    "    and runs a training loop while evaluating on a validation set.\n",
    "\n",
    "    Args:\n",
    "        base_model (tf.keras.Model): The original pre-trained model.\n",
    "        train_dataset (tf.data.Dataset): The training dataset containing input-label pairs.\n",
    "        validation_dataset (tf.data.Dataset): The validation dataset for evaluation.\n",
    "        config (LoRAConfig): Configuration specifying LoRA parameters and training settings.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: The trained LoRA-adapted model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create LoRA adapter and generate an adapted model\n",
    "    adapter = LoRAModelAdapter(base_model, config)\n",
    "    adapted_model = adapter.create_adapted_model()\n",
    "    \n",
    "    # Initialize training manager\n",
    "    trainer = LoRATrainingManager(adapted_model, learning_rate=config.learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(config.epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{config.epochs}\")\n",
    "\n",
    "        # Train on all batches\n",
    "        for batch in train_dataset:\n",
    "            metrics = trainer.train_step(\n",
    "                batch['input_ids'],  # Input sequence\n",
    "                batch['labels']      # Corresponding labels\n",
    "            )\n",
    "        \n",
    "        # Validate on the entire validation set\n",
    "        val_metrics = trainer.evaluate(validation_dataset)\n",
    "\n",
    "        # Print metrics\n",
    "        print(f\"Training loss: {metrics['loss']:.4f}\")\n",
    "        print(f\"Validation loss: {val_metrics['loss']:.4f}\")\n",
    "\n",
    "    return adapted_model  # Return the trained model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Function Definition\n",
    "    ```python\n",
    "    def train_with_lora(\n",
    "        base_model,\n",
    "        train_dataset,\n",
    "        validation_dataset,\n",
    "        config: LoRAConfig\n",
    "    ):\n",
    "    ```\n",
    "    - **Purpose**: Main training pipeline for LoRA\n",
    "    - **Parameters**:\n",
    "    - base_model: Original model to adapt\n",
    "    - train_dataset: Training data\n",
    "    - validation_dataset: Validation data\n",
    "    - config: LoRA configuration settings\n",
    "\n",
    "2. Model Adaptation\n",
    "    ```python\n",
    "    # Create LoRA adapter\n",
    "    adapter = LoRAModelAdapter(base_model, config)\n",
    "    adapted_model = adapter.create_adapted_model()\n",
    "    ```\n",
    "    - **Purpose**: Sets up LoRA-adapted model\n",
    "    - **Process**:\n",
    "    1. Creates adapter instance\n",
    "    2. Applies LoRA to specified layers\n",
    "    3. Returns adapted model\n",
    "\n",
    "3. Training Setup\n",
    "    ```python\n",
    "    # Initialize training manager\n",
    "    trainer = LoRATrainingManager(adapted_model)\n",
    "    ```\n",
    "    - **Purpose**: Prepares training environment\n",
    "    - **Features**:\n",
    "    - Sets up optimizer\n",
    "    - Initializes loss tracking\n",
    "    - Manages training state\n",
    "\n",
    "4. Training Loop\n",
    "    ```python\n",
    "    # Training loop\n",
    "    for epoch in range(config.epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{config.epochs}\")\n",
    "        \n",
    "        # Train\n",
    "        for batch in train_dataset:\n",
    "            metrics = trainer.train_step(\n",
    "                batch['input_ids'],\n",
    "                batch['labels']\n",
    "            )\n",
    "    ```\n",
    "    - **Purpose**: Executes training process\n",
    "    - **Components**:\n",
    "    - Epoch iteration\n",
    "    - Batch processing\n",
    "    - Metrics collection\n",
    "\n",
    "5. Validation\n",
    "    ```python\n",
    "    # Validate\n",
    "    val_metrics = trainer.evaluate(validation_dataset)\n",
    "    ```\n",
    "    - **Purpose**: Evaluates model performance\n",
    "    - **Process**:\n",
    "    - Runs validation data\n",
    "    - Computes metrics\n",
    "    - Tracks progress\n",
    "\n",
    "6. Progress Reporting\n",
    "    ```python\n",
    "    # Print metrics\n",
    "    print(f\"Training loss: {metrics['loss']:.4f}\")\n",
    "    print(f\"Validation loss: {val_metrics['loss']:.4f}\")\n",
    "    ```\n",
    "    - **Purpose**: Monitors training progress\n",
    "    - **Output**:\n",
    "    - Training loss\n",
    "    - Validation loss\n",
    "    - Formatted metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.saved_model.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TFAutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/ramin/Documents/llamma3\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = TFAutoModelForCausalLM.from_pretrained(model_path, from_pt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers torch onnx onnx_tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh /home/ramin/.llama/checkpoints/Llama3.2-1B-Instruct-int4-qlora-eo8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = \"/home/ramin/.llama/checkpoints/Llama3.2-1B-Instruct-int4-qlora-eo8\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "print(\"Tokenizer loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv /home/ramin/.llama/checkpoints/Llama3.2-1B-Instruct-int4-qlora-eo8/consolidated.00.pth \\\n",
    "   /home/ramin/.llama/checkpoints/Llama3.2-1B-Instruct-int4-qlora-eo8/pytorch_model.bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-Python-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"codellama/CodeLlama-7b-Python-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    local_files_only=True,  # Prevent downloading\n",
    ")\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaConfig\n",
    "\n",
    "model_path = \"/home/ramin/.llama/checkpoints/Llama3.2-1B-Instruct-int4-qlora-eo8\"\n",
    "checkpoint_path = f\"{model_path}/consolidated.00.pth\"\n",
    "config_path = f\"{model_path}/params.json\"\n",
    "\n",
    "# Load model weights\n",
    "state_dict = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "\n",
    "# Load config\n",
    "config = LlamaConfig.from_json_file(config_path)\n",
    "\n",
    "# Initialize model\n",
    "model = LlamaForCausalLM(config)\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "# Save in Hugging Face format\n",
    "model.save_pretrained(model_path)\n",
    "print(\"Checkpoint converted successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save in Hugging Face format\n",
    "model.save_pretrained(\"llama3_hf\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "tokenizer.save_pretrained(\"llama3_hf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Q-LoRA: Quantized Low-Rank Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"static/image3.gif\" alt=\"Q-LoRA\" style=\"width:50%;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem Q-LoRA Solves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Memory Constraints**\n",
    "   - Even LoRA requires full-precision model weights\n",
    "   - 16-bit models still consume significant memory\n",
    "   - Limited by GPU VRAM during training\n",
    "\n",
    "2. **Hardware Limitations**\n",
    "   - Most consumer GPUs can't handle large models\n",
    "   - Training requires expensive specialized hardware\n",
    "   - Multiple GPUs often needed for fine-tuning\n",
    "\n",
    "3. **Accessibility Issues**\n",
    "   - Research limited by hardware requirements\n",
    "   - High computational costs\n",
    "   - Resource-intensive deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Novel Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **4-bit Quantization**\n",
    "   - Reduces model precision from 16/32-bit to 4-bit\n",
    "   - Uses special NormalFloat (NF4) format\n",
    "   - Maintains model quality despite compression\n",
    "\n",
    "2. **Double Quantization**\n",
    "   - Quantizes both weights and quantization constants\n",
    "   - Further reduces memory footprint\n",
    "   - Minimal impact on model performance\n",
    "\n",
    "3. **Paged Attention**\n",
    "   - Efficient memory management\n",
    "   - CPU offloading for attention computations\n",
    "   - Dynamic memory allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Memory Efficiency**\n",
    "   - 85% memory reduction compared to full fine-tuning\n",
    "   - Enables training on consumer GPUs\n",
    "   - Supports larger context windows\n",
    "\n",
    "2. **Quality Preservation**\n",
    "   - Maintains model performance\n",
    "   - Comparable results to full fine-tuning\n",
    "   - Stable training process\n",
    "\n",
    "3. **Accessibility**\n",
    "   - Works on single GPU setups\n",
    "   - Reduces hardware requirements\n",
    "   - Enables broader research participation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Q-LoRA Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **NormalFloat (NF4) Quantization**:\n",
    "    ```\n",
    "    Q(x) = s * round(clamp(x/s, -1, 1) * (2^b - 1)) / (2^b - 1)\n",
    "    where:\n",
    "    - x is the original value\n",
    "    - s is the scaling factor\n",
    "    - b is bits (4 for NF4)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Double Quantization**:\n",
    "    ```\n",
    "    First level: W_q = Q1(W, s1)\n",
    "    Second level: s_q = Q2(s1, s2)\n",
    "    where:\n",
    "    - W is original weights\n",
    "    - Q1, Q2 are quantization functions\n",
    "    - s1, s2 are scaling factors\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Gradient Computation**:\n",
    "    ```\n",
    "    ∂L/∂W = (∂L/∂W_q) * (∂W_q/∂W)\n",
    "    where:\n",
    "    - L is loss function\n",
    "    - W_q is quantized weights\n",
    "    - Straight-through estimator for gradients\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-LoRA Implementation Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLoRAConfig:\n",
    "    \"\"\"\n",
    "    Configuration class for QLoRA (Quantized Low-Rank Adaptation).\n",
    "    \n",
    "    QLoRA is an extension of LoRA that applies quantization (e.g., 4-bit) to \n",
    "    reduce memory usage while preserving fine-tuning efficiency. This class \n",
    "    holds hyperparameters for the quantization process.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 bits=4,\n",
    "                 group_size=128,\n",
    "                 double_quant=True,\n",
    "                 quant_type=\"nf4\"):\n",
    "        \"\"\"\n",
    "        Initializes the QLoRA configuration.\n",
    "\n",
    "        Args:\n",
    "            bits (int): Number of bits for quantization. Default is 4 (4-bit quantization).\n",
    "            group_size (int): The size of groups for quantization. \n",
    "                              A smaller group size improves precision but increases memory usage.\n",
    "            double_quant (bool): Whether to use double quantization (quantizing the quantization constants).\n",
    "                                 Helps reduce memory footprint while maintaining performance.\n",
    "            quant_type (str): The type of quantization format. \n",
    "                              Common choices include:\n",
    "                              - \"nf4\" (Normal Float 4), a format designed for efficient low-bit quantization.\n",
    "                              - \"fp4\" (Float 4), another floating-point-based 4-bit quantization.\n",
    "        \"\"\"\n",
    "        self.bits = bits  # Number of bits for weight quantization (e.g., 4-bit)\n",
    "        self.group_size = group_size  # Group size for quantization; controls trade-off between accuracy and efficiency\n",
    "        self.double_quant = double_quant  # Enables double quantization (quantizing quantization constants)\n",
    "        self.quant_type = quant_type  # Specifies the quantization type, e.g., \"nf4\" (Normal Float 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NF4Quantizer:\n",
    "    \"\"\"\n",
    "    A quantizer that implements Normal Float 4 (NF4) quantization.\n",
    "    \n",
    "    NF4 is a 4-bit floating-point quantization scheme optimized for \n",
    "    low-bit representation while maintaining numerical stability.\n",
    "    The range of NF4 is (-1, +1)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the NF4 quantizer with predefined quantization levels.\n",
    "        \n",
    "        The NF4 levels are chosen based on a non-uniform distribution to \n",
    "        better capture the important numerical ranges in LLM fine-tuning.\n",
    "        \"\"\"\n",
    "        # Predefined NF4 quantization levels (non-uniformly spaced)\n",
    "        self.levels = np.array([\n",
    "            -1.0, -0.72, -0.34, -0.11,  # Negative values\n",
    "            0.0, 0.11, 0.34, 0.72, 1.0   # Positive values\n",
    "        ])\n",
    "    \n",
    "    def quantize(self, x):\n",
    "        \"\"\"\n",
    "        Quantizes the input tensor using NF4 quantization levels.\n",
    "        \n",
    "        Args:\n",
    "            x (numpy array): Input tensor to be quantized.\n",
    "        \n",
    "        Returns:\n",
    "            numpy array: Quantized tensor where each value is mapped \n",
    "                         to the nearest NF4 quantization level.\n",
    "        \"\"\"\n",
    "        # Find the nearest NF4 quantization level for each element in x\n",
    "        indices = np.digitize(x, self.levels) - 1  # Get index of closest bin\n",
    "        return self.levels[indices]  # Map values to the nearest quantization level\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Q-LoRA in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantized Layer Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Quantization Formula**\n",
    "The real-valued input \\( x \\) is quantized into an integer representation \\( x_q \\):\n",
    "\n",
    "$$\n",
    "x_q = \\text{round} \\left( \\frac{x}{s} \\times (2^b - 1) \\right) - z\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( x_q \\) → Quantized integer value\n",
    "- \\( x \\) → Original floating-point weight\n",
    "- \\( s \\) → Scale factor (learned or computed)\n",
    "- \\( z \\) → Zero-point offset\n",
    "- \\( b \\) → Number of bits (e.g., \\( b = 4 \\) for 4-bit quantization)\n",
    "- round → Rounds to the nearest integer\n",
    "\n",
    "For **4-bit quantization**, the range is:\n",
    "\n",
    "$$\n",
    "2^4 - 1 = 15\n",
    "$$\n",
    "\n",
    "Thus, simplifying:\n",
    "\n",
    "$$\n",
    "x_q = \\text{round} \\left( \\frac{x}{s} \\times 15 \\right) - z\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Dequantization Formula**\n",
    "To recover the floating-point value \\( \\hat{x} \\) from the quantized integer \\( x_q \\):\n",
    "\n",
    "$$\n",
    "\\hat{x} = (x_q + z) \\times s\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( \\hat{x} \\) → Recovered floating-point value (approximation of \\( x \\))\n",
    "- \\( x_q \\) → Quantized integer value\n",
    "- \\( s \\) → Scale factor\n",
    "- \\( z \\) → Zero-point offset\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Clipping (Ensuring the Range)**\n",
    "Before quantization, values are clipped to ensure they stay within the allowed range:\n",
    "\n",
    "$$\n",
    "x_{\\text{clipped}} = \\text{clip}(x/s, -1, 1)\n",
    "$$\n",
    "\n",
    "This ensures that out-of-range values do not cause errors in quantization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class QLoRALayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    QLoRA Layer for applying Low-Rank Adaptation (LoRA) with quantization.\n",
    "\n",
    "    This layer replaces a dense layer with a quantized version and \n",
    "    injects trainable low-rank adaptation matrices (LoRA) while keeping \n",
    "    the original weights frozen.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 original_layer,\n",
    "                 rank=8,\n",
    "                 alpha=32,\n",
    "                 bits=4,\n",
    "                 group_size=128,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes a QLoRA layer.\n",
    "\n",
    "        Args:\n",
    "            original_layer (tf.keras.layers.Layer): The dense layer being adapted.\n",
    "            rank (int): The rank for LoRA decomposition (typically small, e.g., 8).\n",
    "            alpha (int): Scaling factor for LoRA updates (controls adaptation strength).\n",
    "            bits (int): Number of bits for quantization (e.g., 4-bit quantization).\n",
    "            group_size (int): Number of weights per quantization group (affects precision).\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.original_layer = original_layer  # Store the original dense layer\n",
    "        self.rank = rank  # LoRA rank for low-rank decomposition\n",
    "        self.alpha = alpha  # LoRA scaling factor\n",
    "        self.bits = bits  # Bit precision for quantization\n",
    "        self.group_size = group_size  # Group size for quantization\n",
    "\n",
    "        # Initialize quantization parameters\n",
    "        self.quantizer = self._create_quantizer()\n",
    "        \n",
    "        # Get original weight shape\n",
    "        self.original_shape = original_layer.get_weights()[0].shape\n",
    "        \n",
    "        # Store the quantized weights in low-bit format\n",
    "        self.quantized_weights = self._quantize_weights(original_layer.get_weights()[0])\n",
    "        \n",
    "        # Initialize LoRA matrices (learnable parameters)\n",
    "        self.lora_a = self._create_lora_weights(\"a\")  # LoRA A matrix\n",
    "        self.lora_b = self._create_lora_weights(\"b\")  # LoRA B matrix\n",
    "        \n",
    "        # Freeze original weights (only train LoRA parameters)\n",
    "        self.original_layer.trainable = False\n",
    "\n",
    "    def _create_quantizer(self):\n",
    "        \"\"\"\n",
    "        Creates quantization parameters for weight quantization.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing scale and zero point variables.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'scale': tf.Variable(1.0, trainable=False),  # Scale factor for quantization\n",
    "            'zero_point': tf.Variable(0.0, trainable=False)  # Zero point offset\n",
    "        }\n",
    "    \n",
    "    def _create_lora_weights(self, name):\n",
    "        \"\"\"\n",
    "        Initializes trainable LoRA matrices.\n",
    "\n",
    "        Args:\n",
    "            name (str): \"a\" or \"b\", indicating which LoRA matrix to create.\n",
    "\n",
    "        Returns:\n",
    "            tf.Variable: LoRA weight matrix.\n",
    "        \"\"\"\n",
    "        if name == \"a\":\n",
    "            shape = (self.original_shape[0], self.rank)  # A: (input_dim, rank)\n",
    "        else:\n",
    "            shape = (self.rank, self.original_shape[1])  # B: (rank, output_dim)\n",
    "            \n",
    "        return self.add_weight(\n",
    "            name=f\"lora_{name}\",\n",
    "            shape=shape,\n",
    "            initializer=\"zeros\",  # Start with zero initialization\n",
    "            trainable=True  # Only LoRA matrices are trainable\n",
    "        )\n",
    "    \n",
    "    def quantize(self, x):\n",
    "        \"\"\"\n",
    "        Quantizes the input tensor using scale and zero-point.\n",
    "\n",
    "        Args:\n",
    "            x (tf.Tensor): The tensor to quantize.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Quantized tensor stored in int8 format.\n",
    "        \"\"\"\n",
    "        scale = self.quantizer['scale']\n",
    "        zero_point = self.quantizer['zero_point']\n",
    "        \n",
    "        range_float = 2.0 ** self.bits - 1.0  # Max integer representation\n",
    "        x_scaled = tf.clip_by_value(x / scale, -1.0, 1.0)  # Normalize range\n",
    "        x_scaled_q = tf.round(x_scaled * range_float)  # Convert to integer\n",
    "        \n",
    "        return tf.cast(x_scaled_q - zero_point, dtype=tf.int8)  # Store as INT8\n",
    "\n",
    "    def dequantize(self, x_q):\n",
    "        \"\"\"\n",
    "        Dequantizes an int8 tensor back to floating point.\n",
    "\n",
    "        Args:\n",
    "            x_q (tf.Tensor): The quantized int8 tensor.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Floating-point tensor for computation.\n",
    "        \"\"\"\n",
    "        scale = self.quantizer['scale']\n",
    "        zero_point = self.quantizer['zero_point']\n",
    "        \n",
    "        return (tf.cast(x_q, dtype=tf.float32) + zero_point) * scale\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass of the QLoRA layer.\n",
    "\n",
    "        Args:\n",
    "            inputs (tf.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Output tensor after applying quantized transformation and LoRA adaptation.\n",
    "        \"\"\"\n",
    "        # Quantize the original layer's weights\n",
    "        q_weights = self.quantize(self.original_layer.weights[0])\n",
    "        \n",
    "        # Apply the original transformation with quantized weights\n",
    "        original_output = tf.matmul(inputs, q_weights)\n",
    "        \n",
    "        # Compute the LoRA update\n",
    "        lora_output = tf.matmul(\n",
    "            tf.matmul(inputs, self.lora_a),  # First projection (low-rank A)\n",
    "            self.lora_b  # Second projection (low-rank B)\n",
    "        )\n",
    "        \n",
    "        # Combine the original output with LoRA adaptation (scaled by alpha)\n",
    "        return original_output + (self.alpha * lora_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Class Initialization\n",
    "    ```python\n",
    "    def __init__(self, \n",
    "                original_layer,\n",
    "                rank=8,\n",
    "                alpha=32,\n",
    "                bits=4,\n",
    "                group_size=128,\n",
    "                **kwargs):\n",
    "    ```\n",
    "    - **Purpose**: Initializes quantized LoRA layer\n",
    "    - **Parameters**:\n",
    "    - original_layer: Base layer to adapt\n",
    "    - rank: LoRA rank dimension\n",
    "    - alpha: Scaling factor\n",
    "    - bits: Quantization precision (default 4-bit)\n",
    "    - group_size: Quantization group size\n",
    "\n",
    "2. Quantizer Creation\n",
    "    ```python\n",
    "    def _create_quantizer(self):\n",
    "        return {\n",
    "            'scale': tf.Variable(1.0, trainable=False),\n",
    "            'zero_point': tf.Variable(0.0, trainable=False)\n",
    "        }\n",
    "    ```\n",
    "    - **Purpose**: Sets up quantization parameters\n",
    "    - **Components**:\n",
    "    - scale: Scaling factor for quantization\n",
    "    - zero_point: Offset for quantization\n",
    "    - **Features**: Non-trainable variables\n",
    "\n",
    "3. Weight Creation\n",
    "    ```python\n",
    "    def _create_lora_weights(self, name):\n",
    "        if name == \"a\":\n",
    "            shape = (self.original_shape[0], self.rank)\n",
    "        else:\n",
    "            shape = (self.rank, self.original_shape[1])\n",
    "            \n",
    "        return self.add_weight(\n",
    "            name=f\"lora_{name}\",\n",
    "            shape=shape,\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True\n",
    "        )\n",
    "    ```\n",
    "    - **Purpose**: Creates LoRA matrices\n",
    "    - **Features**:\n",
    "    - Matrix A: input_dim × rank\n",
    "    - Matrix B: rank × output_dim\n",
    "    - Trainable parameters\n",
    "\n",
    "4. Quantization Implementation\n",
    "    ```python\n",
    "    def quantize(self, x):\n",
    "        scale = self.quantizer['scale']\n",
    "        zero_point = self.quantizer['zero_point']\n",
    "        \n",
    "        range_float = 2.0 ** self.bits - 1.0\n",
    "        x_scaled = tf.clip_by_value(x / scale, -1.0, 1.0)\n",
    "        x_scaled_q = tf.round(x_scaled * range_float)\n",
    "        return (x_scaled_q - zero_point) * scale\n",
    "    ```\n",
    "    - **Purpose**: Implements weight quantization\n",
    "    - **Process**:\n",
    "    1. Scale input values\n",
    "    2. Clip to range [-1, 1]\n",
    "    3. Quantize to specified bits\n",
    "    4. Rescale to original range\n",
    "\n",
    "5. Forward Pass Implementation\n",
    "    ```python\n",
    "    def call(self, inputs):\n",
    "        # Quantize original weights\n",
    "        q_weights = self.quantize(self.original_layer.weights[0])\n",
    "        \n",
    "        # Original path with quantized weights\n",
    "        original_output = tf.matmul(inputs, q_weights)\n",
    "        \n",
    "        # LoRA path\n",
    "        lora_output = tf.matmul(\n",
    "            tf.matmul(inputs, self.lora_a),\n",
    "            self.lora_b\n",
    "        )\n",
    "        \n",
    "        # Combine outputs\n",
    "        return original_output + (self.alpha * lora_output)\n",
    "    ```\n",
    "    - **Purpose**: Executes forward pass\n",
    "    - **Steps**:\n",
    "    1. Quantize original weights\n",
    "    2. Compute original path\n",
    "    3. Compute LoRA path\n",
    "    4. Combine results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Management Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PagedAttention:\n",
    "    \"\"\"\n",
    "    Implements memory-efficient attention by splitting computation into manageable chunks.\n",
    "    \n",
    "    Paged Attention is useful for handling large inputs that do not fit into memory\n",
    "    by processing smaller chunks sequentially while maintaining correctness.\n",
    "\n",
    "    Attributes:\n",
    "        max_memory (int): The maximum amount of memory available for computation.\n",
    "        cache (dict): A cache for storing intermediate results if needed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_memory=None):\n",
    "        \"\"\"\n",
    "        Initializes the PagedAttention module.\n",
    "\n",
    "        Args:\n",
    "            max_memory (int, optional): The maximum memory available for processing.\n",
    "                                        This is used to determine the optimal chunk size.\n",
    "        \"\"\"\n",
    "        self.max_memory = max_memory\n",
    "        self.cache = {}  # Reserved for potential caching of attention computations\n",
    "    \n",
    "    def compute(self, query, key, value):\n",
    "        \"\"\"\n",
    "        Computes attention using chunked processing for memory efficiency.\n",
    "\n",
    "        Args:\n",
    "            query (tf.Tensor): The query tensor of shape (batch_size, seq_len, dim).\n",
    "            key (tf.Tensor): The key tensor of shape (batch_size, seq_len, dim).\n",
    "            value (tf.Tensor): The value tensor of shape (batch_size, seq_len, dim).\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The output tensor after applying attention, with the same shape\n",
    "                       as the query (batch_size, seq_len, dim).\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # Determine chunk size based on available memory\n",
    "        chunk_size = self._calculate_chunk_size(query)\n",
    "        num_chunks = tf.shape(query)[1] // chunk_size\n",
    "\n",
    "        outputs = []\n",
    "        for i in range(num_chunks):\n",
    "            start_idx = i * chunk_size\n",
    "            end_idx = (i + 1) * chunk_size\n",
    "\n",
    "            # Process the chunked portion of the sequence\n",
    "            chunk_output = self._process_chunk(\n",
    "                query[:, start_idx:end_idx], key, value\n",
    "            )\n",
    "            outputs.append(chunk_output)\n",
    "\n",
    "        # Concatenate all processed chunks to reconstruct the full sequence output\n",
    "        return tf.concat(outputs, axis=1)\n",
    "    \n",
    "    def _calculate_chunk_size(self, tensor):\n",
    "        \"\"\"\n",
    "        Computes the optimal chunk size for attention computation based on memory constraints.\n",
    "\n",
    "        Args:\n",
    "            tensor (tf.Tensor): The input tensor to determine chunking strategy.\n",
    "\n",
    "        Returns:\n",
    "            int: The computed chunk size to fit within the memory limit.\n",
    "        \"\"\"\n",
    "        element_size = tensor.dtype.size  # Get the size of each element in bytes\n",
    "        return min(\n",
    "            tf.shape(tensor)[1],  # Ensure chunk size does not exceed sequence length\n",
    "            self.max_memory // (element_size * tf.shape(tensor)[2])  # Memory-based limit\n",
    "        )\n",
    "    \n",
    "    def _process_chunk(self, query_chunk, key, value):\n",
    "        \"\"\"\n",
    "        Computes scaled dot-product attention for a single chunk.\n",
    "\n",
    "        Args:\n",
    "            query_chunk (tf.Tensor): A chunk of the query tensor (batch_size, chunk_size, dim).\n",
    "            key (tf.Tensor): The full key tensor (batch_size, seq_len, dim).\n",
    "            value (tf.Tensor): The full value tensor (batch_size, seq_len, dim).\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The attention-weighted value tensor for the chunk.\n",
    "        \"\"\"\n",
    "        # Compute scaled dot-product attention scores\n",
    "        scores = tf.matmul(query_chunk, key, transpose_b=True)\n",
    "        scores = scores / tf.sqrt(tf.cast(tf.shape(key)[-1], tf.float32))  # Scale by sqrt(dim)\n",
    "        attention = tf.nn.softmax(scores, axis=-1)  # Apply softmax to get attention weights\n",
    "        \n",
    "        # Multiply attention scores with value tensor\n",
    "        return tf.matmul(attention, value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Class Initialization\n",
    "    ```python\n",
    "    def __init__(self, max_memory=None):\n",
    "        self.max_memory = max_memory\n",
    "        self.cache = {}\n",
    "    ```\n",
    "    - **Purpose**: Initializes paged attention system\n",
    "    - **Parameters**:\n",
    "    - max_memory: Memory limit for chunks\n",
    "    - **Features**: \n",
    "    - Caching mechanism\n",
    "    - Memory management\n",
    "\n",
    "2. Main Computation Method\n",
    "    ```python\n",
    "    def compute(self, query, key, value):\n",
    "        batch_size = tf.shape(query)[0]\n",
    "        \n",
    "        # Split computation into manageable chunks\n",
    "        chunk_size = self._calculate_chunk_size(query)\n",
    "        num_chunks = tf.shape(query)[1] // chunk_size\n",
    "    ```\n",
    "    - **Purpose**: Manages chunked attention computation\n",
    "    - **Process**:\n",
    "    1. Determines chunk size\n",
    "    2. Calculates number of chunks\n",
    "    3. Processes each chunk separately\n",
    "\n",
    "3. Chunk Size Calculation\n",
    "    ```python\n",
    "    def _calculate_chunk_size(self, tensor):\n",
    "        element_size = tensor.dtype.size\n",
    "        return min(\n",
    "            tf.shape(tensor)[1],\n",
    "            self.max_memory // (element_size * tf.shape(tensor)[2])\n",
    "        )\n",
    "    ```\n",
    "    - **Purpose**: Determines optimal chunk size\n",
    "    - **Factors**:\n",
    "    - Memory limit\n",
    "    - Element size\n",
    "    - Tensor dimensions\n",
    "\n",
    "4. Chunk Processing\n",
    "    ```python\n",
    "    def _process_chunk(self, query_chunk, key, value):\n",
    "        # Compute attention for chunk\n",
    "        scores = tf.matmul(query_chunk, key, transpose_b=True)\n",
    "        scores = scores / tf.sqrt(tf.cast(tf.shape(key)[-1], tf.float32))\n",
    "        attention = tf.nn.softmax(scores, axis=-1)\n",
    "        return tf.matmul(attention, value)\n",
    "    ```\n",
    "    - **Purpose**: Processes individual attention chunks\n",
    "    - **Steps**:\n",
    "    1. Compute attention scores\n",
    "    2. Apply scaling factor\n",
    "    3. Calculate softmax\n",
    "    4. Compute final values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Components:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Memory Management\n",
    "    ```python\n",
    "    chunk_size = self._calculate_chunk_size(query)\n",
    "    num_chunks = tf.shape(query)[1] // chunk_size\n",
    "    ```\n",
    "    - Manages memory usage\n",
    "    - Prevents OOM errors\n",
    "    - Optimizes chunk size\n",
    "\n",
    "2. Chunked Processing\n",
    "    ```python\n",
    "    outputs = []\n",
    "    for i in range(num_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = (i + 1) * chunk_size\n",
    "        \n",
    "        chunk_output = self._process_chunk(\n",
    "            query[:, start_idx:end_idx],\n",
    "            key,\n",
    "            value\n",
    "        )\n",
    "        outputs.append(chunk_output)\n",
    "    ```\n",
    "    - Processes in manageable chunks\n",
    "    - Maintains sequence order\n",
    "    - Accumulates results\n",
    "\n",
    "3. Attention Computation\n",
    "    ```python\n",
    "    scores = tf.matmul(query_chunk, key, transpose_b=True)\n",
    "    scores = scores / tf.sqrt(tf.cast(tf.shape(key)[-1], tf.float32))\n",
    "    attention = tf.nn.softmax(scores, axis=-1)\n",
    "    ```\n",
    "    - Standard attention mechanism\n",
    "    - Scaled dot-product attention\n",
    "    - Memory-efficient implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Wrapper Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLoRAModelWrapper:\n",
    "    \"\"\"\n",
    "    A wrapper for applying QLoRA (Quantized Low-Rank Adaptation) to a base model.\n",
    "\n",
    "    QLoRA reduces memory usage by quantizing weights and introducing \n",
    "    low-rank trainable matrices, allowing efficient fine-tuning of large models.\n",
    "\n",
    "    Attributes:\n",
    "        base_model (tf.keras.Model): The original pre-trained model.\n",
    "        rank (int): The rank of LoRA decomposition (low-rank factor).\n",
    "        alpha (int): Scaling factor for LoRA updates.\n",
    "        bits (int): Number of bits for quantization (e.g., 4-bit).\n",
    "        group_size (int): The size of quantization groups.\n",
    "        qlora_layers (list): A list storing all applied QLoRA layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 base_model,\n",
    "                 rank=8,\n",
    "                 alpha=32,\n",
    "                 bits=4,\n",
    "                 group_size=128):\n",
    "        \"\"\"\n",
    "        Initializes the QLoRAModelWrapper.\n",
    "\n",
    "        Args:\n",
    "            base_model (tf.keras.Model): The original model to be adapted with QLoRA.\n",
    "            rank (int, optional): The rank of the LoRA decomposition. Default is 8.\n",
    "            alpha (int, optional): Scaling factor for LoRA. Default is 32.\n",
    "            bits (int, optional): Number of bits for weight quantization. Default is 4.\n",
    "            group_size (int, optional): The size of weight groups for quantization. Default is 128.\n",
    "        \"\"\"\n",
    "        self.base_model = base_model  # Store the original model\n",
    "        self.rank = rank  # LoRA rank (controls compression level)\n",
    "        self.alpha = alpha  # LoRA scaling factor\n",
    "        self.bits = bits  # Bit-width for quantization\n",
    "        self.group_size = group_size  # Number of grouped weights per quantization\n",
    "        self.qlora_layers = []  # Store LoRA-applied layers\n",
    "    \n",
    "    def apply_qlora(self, layer_names=None):\n",
    "        \"\"\"\n",
    "        Applies QLoRA modifications to specific layers in the model.\n",
    "\n",
    "        Args:\n",
    "            layer_names (list, optional): A list of layer name substrings to be adapted with QLoRA.\n",
    "                                          Default applies LoRA to ['query', 'key', 'value'] layers.\n",
    "\n",
    "        Returns:\n",
    "            tf.keras.Model: A new model with QLoRA modifications applied.\n",
    "        \"\"\"\n",
    "        if layer_names is None:\n",
    "            layer_names = ['query', 'key', 'value']  # Default: Apply QLoRA to attention layers\n",
    "\n",
    "        def replace_layer(layer):\n",
    "            \"\"\"\n",
    "            Replaces the specified layers with their QLoRA-adapted versions.\n",
    "\n",
    "            Args:\n",
    "                layer (tf.keras.layers.Layer): The current layer in the model.\n",
    "\n",
    "            Returns:\n",
    "                tf.keras.layers.Layer: The original or modified layer.\n",
    "            \"\"\"\n",
    "            # Check if the layer's name matches any of the target layer names\n",
    "            if any(name in layer.name for name in layer_names):\n",
    "                if isinstance(layer, tf.keras.layers.Dense):  # Apply only to Dense layers\n",
    "                    qlora_layer = QLoRALayer(\n",
    "                        layer,\n",
    "                        rank=self.rank,\n",
    "                        alpha=self.alpha,\n",
    "                        bits=self.bits,\n",
    "                        group_size=self.group_size\n",
    "                    )\n",
    "                    self.qlora_layers.append(qlora_layer)  # Track applied layers\n",
    "                    return qlora_layer  # Replace with QLoRA layer\n",
    "            return layer  # Keep the layer unchanged\n",
    "\n",
    "        # Clone the base model while applying QLoRA modifications\n",
    "        new_model = tf.keras.models.clone_model(\n",
    "            self.base_model,\n",
    "            clone_function=replace_layer\n",
    "        )\n",
    "\n",
    "        return new_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Class Initialization\n",
    "    ```python\n",
    "    def __init__(self,\n",
    "                base_model,\n",
    "                rank=8,\n",
    "                alpha=32,\n",
    "                bits=4,\n",
    "                group_size=128):\n",
    "        self.base_model = base_model\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.bits = bits\n",
    "        self.group_size = group_size\n",
    "        self.qlora_layers = []\n",
    "    ```\n",
    "    - **Purpose**: Initializes Q-LoRA wrapper\n",
    "    - **Parameters**:\n",
    "    - base_model: Original model to adapt\n",
    "    - rank: LoRA rank dimension\n",
    "    - alpha: Scaling factor\n",
    "    - bits: Quantization precision\n",
    "    - group_size: Quantization group size\n",
    "    - **Storage**: Tracks modified layers\n",
    "\n",
    "2. Layer Replacement Method\n",
    "    ```python\n",
    "    def apply_qlora(self, layer_names=None):\n",
    "        if layer_names is None:\n",
    "            layer_names = ['query', 'key', 'value']\n",
    "    ```\n",
    "    - **Purpose**: Applies Q-LoRA to specified layers\n",
    "    - **Default Targets**: \n",
    "    - query layers\n",
    "    - key layers\n",
    "    - value layers\n",
    "\n",
    "3. Layer Replacement Function\n",
    "    ```python\n",
    "    def replace_layer(layer):\n",
    "        if any(name in layer.name for name in layer_names):\n",
    "            if isinstance(layer, tf.keras.layers.Dense):\n",
    "                qlora_layer = QLoRALayer(\n",
    "                    layer,\n",
    "                    rank=self.rank,\n",
    "                    alpha=self.alpha,\n",
    "                    bits=self.bits,\n",
    "                    group_size=self.group_size\n",
    "                )\n",
    "                self.qlora_layers.append(qlora_layer)\n",
    "                return qlora_layer\n",
    "        return layer\n",
    "    ```\n",
    "    - **Purpose**: Handles individual layer replacement\n",
    "    - **Process**:\n",
    "    1. Checks layer name match\n",
    "    2. Verifies layer type\n",
    "    3. Creates Q-LoRA layer\n",
    "    4. Tracks modifications\n",
    "\n",
    "4. Model Modification\n",
    "    ```python\n",
    "    # Clone and modify model\n",
    "    new_model = tf.keras.models.clone_model(\n",
    "        self.base_model,\n",
    "        clone_function=replace_layer\n",
    "    )\n",
    "    ```\n",
    "    - **Purpose**: Creates adapted model\n",
    "    - **Features**:\n",
    "    - Non-destructive modification\n",
    "    - Preserves original model\n",
    "    - Selective adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Configuration Management\n",
    "   ```python\n",
    "   self.rank = rank\n",
    "   self.alpha = alpha\n",
    "   self.bits = bits\n",
    "   self.group_size = group_size\n",
    "   ```\n",
    "   - Centralized parameter storage\n",
    "   - Consistent configuration\n",
    "   - Easy modification\n",
    "\n",
    "2. Layer Tracking\n",
    "   ```python\n",
    "   self.qlora_layers.append(qlora_layer)\n",
    "   ```\n",
    "   - Maintains layer registry\n",
    "   - Enables monitoring\n",
    "   - Facilitates management\n",
    "\n",
    "3. Selective Adaptation\n",
    "   ```python\n",
    "   if any(name in layer.name for name in layer_names):\n",
    "   ```\n",
    "   - Targeted modifications\n",
    "   - Flexible layer selection\n",
    "   - Controlled adaptation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLoRATrainer:\n",
    "    \"\"\"\n",
    "    A trainer for fine-tuning models with QLoRA (Quantized Low-Rank Adaptation)\n",
    "    and memory-efficient attention.\n",
    "\n",
    "    This class uses `PagedAttention` to reduce memory consumption while training\n",
    "    large models and applies gradient-based optimization.\n",
    "\n",
    "    Attributes:\n",
    "        model (tf.keras.Model): The model being fine-tuned with QLoRA.\n",
    "        learning_rate (float): Learning rate for optimization.\n",
    "        paged_attention (PagedAttention): A memory-efficient attention mechanism.\n",
    "        optimizer (tf.keras.optimizers.Optimizer): The optimizer for training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 learning_rate=1e-4,\n",
    "                 max_memory=None):\n",
    "        \"\"\"\n",
    "        Initializes the QLoRATrainer.\n",
    "\n",
    "        Args:\n",
    "            model (tf.keras.Model): The model to be fine-tuned with QLoRA.\n",
    "            learning_rate (float, optional): The learning rate for optimization. Default is 1e-4.\n",
    "            max_memory (int, optional): The maximum memory limit for PagedAttention. Default is None.\n",
    "        \"\"\"\n",
    "        self.model = model  # Store the QLoRA model\n",
    "        self.learning_rate = learning_rate  # Learning rate for training\n",
    "        self.paged_attention = PagedAttention(max_memory)  # Memory-efficient attention\n",
    "        \n",
    "        # Define an optimizer (default: Adam)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, inputs, labels):\n",
    "        \"\"\"\n",
    "        Performs a single training step using memory-efficient attention.\n",
    "\n",
    "        Args:\n",
    "            inputs (tf.Tensor): The input data (e.g., tokenized sequences).\n",
    "            labels (tf.Tensor): The target labels for training.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The computed loss value for this step.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass using memory-efficient PagedAttention\n",
    "            predictions = self.model(\n",
    "                inputs,\n",
    "                attention_implementation=self.paged_attention\n",
    "            )\n",
    "            loss = self.compute_loss(labels, predictions)  # Compute loss\n",
    "        \n",
    "        # Compute gradients of the loss w.r.t model parameters\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        \n",
    "        # Apply gradients using optimizer\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def compute_loss(self, labels, predictions):\n",
    "        \"\"\"\n",
    "        Computes the loss function for model training.\n",
    "\n",
    "        Args:\n",
    "            labels (tf.Tensor): The ground-truth labels.\n",
    "            predictions (tf.Tensor): The model predictions.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The computed loss value.\n",
    "        \"\"\"\n",
    "        return tf.keras.losses.sparse_categorical_crossentropy(labels, predictions, from_logits=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Class Initialization\n",
    "    ```python\n",
    "    def __init__(self,\n",
    "                model,\n",
    "                learning_rate=1e-4,\n",
    "                max_memory=None):\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.paged_attention = PagedAttention(max_memory)\n",
    "    ```\n",
    "    - **Purpose**: Sets up Q-LoRA training environment\n",
    "    - **Parameters**:\n",
    "    - model: Q-LoRA adapted model\n",
    "    - learning_rate: Training rate\n",
    "    - max_memory: Memory limit for attention\n",
    "    - **Features**: \n",
    "    - Memory-efficient attention\n",
    "    - Configurable learning rate\n",
    "\n",
    "2. Training Step Method\n",
    "    ```python\n",
    "    @tf.function  # TensorFlow optimization decorator\n",
    "    def train_step(self, inputs, labels):\n",
    "    ```\n",
    "    - **Purpose**: Executes single training iteration\n",
    "    - **Optimization**: Graph mode execution\n",
    "    - **Parameters**:\n",
    "    - inputs: Training data\n",
    "    - labels: Target values\n",
    "\n",
    "3. Forward Pass\n",
    "    ```python\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass with memory-efficient attention\n",
    "        predictions = self.model(\n",
    "            inputs,\n",
    "            attention_implementation=self.paged_attention\n",
    "        )\n",
    "        loss = self.compute_loss(labels, predictions)\n",
    "    ```\n",
    "    - **Purpose**: Computes model predictions\n",
    "    - **Features**:\n",
    "    - Gradient tracking\n",
    "    - Paged attention usage\n",
    "    - Loss computation\n",
    "\n",
    "4. Gradient Computation and Application\n",
    "    ```python\n",
    "    # Compute gradients\n",
    "    gradients = tape.gradient(\n",
    "        loss,\n",
    "        self.model.trainable_variables\n",
    "    )\n",
    "\n",
    "    # Apply gradients\n",
    "    self.optimizer.apply_gradients(\n",
    "        zip(gradients, self.model.trainable_variables)\n",
    "    )\n",
    "    ```\n",
    "    - **Purpose**: Updates model parameters\n",
    "    - **Process**:\n",
    "    1. Compute gradients\n",
    "    2. Apply updates\n",
    "    3. Return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Components:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Memory Management\n",
    "   ```python\n",
    "   self.paged_attention = PagedAttention(max_memory)\n",
    "   ```\n",
    "   - Efficient attention computation\n",
    "   - Memory-aware processing\n",
    "   - Controlled resource usage\n",
    "\n",
    "2. Optimization\n",
    "   ```python\n",
    "   @tf.function\n",
    "   def train_step(self, inputs, labels):\n",
    "   ```\n",
    "   - Graph compilation\n",
    "   - Performance optimization\n",
    "   - Efficient execution\n",
    "\n",
    "3. Gradient Management\n",
    "   ```python\n",
    "   gradients = tape.gradient(\n",
    "      loss,\n",
    "      self.model.trainable_variables\n",
    "   )\n",
    "   ```\n",
    "   - Automatic differentiation\n",
    "   - Parameter updates\n",
    "   - Training optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUgRVUfXQKl4"
   },
   "source": [
    "<h1>\n",
    "Instruct Tune\n",
    "</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDUpAOnYQQSo"
   },
   "source": [
    "# Brief Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxHI6AdUQRdh"
   },
   "source": [
    "**Instruction tuning** is a technique used to enhance the performance of large language models (LLMs) by fine-tuning them on datasets containing instruction-output pairs. This process aims to improve the model's ability to understand and follow natural language instructions, making it more versatile and capable of performing a wide range of tasks without extensive prompt engineering.\n",
    "\n",
    "Instruction tuning was introduced in 2021 by Google Research. Their influential paper, \"Finetuned Language Models are Zero-Shot Learners,\" presented the concept of instruction tuning as a technique to improve the ability of large language models (LLMs) to respond to natural language processing (NLP) instructions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWQGPeAmYLMq"
   },
   "source": [
    "# Architecture\n",
    "\n",
    "<img src='assets/arch.png' width=500>\n",
    "\n",
    "This flowchart illustrates the complete instruction tuning pipeline and inference process. Let me break down each component:\n",
    "\n",
    "* **Pre-trained Language Model:**\n",
    "The starting point is a base language model that has already been trained on large amounts of text data. This model has general language understanding capabilities but hasn't been specifically optimized for following instructions.\n",
    "\n",
    "* **Fine-tuning on Instruction Dataset:**\n",
    "This stage involves training the base model on carefully curated instruction-response pairs. The model learns to understand and respond to specific instructions through this process, adapting its weights to better handle directed tasks.\n",
    "\n",
    "* **Instruction-tuned Model:**\n",
    "The resulting model after the fine-tuning process is now specialized in understanding and responding to instructions. It maintains the base knowledge from pre-training but has enhanced capabilities for following specific directives.\n",
    "\n",
    "* **User Prompt:**\n",
    "This represents the actual input from users - questions, instructions, or tasks that they want the model to process. It's separate from the training process and represents the real-world usage of the model.\n",
    "\n",
    "* **Inference:**\n",
    "This is the decision-making stage where the instruction-tuned model processes the user prompt. The model applies its learned knowledge to interpret the prompt and determine an appropriate response.\n",
    "\n",
    "* **Generated Response:**\n",
    "The final output produced by the model in response to the user prompt. This represents the model's attempt to fulfill the given instruction or answer the question based on its training.\n",
    "\n",
    "The flowchart effectively shows how the model evolves from a general-purpose language model to a specialized instruction-following system, and how it ultimately processes user inputs to generate appropriate responses.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkFEkfkHaQtS"
   },
   "source": [
    "# Use cases\n",
    "\n",
    "Instruction tuning has numerous practical applications across various industries. Here are the key use cases:\n",
    "\n",
    "* **Virtual Assistance**\n",
    "  - Customer service chatbots with enhanced understanding of user queries\n",
    "  - Personalized virtual assistants for task automation\n",
    "  - Real-time support systems with improved response accuracy\n",
    "\n",
    "* **Educational Technology**\n",
    "\n",
    "  - Personalized tutoring systems\n",
    "  - Interactive learning tools with real-time feedback\n",
    "  - Adaptive educational content delivery\n",
    "\n",
    "* **Sales and Business**\n",
    "\n",
    "  - Real-time suggestions for sales representatives\n",
    "  - Conversation guidance during customer calls\n",
    "  - Objection handling assistance\n",
    "\n",
    "* **Financial Services**\n",
    "\n",
    "  - Personalized investment recommendations\n",
    "  - Risk assessment analysis\n",
    "  - Financial planning assistance\n",
    "\n",
    "* **Healthcare**\n",
    "  - Diagnostic assistance for healthcare professionals\n",
    "  - Analysis of patient data and symptoms\n",
    "  - Medical literature recommendations\n",
    "\n",
    "* **Software Development**\n",
    "\n",
    "  - Code review automation\n",
    "  - Bug detection and quality assessment\n",
    "  - Performance optimization suggestions\n",
    "  - Best practices recommendations\n",
    "\n",
    "* **Content Creation**\n",
    "  - Text summarization\n",
    "  - Document analysis\n",
    "  - Content generation for specific domains\n",
    "\n",
    "The versatility of instruction tuning makes it particularly valuable in scenarios requiring specialized knowledge and natural language understanding, while maintaining the ability to adapt to new contexts and requirements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfyOEN7AbTa9"
   },
   "source": [
    "# Instruction Tune Implementation\n",
    "\n",
    "### **Methodology**\n",
    "* **Dataset Preparation**: Construct a dataset of diverse instruction-output pairs, either manually or through automated generation.\n",
    "* **Input Format**: Each training sample typically consists of three elements:\n",
    "  * **Instruction**: The task description\n",
    "  * **Optional input**: Supplementary context information\n",
    "  * **Anticipated output**: The desired response\n",
    "* **Training Objective**: The model is trained to predict each token in the output sequence given the instruction and input.\n",
    "* **Optimization**: The model's parameters are adjusted using techniques like gradient descent to minimize the difference between predicted and target outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the same dummy dataset\n",
    "dummy_data = [\n",
    "    {\n",
    "        \"instruction\": \"Translate English to French\",\n",
    "        \"input\": \"Hello, how are you?\",\n",
    "        \"output\": \"Bonjour, comment Ã§a va?\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Summarize the following text\",\n",
    "        \"input\": \"Artificial intelligence is transforming technology.\",\n",
    "        \"output\": \"AI is changing tech.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Convert temperature from Celsius to Fahrenheit\",\n",
    "        \"input\": \"25 C\",\n",
    "        \"output\": \"77 F\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Implementation of dataset preparation\n",
    "def prepare_instruction_data(data):\n",
    "    formatted_examples = []\n",
    "    for item in data:\n",
    "        prompt = f\"### Instruction:\\n{item['instruction']}\\n\\n\"\n",
    "        prompt += f\"### Input:\\n{item['input']}\\n\\n\"\n",
    "        prompt += f\"### Response:\\n{item['output']}\"\n",
    "        \n",
    "        formatted_examples.append({\n",
    "            \"text\": prompt,\n",
    "            \"labels\": item['output']\n",
    "        })\n",
    "    return formatted_examples\n",
    "\n",
    "# Training setup using TensorFlow\n",
    "def create_training_batch(examples, tokenizer):\n",
    "    # Tokenize the examples\n",
    "    encoded = tokenizer(\n",
    "        [ex[\"text\"] for ex in examples],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": encoded[\"input_ids\"],\n",
    "        \"attention_mask\": encoded[\"attention_mask\"],\n",
    "        \"labels\": encoded[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "# Inference function for TensorFlow\n",
    "def generate_response(instruction, input_text, model, tokenizer):\n",
    "    prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"tf\", padding=True)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=128,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        no_repeat_ngram_size=3\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "model_name = \"gpt2\"  # Using GPT-2 as an example\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"  # For causal language models\n",
    "\n",
    "model = TFAutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_layer = model.get_layer(\"transformer\")\n",
    "print(transformer_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for weight in transformer_layer.trainable_weights:\n",
    "    print(weight.name, weight.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transformer_layer.get_config())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for weight in transformer_layer.trainable_weights:\n",
    "    print(weight.name, weight.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub_layer in transformer_layer.submodules:\n",
    "    print(sub_layer.name, sub_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(transformer_layer, to_file=\"model.png\", show_shapes=True, show_layer_names=True, expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netron\n",
    "netron.start(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netron.stop(8080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare the dataset\n",
    "formatted_data = prepare_instruction_data(dummy_data)\n",
    "\n",
    "# Create a training batch\n",
    "batch = create_training_batch(formatted_data, tokenizer)\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"instruction\": \"Translate English to French\",\n",
    "        \"input\": \"Good morning!\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Summarize the following text\",\n",
    "        \"input\": \"The weather today is sunny with clear skies and mild temperatures.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Convert temperature from Celsius to Fahrenheit\",\n",
    "        \"input\": \"30 C\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Example of inference\n",
    "instruction = \"Translate English to French\"\n",
    "input_text = \"Good morning!\"\n",
    "response = generate_response(instruction, input_text, model, tokenizer)\n",
    "print(f\"Generated Response: {response}\")\n",
    "\n",
    "# Generate responses for each example\n",
    "for example in examples:\n",
    "    response = generate_response(\n",
    "        example[\"instruction\"],\n",
    "        example[\"input\"],\n",
    "        model,\n",
    "        tokenizer\n",
    "    )\n",
    "    print(f\"\\nInstruction: {example['instruction']}\")\n",
    "    print(f\"Input: {example['input']}\")\n",
    "    print(f\"Generated Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRpTY_JRwg1k"
   },
   "source": [
    "**Reflections**\n",
    "\n",
    "* The model failed to generate any responses for both test cases:\n",
    "  * **Translation task**: No response generated\n",
    "  * **Summarization task**: No response generated\n",
    "* This suggests that while the model's training loss improved significantly, there are issues with the generation phase that need to be addressed:\n",
    "  * The model might need more training data\n",
    "  * Generation parameters may need adjustment\n",
    "* The connection between training and inference might be broken.\n",
    "To fix this, we should:\n",
    "  * Increase the training dataset size\n",
    "  * Adjust generation parameters (temperature, top_p, etc.)\n",
    "  * Verify the inference pipeline is properly connected to the trained model\n",
    "Add more diverse examples to the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Release Memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete models and variables\n",
    "del model\n",
    "del batch\n",
    "\n",
    "# Clear Keras/TF session\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Garbage collection\n",
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4bMV79vMUEU"
   },
   "source": [
    "# English to French Translation using LoRA\n",
    "\n",
    "#### **Dataset**\n",
    "We will use the MTNT (Machine Translation of Noisy Text) dataset, which is available from TensorFlow Datasets. MTNT is a collection of comments from the Reddit discussion website in English, French and Japanese, translated to and from English. The particularity of this dataset is that the data consists of \"noisy\" text, that exhibits typos, grammar errors, code switching and more.\n",
    "\n",
    "In this example, we will use the **French-to-English** portion of the dataset.\n",
    "\n",
    "#### **Initial Setup**\n",
    "* Install and import all the\n",
    "libraries we need. We'll be using the KerasHub library.\n",
    "\n",
    "* Secondly, let's set the precision to bfloat16. This will help us reduce the\n",
    "memory usage and training time.\n",
    "\n",
    "* Also, ensure that `KAGGLE_USERNAME` and `KAGGLE_KEY` have been correctly\n",
    "configured to access the Gemma model.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iA_5hIqpNauh"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\".env\")\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # Suppress verbose logging from TF\n",
    "\n",
    "\n",
    "import keras\n",
    "import keras_hub\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "keras.config.set_dtype_policy(\"bfloat16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eOaNW_0iNasU"
   },
   "outputs": [],
   "source": [
    "# Downloading dataset\n",
    "train_ds = tfds.load(\"mtnt/fr-en\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBhWXaP5KR-b"
   },
   "source": [
    "We can print some samples. Each sample in the dataset contains two entries:\n",
    "\n",
    "- src: the original French sentence.\n",
    "- dst: the corresponding English translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9OvsGYu4KR-c"
   },
   "outputs": [],
   "source": [
    "examples = train_ds.take(3)\n",
    "examples = examples.as_numpy_iterator()\n",
    "\n",
    "for idx, example in enumerate(examples):\n",
    "    print(f\"Example {idx}:\")\n",
    "    for key, val in example.items():\n",
    "        print(f\"{key}: {val}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmX9jTClKR-c"
   },
   "source": [
    "Since we will fine-tune our model to perform a French-to-English translation\n",
    "task, we should format the inputs for instruction tuning. For example, we could\n",
    "format the translation task in this example like:\n",
    "\n",
    "```\n",
    "<start_of_turn>user\n",
    "Translate French into English:\n",
    "{src}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "{dst}<end_of_turn>\n",
    "```\n",
    "\n",
    "The special tokens such as `<start_of_turn>user`, `<start_of_turn>model` and\n",
    "`<end_of_turn>` are used for Gemma models. You can learn more from\n",
    "https://ai.google.dev/gemma/docs/formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kn2VEmIOKR-c"
   },
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(\n",
    "    lambda x: tf.strings.join(\n",
    "        [\n",
    "            \"<start_of_turn>user\\n\",\n",
    "            \"Translate French into English:\\n\",\n",
    "            x[\"src\"],\n",
    "            \"<end_of_turn>\\n\",\n",
    "            \"<start_of_turn>model\\n\",\n",
    "            \"Translation:\\n\",\n",
    "            x[\"dst\"],\n",
    "            \"<end_of_turn>\",\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE) # Add prefetch with AUTOTUNE\n",
    "\n",
    "examples = train_ds.take(3)\n",
    "examples = examples.as_numpy_iterator()\n",
    "\n",
    "for idx, example in enumerate(examples):\n",
    "    print(f\"Example {idx}:\")\n",
    "    print(example)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCclVyZgKR-c"
   },
   "source": [
    "We will take a subset of the dataset for the purpose of this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NNXSVO1KKR-c"
   },
   "outputs": [],
   "source": [
    "train_ds = train_ds.batch(1).take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTWSE40BKR-c"
   },
   "source": [
    "\n",
    "\n",
    "KerasHub provides implementations of many popular model architectures.\n",
    "In this example, we will use `GemmaCausalLM`, an end-to-end Gemma model for\n",
    "causal language modeling. A causal language model predicts the next token based\n",
    "on previous tokens.\n",
    "\n",
    "Note that `sequence_length` is set to `256` to speed up the fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSJUsA5ZKR-c"
   },
   "outputs": [],
   "source": [
    "preprocessor = keras_hub.models.GemmaCausalLMPreprocessor.from_preset(\n",
    "    \"gemma_1.1_instruct_2b_en\", sequence_length=256\n",
    ")\n",
    "gemma_lm = keras_hub.models.GemmaCausalLM.from_preset(\n",
    "    \"gemma_1.1_instruct_2b_en\", preprocessor=preprocessor\n",
    ")\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.saved_model.save(preprocessor, \"tf_processor_gemma_1.1_instruct_2b_en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Brab6qBDKR-c"
   },
   "source": [
    "## LoRA Instruct-tuning\n",
    "\n",
    "<img src='assets/lora.png' width=500>\n",
    "\n",
    "### What exactly is LoRA?\n",
    "\n",
    "Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning technique for\n",
    "LLMs. It freezes the weights of the LLM, and injects trainable\n",
    "rank-decomposition matrices. Let's understand this more clearly.\n",
    "\n",
    "Assume we have an `n x n` pre-trained dense layer (or weight matrix), `W0`. We\n",
    "initialize two dense layers, `A` and `B`, of shapes `n x rank`, and `rank x n`,\n",
    "respectively. `rank` is much smaller than `n`. In the paper, values between 1\n",
    "and 4 are shown to work well.\n",
    "\n",
    "### LoRA equation\n",
    "\n",
    "The original equation is `output = W0x + b0`, where `x` is the input, `W0` and\n",
    "`b0` are the weight matrix and bias terms of the original dense layer (frozen).\n",
    "The LoRA equation is: `output = W0x + b0 + BAx`, where `A` and `B` are the\n",
    "rank-decomposition matrices.\n",
    "\n",
    "LoRA is based on the idea that updates to the weights of the pre-trained\n",
    "language model have a low \"intrinsic rank\" since pre-trained language models are\n",
    "over-parametrized. Predictive performance of full fine-tuning can be replicated\n",
    "even by constraining `W0`'s updates to low-rank decomposition matrices.\n",
    "\n",
    "### Number of trainable parameters\n",
    "\n",
    "Let's do some quick math. Suppose `n` is 768, and `rank` is 4. `W0` has\n",
    "`768 x 768 = 589,824` parameters, whereas the LoRA layers, `A` and `B` together\n",
    "have `768 x 4 + 4 x 768 = 6,144` parameters. So, for the dense layer, we go\n",
    "from `589,824` trainable parameters to `6,144` trainable parameters!\n",
    "\n",
    "### Why does LoRA reduce memory footprint?\n",
    "\n",
    "Even though the total number of parameters increase\n",
    "(since we are adding LoRA layers), the memory footprint reduces, because the\n",
    "number of trainable parameters reduces. Let's dive deeper into this.\n",
    "\n",
    "The memory usage of a model can be split into four parts:\n",
    "\n",
    "- Model memory: This is the memory required to store the model weights. This\n",
    "will be slightly higher for LoRA than the original model.\n",
    "- Forward pass memory: This mostly depends on batch size, sequence length, etc.\n",
    "We keep this constant for both models for a fair comparison.\n",
    "- Backward pass memory: This is the memory required to store the gradients. Note\n",
    "that the gradients are computed only for the trainable parameters.\n",
    "- Optimizer memory: This is the memory required to store the optimizer state.\n",
    "For example, the Adam optimizer stores the \"1st moment vectors\" and\n",
    "\"2nd moment vectors\" for the trainable parameters.\n",
    "\n",
    "Since, with LoRA, there is a huge reduction in the number of trainable\n",
    "parameters, the optimizer memory and the memory required to store the gradients\n",
    "for LoRA is much less than the original model. This is where most of the memory\n",
    "savings happen.\n",
    "\n",
    "### Why is LoRA so popular?\n",
    "\n",
    "- Reduces GPU memory usage;\n",
    "- Faster training; and\n",
    "- No additional inference latency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kW5OF-3wKR-c"
   },
   "source": [
    "When using KerasHub, we can enable LoRA with an one-line API:\n",
    "`enable_lora(rank=4)`\n",
    "\n",
    "From `gemma_lm.summary()`, we can see enabling LoRA reduces the number of\n",
    "trainable parameters significantly (from 2.5 billion to 1.3 million)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4EJl9SguKR-d"
   },
   "outputs": [],
   "source": [
    "gemma_lm.backbone.enable_lora(rank=2)\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Rjq30D0KR-d"
   },
   "source": [
    "Let's fine-tune the LoRA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BG-R806DKR-d"
   },
   "outputs": [],
   "source": [
    "# To save memory, use the SGD optimizer instead of the usual AdamW optimizer.\n",
    "# For this specific example, SGD is more than enough.\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-4)\n",
    "gemma_lm.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=optimizer,\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "gemma_lm.fit(train_ds, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFZmu48uKR-d"
   },
   "source": [
    "After fine-tuning, responses will follow the instructions provided in the\n",
    "prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EEcdoYaTKR-d"
   },
   "outputs": [],
   "source": [
    "template = (\n",
    "    \"<start_of_turn>user\\n\"\n",
    "    \"Translate French into English:\\n\"\n",
    "    \"{inputs}\"\n",
    "    \"<end_of_turn>\\n\"\n",
    "    \"<start_of_turn>model\\n\"\n",
    "    \"Translation:\\n\"\n",
    ")\n",
    "prompt = template.format(inputs=\"Bonjour, je m'appelle Morgane.\")\n",
    "outputs = gemma_lm.generate(prompt, max_length=256)\n",
    "print(\"Translation:\\n\", outputs.replace(prompt, \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that **LoRA instruct tuning** focuses on efficiently fine-tuning a pre-trained LLM for instruction following by adding a small number of trainable parameters while freezing the original model weights. This approach reduces memory usage and training time compared to traditional fine-tuning methods while achieving comparable performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJHovQXOKR-d"
   },
   "outputs": [],
   "source": [
    "preprocessor = keras_hub.models.GemmaCausalLMPreprocessor.from_preset(\n",
    "    \"gemma_1.1_instruct_2b_en\", sequence_length=128\n",
    ")\n",
    "gemma_lm = keras_hub.models.GemmaCausalLM.from_preset(\n",
    "    \"gemma_1.1_instruct_2b_en\", preprocessor=preprocessor\n",
    ")\n",
    "gemma_lm.quantize(\"int8\")\n",
    "gemma_lm.backbone.enable_lora(rank=4)\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9GtaMwQKR-d"
   },
   "source": [
    "Let's fine-tune the QLoRA model.\n",
    "\n",
    "If you are using a device with int8 acceleration support, you should see an\n",
    "improvement in the training speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N6GwocNiKR-d"
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=1e-4)\n",
    "gemma_lm.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=optimizer,\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "gemma_lm.fit(train_ds, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sw0DaUz_KR-d"
   },
   "source": [
    "You should get a similar output with QLoRA fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JzRobgUnKR-d"
   },
   "outputs": [],
   "source": [
    "prompt = template.format(inputs=\"Bonjour, je m'appelle Morgane.\")\n",
    "outputs = gemma_lm.generate(prompt, max_length=256)\n",
    "print(\"Translation:\\n\", outputs.replace(prompt, \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLIEIX9kKR-d"
   },
   "source": [
    "And we're all done!\n",
    "\n",
    "Note that for demonstration purposes, this example fine-tunes the model on a\n",
    "small subset of the dataset for just one epoch and with a low LoRA rank value.\n",
    "To get better responses from the fine-tuned model, you can experiment with:\n",
    "\n",
    "- Increasing the size of the fine-tuning dataset.\n",
    "- Training for more steps (epochs).\n",
    "- Setting a higher LoRA rank.\n",
    "- Modifying the hyperparameter values such as `learning_rate` and\n",
    "`weight_decay`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pgMU3VSnNaqZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "env_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

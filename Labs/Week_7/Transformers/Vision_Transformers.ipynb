{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformers (ViT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Recap of Vision Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vision Transformers (ViT) represent a groundbreaking approach to computer vision tasks, applying transformer architectures traditionally used in NLP to image processing. \n",
    "\n",
    "- Introduced by Dosovitskiy et al. in 2020 in their paper \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\", ViT has demonstrated remarkable performance on image classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vision Transformers work by:\n",
    "1. Splitting an image into fixed-size patches (typically 16x16 pixels)\n",
    "2. Linearly embedding these patches\n",
    "3. Adding positional embeddings\n",
    "4. Processing the resulting sequence with a standard Transformer encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key advantages of Vision Transformers include:\n",
    "- Global receptive field from the start, unlike CNNs which build it up gradually\n",
    "- Strong performance on large datasets\n",
    "- Efficient parallel processing of image patches\n",
    "- Ability to capture long-range dependencies in images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture of Vision Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Vision Transformer architecture consists of several key components:\n",
    "\n",
    "1. **Patch Embedding**: \n",
    "   - Divides the input image into non-overlapping patches\n",
    "   - Flattens each patch and projects it to a lower-dimensional space\n",
    "   - Adds a learnable classification token at the start of the sequence\n",
    "\n",
    "2. **Position Embedding**:\n",
    "   - Adds learnable position embeddings to provide spatial information\n",
    "   - Helps the model understand the relative positions of patches\n",
    "\n",
    "3. **Transformer Encoder**:\n",
    "   - Multiple layers of multi-head self-attention\n",
    "   - Feed-forward networks\n",
    "   - Layer normalization and residual connections\n",
    "\n",
    "4. **Classification Head**:\n",
    "   - Uses the transformed classification token for final prediction\n",
    "   - Typically consists of a simple MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Patch Creation and Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll implement the patch creation and embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(layers.Layer):\n",
    "    def __init__(self, img_size=32, patch_size=4, embed_dim=64):\n",
    "        super(PatchEmbed, self).__init__()\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = layers.Conv2D(\n",
    "            filters=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            strides=patch_size,\n",
    "            padding='VALID'\n",
    "        )\n",
    "        self.flatten = layers.Reshape((-1, embed_dim))\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = self.flatten(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch Embedding Layer Explanation\n",
    "\n",
    "The PatchEmbed class is a crucial component of the Vision Transformer that transforms input images into a sequence of embedded patches.\n",
    "\n",
    "**Input Processing**\n",
    "- Takes an image of size `img_size x img_size` (default 32x32)\n",
    "- Divides it into non-overlapping patches of size `patch_size x patch_size` (default 4x4)\n",
    "- The total number of patches is calculated as `(img_size // patch_size)Â²`[1]\n",
    "\n",
    "**Layer Components**\n",
    "\n",
    "1. **Convolutional Projection**\n",
    "- Uses a Conv2D layer with:\n",
    "  - `embed_dim` output filters (default 64)\n",
    "  - Kernel size equal to patch_size (4x4)\n",
    "  - Stride equal to patch_size for non-overlapping patches\n",
    "  - 'VALID' padding to avoid padding artifacts[1]\n",
    "\n",
    "2. **Flattening Operation**\n",
    "- Reshapes the convolution output into a sequence of patch embeddings\n",
    "- Transforms the 4D tensor into a 2D sequence of shape `(num_patches, embed_dim)`[1]\n",
    "\n",
    "**Workflow**\n",
    "1. Input image (batch_size, height, width, channels)\n",
    "2. Conv2D projects patches to embedding dimension\n",
    "3. Reshape flattens spatial dimensions into sequence\n",
    "4. Output shape: (batch_size, num_patches, embed_dim)[1]\n",
    "\n",
    "This layer effectively converts a 2D image into a sequence of embedded patches that can be processed by the transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Vision Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll implement the complete Vision Transformer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleViT(tf.keras.Model):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleViT, self).__init__()\n",
    "        self.patch_embed = PatchEmbed()\n",
    "        \n",
    "        # Fix the add_weight call by using keyword arguments properly\n",
    "        self.pos_embed = self.add_weight(\n",
    "            shape=(1, 64, 64),  # (1, num_patches, embed_dim)\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True,\n",
    "            name=\"pos_embed\"\n",
    "        )\n",
    "        \n",
    "        self.attention = layers.MultiHeadAttention(num_heads=4, key_dim=16)\n",
    "        self.layernorm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.mlp = tf.keras.Sequential([\n",
    "            layers.Dense(128, activation=\"gelu\"),\n",
    "            layers.Dense(64)\n",
    "        ])\n",
    "        \n",
    "        self.head = layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Create patches\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # Add position embeddings directly\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        # Transformer block\n",
    "        attention_output = self.attention(x, x)\n",
    "        x = self.layernorm(x + attention_output)\n",
    "        x = x + self.mlp(x)\n",
    "        \n",
    "        # Classification head\n",
    "        x = tf.reduce_mean(x, axis=1)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleViT Architecture Explanation\n",
    "\n",
    "The SimpleViT class implements a simplified version of the Vision Transformer architecture with the following key components:\n",
    "\n",
    "**Initialization Components**\n",
    "- PatchEmbed layer converts the input image into patches and embeds them\n",
    "- Position embeddings matrix of shape (1, 64, 64) to encode spatial information[1]\n",
    "- Multi-head attention layer with 4 heads and key dimension of 16\n",
    "- Layer normalization for stabilizing the network\n",
    "- MLP block with two dense layers (128 -> 64 dimensions)\n",
    "- Classification head for final prediction\n",
    "\n",
    "**Forward Pass Flow**\n",
    "\n",
    "1. **Patch Embedding**\n",
    "- Input image is divided into patches and embedded using PatchEmbed layer\n",
    "- Output shape: (batch_size, num_patches, embed_dim)[1]\n",
    "\n",
    "2. **Position Information**\n",
    "- Adds learnable position embeddings to provide spatial context\n",
    "- Position embeddings are added directly to patch embeddings[1]\n",
    "\n",
    "3. **Transformer Processing**\n",
    "- Self-attention mechanism processes patch relationships\n",
    "- Layer normalization and residual connection combine attention output\n",
    "- MLP further processes the features with residual connection\n",
    "\n",
    "4. **Classification**\n",
    "- Global average pooling across patches (reduce_mean)\n",
    "- Final dense layer produces class logits\n",
    "\n",
    "This implementation simplifies the original ViT architecture by:\n",
    "- Using a single transformer block instead of multiple layers\n",
    "- Omitting the class token\n",
    "- Simplifying the MLP structure\n",
    "- Using mean pooling instead of the class token for final classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up the data pipeline using the CIFAR-10 dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess CIFAR-10\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Create training and validation splits\n",
    "train_size = int(0.8 * len(x_train[:1000]))  # Use 80% for training\n",
    "\n",
    "# Split the data\n",
    "train_x = x_train[:train_size]\n",
    "train_y = y_train[:train_size]\n",
    "val_x = x_train[train_size:1000]\n",
    "val_y = y_train[train_size:1000]\n",
    "\n",
    "# Create datasets\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((val_x, val_y))\n",
    "\n",
    "# Configure datasets\n",
    "train_ds = train_ds.shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we can configure and train the Vision Transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and compile model\n",
    "model = SimpleViT()\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Train model with validation data\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vision Transformers represent a powerful alternative to traditional convolutional neural networks for image classification tasks. Their ability to process images as sequences of patches and capture global dependencies makes them particularly effective for many computer vision applications. While they typically require more data and computational resources than CNNs for training from scratch, they can achieve state-of-the-art performance when properly trained.\n",
    "\n",
    "Key takeaways from this implementation:\n",
    "- Vision Transformers can effectively process images by treating them as sequences of patches\n",
    "- The architecture maintains the core components of the original Transformer while adapting them for image data\n",
    "- The model can be effectively trained on standard image classification datasets\n",
    "- Attention mechanisms provide interpretability through visualization of learned patterns"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

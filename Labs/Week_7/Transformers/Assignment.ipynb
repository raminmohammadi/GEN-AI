{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers Assignment (Graded): Emotion Classification using Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Welcome to your programming assignment on Transformer Architecture! You will build a Deep Learning Model with Transformer Architecture to accurately predict the emotion expressed in a given text input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop a text-based emotion classification model using a Transformer architecture. The model should accurately predict the emotion expressed in a given text input, choosing from a predefined set of emotion categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project utilizes the \"emotion\" dataset from the Hugging Face datasets library. This dataset contains text samples labeled with various emotions. The emotions are represented as numerical labels, which correspond to specific emotional states (e.g., joy, sadness, anger, etc.).\n",
    "\n",
    "Key characteristics of the dataset:\n",
    "\n",
    "- 20,000 examples (split configuration)\n",
    "- 6 emotion classes: sadness, joy, love, anger, fear, surprise\n",
    "- Text data: English Twitter messages\n",
    "- Labels: Numerical (0-5) corresponding to emotions\n",
    "\n",
    "The emotion dataset contains English Twitter messages labeled with six basic emotions. It has two configurations:\n",
    "\n",
    "1. Split configuration:\n",
    "   - 16,000 training examples\n",
    "   - 2,000 validation examples\n",
    "   - 2,000 test examples\n",
    "\n",
    "2. Unsplit configuration:\n",
    "   - 416,809 examples in a single train split\n",
    "\n",
    "Each data instance consists of:\n",
    "- \"text\": a string feature containing the Twitter message\n",
    "- \"label\": a numerical label (0-5) corresponding to the emotion\n",
    "\n",
    "For more information, refer to this link: [Emotion Dataset](https://huggingface.co/datasets/dair-ai/emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the scope of this Assignment, we'll be using the Split configuration of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Data Preprocessing**\n",
    "   - Load the emotion dataset using the Hugging Face datasets library\n",
    "   - Preprocess the text data (lowercase conversion, punctuation removal)\n",
    "   - Tokenize and pad the text sequences\n",
    "   - Handle class imbalance using oversampling techniques\n",
    "\n",
    "2. **Transformer Architecture Implementation**\n",
    "   - Implement key components of the Transformer architecture:\n",
    "     - Positional Encoding\n",
    "     - Scaled Dot-Product Attention\n",
    "     - Multi-Head Attention\n",
    "     - Transformer Block\n",
    "\n",
    "3. **Model Development**\n",
    "   - Create a Transformer-based model for emotion classification\n",
    "   - Implement the model using TensorFlow and Keras\n",
    "   - Configure model hyperparameters (number of layers, dimensions, etc.)\n",
    "\n",
    "4. **Model Training**\n",
    "   - Compile the model with appropriate loss function and optimizer\n",
    "   - Train the model on the preprocessed dataset\n",
    "   - Implement validation during training\n",
    "\n",
    "5. **Model Evaluation**\n",
    "   - Evaluate the trained model on the test set\n",
    "   - Calculate and report test accuracy\n",
    "\n",
    "6. **Emotion Prediction**\n",
    "   - Develop a function to predict emotions for new text inputs\n",
    "   - Demonstrate the use of the prediction function with a sample text\n",
    "\n",
    "7. **Analysis and Reporting**\n",
    "   - Analyze the model's performance and discuss results\n",
    "   - Suggest potential improvements or extensions to the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Only write code when you see any of the below prompts,\n",
    "\n",
    "    ```\n",
    "    # YOUR CODE GOES HERE\n",
    "    # YOUR CODE ENDS HERE\n",
    "    # TODO\n",
    "    ```\n",
    "\n",
    "- Do not modify any other section of the code unless tated otherwise in the comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from helpers.methods import load_emotion_dataset, detect_and_set_device\n",
    "from tests.test_methods import test_preprocess_data, test_positional_encoding, test_multi_head_attention, test_transformer_block, test_transformer_model, test_train_model, test_evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into training and testing sets\n",
    "train_df, test_df = load_emotion_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Let's get to know about our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape of the dataset: Testing and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display the shape of the data\n",
    "train_df_shape = \n",
    "test_df_shape = \n",
    "\n",
    "print(f\"Train data shape: {train_df_shape}\")\n",
    "print(f\"Test data shape: {test_df_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Distribution in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO: Plot the class distribution of the training data\n",
    "class_counts_before = \n",
    "\n",
    "# Create a figure \n",
    "fig, ax1 = plt.subplots(1, 1, figsize=(20, 8))\n",
    "\n",
    "# Plot class distribution before oversampling\n",
    "sns.barplot(x=list(class_counts_before.keys()), y=list(class_counts_before.values()), ax=ax1)\n",
    "ax1.set_title('Class Distribution Before Oversampling')\n",
    "ax1.set_xlabel('Emotion')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.tick_params(axis='x', rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**write your interpretation of the visualization here:**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Hints:**\n",
    "\n",
    "Complete the preprocess_data method.\n",
    "\n",
    "- Implement the preprocess_text function to convert text to lowercase and remove non-alphanumeric characters.\n",
    "\n",
    "- Use the Tokenizer class from Keras to tokenize the text data. Set the vocabulary size and out-of-vocabulary token.\n",
    "\n",
    "- Apply padding to the tokenized sequences using pad_sequences to ensure uniform length. Set the maximum sequence length.\n",
    "\n",
    "- Convert the labels (y_train and y_test) to numpy arrays for compatibility with the model.\n",
    "\n",
    "- Use RandomOverSampler from imbalanced-learn to handle class imbalance in the training data.\n",
    "\n",
    "- Extract unique emotion labels from the training data and calculate the number of classes.\n",
    "\n",
    "- Ensure all preprocessed data and metadata (X_train, y_train, X_test, y_test, tokenizer, emotion_labels, num_classes) are returned by the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text_series):\n",
    "    # TODO: Preprocess text data by converting to lowercase and removing special characters.\n",
    "    # The function takes a pandas Series containing text data and performs the following:\n",
    "    # 1. Converts all text to lowercase\n",
    "    # 2. Removes all non-alphanumeric characters except spaces using regex\n",
    "    \n",
    "\n",
    "    return text_series\n",
    "\n",
    "def preprocess_data(train_data, test_data, max_vocab_size=10000, max_seq_length=50):\n",
    "    # TODO: Preprocess text\n",
    "\n",
    "\n",
    "    # TODO: Tokenize the text data\n",
    "\n",
    "    \n",
    "    # TODO: Fit the tokenizer on the training data\n",
    "    \n",
    "    \n",
    "    # TODO: Convert text data to sequences and pad sequences\n",
    "\n",
    "    \n",
    "    # TODO: Get the labels\n",
    "\n",
    "    \n",
    "    # TODO: Perform oversampling on the training data\n",
    "\n",
    "    \n",
    "    # TODO: Resample the training data\n",
    "\n",
    "\n",
    "    # TODO: Get the emotion labels and number of classes\n",
    "    emotion_labels = \n",
    "    num_classes =\n",
    "    \n",
    "    \n",
    "    \n",
    "    #---------------- Do not change the code below ----------------#\n",
    "    # Run the test\n",
    "    test_preprocess_data(X_train, X_test, y_train, y_test, tokenizer, emotion_labels, num_classes)\n",
    "    return X_train_resampled, y_train_resampled, X_test, y_test, tokenizer, emotion_labels, num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Hints:**\n",
    "\n",
    "Implement the PositionalEncoding class:\n",
    "\n",
    "- In `__init__`, initialize the positional encoding matrix using `positional_encoding`.\n",
    "\n",
    "- Implement `get_angles` to calculate the angles for positional encoding using the provided formula.\n",
    "\n",
    "- In `positional_encoding`:\n",
    "  - Generate angle radians using `get_angles`.\n",
    "  - Apply sine to even indices and cosine to odd indices of the angle radians.\n",
    "  - Concatenate sine and cosine values to create the positional encoding.\n",
    "  - Add an extra dimension at the start and cast to float32.\n",
    "\n",
    "- In `call`, add the positional encoding to the input tensor. Ensure the encoding is sliced to match the input sequence length.\n",
    "\n",
    "- Use TensorFlow operations (tf.range, tf.newaxis, tf.concat, etc.) for efficient computation.\n",
    "\n",
    "- Ensure the output shape matches the input shape with the added positional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        # TODO: Compute the angles for the positional encoding\n",
    "        angles =         \n",
    "        # Return the angles * position\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        # TODO: Compute the angles for the positional encoding\n",
    "        angle_rads = self.get_angles(\n",
    "            # YOUR CODE BEGINS HERE\n",
    "            # pass the position, i and d_model\n",
    "            \n",
    "            # YOUR CODE ENDS HERE\n",
    "        )\n",
    "        \n",
    "        # TODO: Apply sin to even indices in the array\n",
    "        \n",
    "        \n",
    "        # TODO: Apply cos to odd indices in the array\n",
    "        \n",
    "        \n",
    "        # TODO: Concatenate the sines and cosines\n",
    "        \n",
    "        \n",
    "        # TODO: Add batch dimension\n",
    "        \n",
    "        \n",
    "        # Return the positional encoding\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "    \n",
    "#---------------- Do not change the code below ----------------#\n",
    "test_positional_encoding(PositionalEncoding(100, 512))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot Product Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Hints:**\n",
    "\n",
    "Implement the scaled_dot_product_attention function:\n",
    "\n",
    "- Use tf.matmul for matrix multiplication between query and key (with transpose_b=True).\n",
    "\n",
    "- Calculate the depth as the last dimension of the key tensor, casting to float32.\n",
    "\n",
    "- Scale the dot product by dividing by the square root of the depth.\n",
    "\n",
    "- If a mask is provided, add a large negative value (-1e9) to the masked positions in the logits.\n",
    "\n",
    "- Apply softmax to the logits to obtain attention weights.\n",
    "\n",
    "- Compute the final output by matrix multiplication of attention weights and values.\n",
    "\n",
    "- Ensure the function handles batched inputs correctly.\n",
    "\n",
    "- Consider adding an optional parameter for temperature in the softmax calculation for potential experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    # TODO: Compute the dot product of the query and key\n",
    "    \n",
    "    # TODO: Get the depth of the key\n",
    "    \n",
    "    # TODO: Scale the dot product\n",
    "    \n",
    "    # Add the mask to zero out padding tokens\n",
    "    if mask is not None:\n",
    "        \n",
    "        \n",
    "    # TODO: Compute the attention weights\n",
    "    \n",
    "    # TODO: Compute the output which is the weighted sum of the value\n",
    "    \n",
    "    # Return the output\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Hints:**\n",
    "\n",
    "Implement the MultiHeadAttention class:\n",
    "\n",
    "- In `__init__`:\n",
    "  - Ensure that `d_model` is divisible by `num_heads`.\n",
    "  - Calculate `depth` as `d_model // num_heads`.\n",
    "  - Initialize Dense layers for query, key, value projections (wq, wk, wv) and output projection (dense).\n",
    "\n",
    "- Implement `split_heads`:\n",
    "  - Reshape the input tensor to separate the last dimension into `num_heads` and `depth`.\n",
    "  - Transpose the resulting tensor to bring the `num_heads` dimension to the correct position.\n",
    "\n",
    "- In `call`:\n",
    "  - Apply the query, key, and value projections using the respective Dense layers.\n",
    "  - Use `split_heads` to reshape q, k, and v for multi-head processing.\n",
    "  - Call `scaled_dot_product_attention` with the reshaped q, k, v, and the provided mask.\n",
    "  - Transpose and reshape the attention output to combine the heads.\n",
    "  - Apply the output projection using the `dense` layer.\n",
    "\n",
    "- Ensure the method handles batched inputs correctly.\n",
    "\n",
    "- Consider adding a dropout layer after the output projection for regularization.\n",
    "\n",
    "- You may want to add an optional `training` parameter to `call` for potential dropout usage.\n",
    "\n",
    "- Ensure the output shape is correct: (batch_size, seq_len, d_model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Ensure d_model is divisible by the number of heads\n",
    "        assert d_model % self.num_heads == 0\n",
    "        \n",
    "        # TODO: Depth of the model\n",
    "        self.depth = \n",
    "        \n",
    "        # TODO: Dense layers for query, key, and value\n",
    "        self.wq = \n",
    "        self.wk = \n",
    "        self.wv = \n",
    "        \n",
    "        # Dense layer for the output\n",
    "        self.dense = \n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        # TODO: Split the last dimension into (num_heads, depth)\n",
    "        x = \n",
    "        \n",
    "        # Return the transposed result\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        # TODO: Get the batch size\n",
    "        \n",
    "        # TODO: Pass the query, key, and value through the dense layers\n",
    "\n",
    "        \n",
    "        # TODO: Split the heads for the query, key, and value\n",
    "\n",
    "        \n",
    "        # TODO: Compute the scaled dot product attention\n",
    "        \n",
    "        \n",
    "        # TODO: Transpose the result to (batch_size, seq_len_q, num_heads, depth)\n",
    "        \n",
    "        \n",
    "        # TODO: Concatenate the heads to get the concatenated attention result\n",
    "        \n",
    "        \n",
    "        # TODO: Pass the con through the dense layer to get the final output\n",
    "\n",
    "        return output\n",
    "    \n",
    "    \n",
    "#---------------- Do not change the code below ----------------#\n",
    "mha = MultiHeadAttention(d_model=128, num_heads=8)\n",
    "print(test_multi_head_attention(mha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Block Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Hints:**\n",
    "\n",
    "Implement the TransformerBlock class:\n",
    "\n",
    "- In `__init__`:\n",
    "  - Initialize the MultiHeadAttention layer with `d_model` and `num_heads`.\n",
    "  - Create a feed-forward network (ffn) using a Sequential model with two Dense layers:\n",
    "    - First layer: `dff` units with ReLU activation\n",
    "    - Second layer: `d_model` units with no activation\n",
    "  - Initialize two LayerNormalization layers with a small epsilon (e.g., 1e-6).\n",
    "  - Create two Dropout layers with the specified rate.\n",
    "\n",
    "- In `call`:\n",
    "  - Apply multi-head attention:\n",
    "    - Pass the input `x` as query, key, and value to the `mha` layer.\n",
    "    - Apply dropout to the attention output.\n",
    "    - Add the input `x` to the attention output (residual connection).\n",
    "    - Apply layer normalization.\n",
    "  \n",
    "  - Apply the feed-forward network:\n",
    "    - Pass the output from the previous step through the `ffn`.\n",
    "    - Apply dropout to the ffn output.\n",
    "    - Add the input from the previous step (residual connection).\n",
    "    - Apply layer normalization.\n",
    "\n",
    "  - Ensure the `training` parameter is used correctly for dropout layers.\n",
    "  \n",
    "  - Use the `mask` parameter in the multi-head attention call if provided.\n",
    "\n",
    "- Consider adding additional parameters or methods for more flexibility, such as:\n",
    "  - A parameter to control whether to use layer normalization before or after each sub-layer (pre-norm vs. post-norm).\n",
    "  - A method to get attention weights for visualization or analysis.\n",
    "\n",
    "- Ensure the output shape matches the input shape: (batch_size, seq_len, d_model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            # TODO: Define First dense layer: dff units, relu activation\n",
    "            \n",
    "            # TODO: Define Second dense layer: d_model units\n",
    "        ])\n",
    "        \n",
    "        # TODO: Define Layer Normalization layers: epsilon=1e-6\n",
    "        self.layernorm1 = \n",
    "        self.layernorm2 =\n",
    "        \n",
    "        # TODO: Define Dropout layers: rate\n",
    "        self.dropout1 = \n",
    "        self.dropout2 = \n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        # TODO: Call the MultiHeadAttention layer with x, x, x, mask\n",
    "        attn_output = \n",
    "        # TODO: Apply dropout on the output and add it to the input\n",
    "        attn_output = \n",
    "        \n",
    "        # TODO: Apply layer normalization and add the output to the input\n",
    "        out1 = \n",
    "        \n",
    "        # TODO: Call the feed forward network with out1\n",
    "        ffn_output = \n",
    "        \n",
    "        # TODO: Apply dropout on the output and add it to the input\n",
    "        ffn_output = \n",
    "        \n",
    "        # TODO: Apply layer normalization and add the output to the input\n",
    "        out2 = \n",
    "\n",
    "        return out2\n",
    "\n",
    "\n",
    "#---------------- Do not change the code below ----------------#\n",
    "transformer_block = TransformerBlock(d_model=128, num_heads=8, dff=512, rate=0.1)\n",
    "test_transformer_block(transformer_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Hints:**\n",
    "\n",
    "Implement the TransformerModel class:\n",
    "\n",
    "- In `__init__`:\n",
    "  - Initialize an Embedding layer with `input_vocab_size` and `d_model`.\n",
    "  - Create a PositionalEncoding layer with `maximum_position_encoding` and `d_model`.\n",
    "  - Create a list of `TransformerBlock` instances based on `num_layers`.\n",
    "  - Initialize a Dropout layer with the specified `rate`.\n",
    "  - Create the final Dense layer with `num_classes` units and softmax activation.\n",
    "\n",
    "- In `call`:\n",
    "  - Apply the embedding layer to the input `x`.\n",
    "  - Scale the embeddings by multiplying with sqrt(d_model).\n",
    "  - Add positional encoding to the scaled embeddings.\n",
    "  - Apply dropout to the result.\n",
    "  - Pass the output through each transformer block sequentially.\n",
    "  - Apply global average pooling to the output of the last transformer block.\n",
    "  - Pass the pooled output through the final dense layer.\n",
    "\n",
    "- Additional considerations:\n",
    "  - Implement input masking if needed for variable-length sequences.\n",
    "  - Add a parameter to control whether positional encoding is trainable.\n",
    "  - Consider adding residual connections or layer normalization after the embedding layer.\n",
    "  - Implement attention visualization functionality if desired.\n",
    "\n",
    "- Ensure the model can handle batched inputs correctly.\n",
    "\n",
    "- The output shape should be (batch_size, num_classes), representing the probability distribution over emotion classes.\n",
    "\n",
    "- Consider adding methods for:\n",
    "  - Getting intermediate representations for analysis.\n",
    "  - Custom training loops if needed.\n",
    "  - Model summary or configuration printing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, num_classes, rate=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # TODO: Define the embedding layer with input_vocab_size and d_model\n",
    "        self.embedding = \n",
    "        \n",
    "        # TODO: Define the positional encoding layer with maximum_position_encoding and d_model\n",
    "        self.pos_encoding = \n",
    "        \n",
    "        # TODO: Define the transformer blocks with num_layers, d_model, num_heads, dff, rate\n",
    "        self.transformer_blocks = \n",
    "        \n",
    "        # TODO: Define the dropout layer with rate \n",
    "        self.dropout = \n",
    "        \n",
    "        # TODO: Define the final dense layer with num_classes and softmax activation\n",
    "        self.final_layer = \n",
    "        \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        # TODO: Get the sequence length\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        # TODO: Pass the input through the embedding layer\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # TODO: Scale the embedding by sqrt(d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        \n",
    "        # TODO: Add the positional encoding to the embedding\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # TODO: Apply dropout to the embedding\n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        # TODO: Pass the embedding through the transformer blocks \n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = \n",
    "            \n",
    "        # TODO: Apply global average pooling to the output\n",
    "        x = \n",
    "        \n",
    "        # TODO: Pass the output through the final dense layer\n",
    "        output = \n",
    "\n",
    "        return output\n",
    "    \n",
    "# Test parameters\n",
    "transformer_model = TransformerModel(num_layers=4, d_model=128, num_heads=8, dff=512, input_vocab_size=10000, maximum_position_encoding=10000, num_classes=6, rate=0.1)\n",
    "test_transformer_model(transformer_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Hints:**\n",
    "\n",
    "Implement the train_transformer_model function:\n",
    "\n",
    "- Define the function with parameters for training data, model architecture, and training configuration.\n",
    "\n",
    "- Calculate the input vocabulary size based on the tokenizer's word index.\n",
    "\n",
    "- Create an instance of the TransformerModel using the provided architecture parameters.\n",
    "\n",
    "- Compile the model:\n",
    "  - Use the Adam optimizer.\n",
    "  - Set the loss function to sparse categorical crossentropy.\n",
    "  - Include accuracy as a metric.\n",
    "\n",
    "- Train the model using the fit method:\n",
    "  - Pass the training data (X_train and y_train).\n",
    "  - Set the batch size, number of epochs, and validation split.\n",
    "  - Capture the training history.\n",
    "\n",
    "- Consider adding optional parameters for:\n",
    "  - Learning rate for the optimizer.\n",
    "  - Custom callbacks (e.g., early stopping, model checkpointing).\n",
    "  - Class weights for imbalanced datasets.\n",
    "\n",
    "- Implement error handling for invalid input parameters.\n",
    "\n",
    "- Add logging or print statements to track training progress.\n",
    "\n",
    "- Return both the trained model and the training history for further analysis.\n",
    "\n",
    "- Ensure the function is flexible enough to handle different dataset sizes and model configurations.\n",
    "\n",
    "- Consider adding a parameter for specifying the validation data explicitly, instead of using validation_split.\n",
    "\n",
    "- Optionally, add functionality to save the model after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer_model(X_train, y_train, tokenizer, num_classes, MAX_SEQ_LENGTH, num_layers=4, d_model=128, num_heads=8, dff=512, epochs=10, batch_size=32, validation_split=0.1):\n",
    "    device = detect_and_set_device()\n",
    "    \n",
    "    # TODO: Get the input vocab size from the tokenizer\n",
    "    input_vocab_size = \n",
    "    \n",
    "    # Create the transformer model with the specified parameters\n",
    "    model = \n",
    "    \n",
    "    # TODO: Compile the model with an Adam optimizer and sparse categorical crossentropy loss and accuracy metric\n",
    "\n",
    "    \n",
    "    # Train the model on the training data with the specified batch size and number of epochs \n",
    "    with tf.device('/' + device + ':0'):\n",
    "        history = model.fit(\n",
    "            # YOUR CODE BEGINS HERE\n",
    "            # pass the training data and labels\n",
    "            \n",
    "            # YOUR CODE ENDS HERE\n",
    "        )\n",
    "        \n",
    "    #---------------- Do not change the code below ----------------#\n",
    "    test_train_model(history)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    # TODO: Evaluate the model on the test data and get the loss and accuracy\n",
    "    test_loss, test_accuracy = \n",
    "    print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    \n",
    "    #---------------- Do not change the code below ----------------#\n",
    "    test_evaluate_model(test_loss, test_accuracy)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Driver Code to run the built pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------- Do not change the code below ----------------#\n",
    "def emotion_classification_pipeline(train_df, test_df):\n",
    "    max_vocab_size=10000\n",
    "    max_seq_length=50\n",
    "    num_layers=4\n",
    "    d_model=128\n",
    "    num_heads=8\n",
    "    dff=512\n",
    "    epochs=10\n",
    "    batch_size=32\n",
    "    validation_split=0.1\n",
    "\n",
    "    # Preprocess data\n",
    "    X_train, y_train, X_test, y_test, tokenizer, emotion_labels, num_classes = preprocess_data(\n",
    "        train_df, test_df, max_vocab_size, max_seq_length\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    model, history = train_transformer_model(\n",
    "        X_train, y_train, tokenizer, num_classes, max_seq_length,\n",
    "        num_layers, d_model, num_heads, dff, epochs, batch_size, validation_split\n",
    "    )\n",
    "\n",
    "    # Evaluate model\n",
    "    test_loss, test_accuracy = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "    return model, history, test_loss, test_accuracy, emotion_labels, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    model, history, test_loss, test_accuracy, emotion_labels, tokenizer = emotion_classification_pipeline(train_df, test_df)\n",
    "    print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test loss: {test_loss:.4f}\")\n",
    "    print(f\"Emotion labels: {emotion_labels}\")\n",
    "    print(f\"Tokenizer: {tokenizer}\")\n",
    "    print(f\"Model summary: {model.summary()}\")\n",
    "    print(f\"History: {history}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

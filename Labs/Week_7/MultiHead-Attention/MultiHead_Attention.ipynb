{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpPSpgoL1jES"
      },
      "source": [
        "<center>\n",
        "    <h1>MultiHead-Attention Mechanism</h1>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6ttICWQjzTd"
      },
      "source": [
        "# Brief Recap\n",
        "**Multi-head attention** mechanism was introduced in 2017 in the paper [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762) by Ashish Vaswani, Noam Shazeer and others.\n",
        "Multihead Attention is a key concept in the field of deep learning, specifically in the realm of Transformer architectures. It allows the model to focus on different parts of the input sequence simultaneously, enhancing its ability to capture relationships between elements in the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnPJ3z4Hj21S"
      },
      "source": [
        "# Architecture\n",
        "\n",
        "<img src='assets/multi-head-attention.png' width=400>\n",
        "\n",
        "The Multihead Attention mechanism involves several key steps:\n",
        "\n",
        "1. **Input Embeddings**: The input is first embedded into a higher-dimensional space.\n",
        "2. **Linear Projections**: The embeddings are linearly projected into three matricesâ€”Query (Q), Key (K), and Value (V).\n",
        "3. **Scaled Dot-Product Attention**:\n",
        "   - Calculate the dot product between Q and K to determine how much focus each element should receive.\n",
        "   - Scale the dot product by the square root of the dimension of Q/K.\n",
        "   - Apply a softmax function to get the attention scores.\n",
        "   - Multiply the attention scores with V to get the weighted output.\n",
        "4. **Concatenation of Heads**: Each attention head performs the above steps independently, and their outputs are concatenated.\n",
        "5. **Final Linear Projection**: The concatenated output is passed through a final linear layer to get the desired dimensions.\n",
        "\n",
        "### Formula\n",
        "The attention score can be computed as:\n",
        "$$ \\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V $$\n",
        "\n",
        "Where:\n",
        "- \\( Q \\): Query Matrix\n",
        "- \\( K \\): Key Matrix\n",
        "- \\( V \\): Value Matrix\n",
        "- \\( d_k \\): Dimension of the Key vector\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF5m_FV8yLCi"
      },
      "source": [
        "# Applications\n",
        "Multihead Attention has diverse applications across several domains:\n",
        "\n",
        "### 1. Natural Language Processing (NLP)\n",
        "   - **Machine Translation**: Allows models to translate languages by focusing on relevant words.\n",
        "   - **Text Summarization**: Extracts key information from text for summarization.\n",
        "\n",
        "### 2. Computer Vision\n",
        "   - **Image Classification**: Attention mechanisms help models focus on important regions in images.\n",
        "   - **Object Detection**: Enhances the ability of models to detect objects in images by focusing on different regions.\n",
        "\n",
        "### 3. Time Series Analysis\n",
        "   - **Forecasting**: Helps models capture long-term dependencies in sequential data for accurate predictions.\n",
        "\n",
        "### 4. Reinforcement Learning\n",
        "   - **Policy Optimization**: Attention mechanisms can be used to improve the focus on key states in an environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9dogSW87zsK"
      },
      "source": [
        "# Implementation of MultiHead-Attention using TensorFlow\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pD6wQ1F_GUUC"
      },
      "source": [
        "## Approach 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4fetQUv6NMT"
      },
      "source": [
        "### Defining the Attention Mechanism\n",
        "\n",
        "**Step 1: Calculate the Dot Product**\n",
        "\n",
        "```shell\n",
        "matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "```\n",
        "\n",
        " This calculates the similarity or relevance between the query and each key in the input sequence. The resulting matrix `matmul_qk` stores these similarity scores, which are then used in subsequent steps of the attention mechanism to determine the attention weights.\n",
        "\n",
        "**Step 2: Scale the Scores**<br>\n",
        "```shell\n",
        "depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "scaled_attention_logits = matmul_qk / tf.math.sqrt(depth)\n",
        "```\n",
        "To stabilize gradients during training, we scale the scores by $\\sqrt{dk}$ (the dimension of keys). This helps to keep the values of the dot product in a range where the softmax function is effective.\n",
        "\n",
        "**Step 3: Apply the Mask (Optional)**<br>\n",
        "The mask is used to prevent certain tokens from being attended to, such as padding tokens. If a position should not be attended to, it is set to a large negative value, making its softmax output near zero.\n",
        "\n",
        "**Step 4: Apply Softmax**<br>\n",
        "\n",
        "```shell\n",
        "attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "```\n",
        "\n",
        "The softmax function converts these scores into probabilities, highlighting the most relevant positions for each query. The output is a probability distribution over the sequence length.\n",
        "\n",
        "**Step 5: Weighted Sum**<br>\n",
        "```shell\n",
        "output = tf.matmul(attention_weights, value)\n",
        "```\n",
        "\n",
        "Finally, we multiply these probabilities (attention weights) by the value matrix to get a weighted sum. This weighted sum is the attention output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP6wVJRwApvA"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "    \"\"\"\n",
        "    Calculates the attention scores between the query, key, and value matrices.\n",
        "\n",
        "    Args:\n",
        "        query: The query matrix (Q).\n",
        "        key: The key matrix (K).\n",
        "        value: The value matrix (V).\n",
        "        mask: Optional mask to apply to the attention scores.\n",
        "\n",
        "    Returns:\n",
        "        output: The result of applying attention scores to the value matrix.\n",
        "        attention_weights: The attention scores after applying softmax.\n",
        "    \"\"\"\n",
        "    # Step 1: Calculate the dot product between Q and K^T (transpose of K)\n",
        "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "    # Step 2: Scale the dot products by the square root of the key dimension\n",
        "    depth = tf.cast(tf.shape(key)[-1], tf.float32)  # Dimension of the key vector\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(depth)\n",
        "\n",
        "    # Step 3: Apply the mask (if provided) to the scaled attention logits\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    # Step 4: Apply softmax to get the attention weights\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "    # Step 5: Multiply the attention weights with the value matrix to get the output\n",
        "    output = tf.matmul(attention_weights, value)\n",
        "\n",
        "    return output, attention_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTOOs2TaCjID"
      },
      "source": [
        "### Defining MultiHead-Attention Layer\n",
        "\n",
        "The `MultiHeadAttention` class extends the scaled dot-product attention mechanism by applying multiple attention heads.\n",
        "\n",
        "**1. Initialization**:\n",
        "\n",
        "* `d_model` is the dimension of the input embeddings.\n",
        "* `num_heads` is the number of attention heads.\n",
        "* We ensure d_model is divisible by `num_heads` because each head will handle a fraction of the total dimension (`depth = d_model // num_heads`).\n",
        "\n",
        "**2. Linear Projections:**\n",
        "\n",
        "* Dense layers (`self.wq`, `self.wk`, `self.wv`) are used to project the input tensors into Query (Q), Key (K), and Value (V) matrices.\n",
        "* These projections are then split into multiple heads.\n",
        "\n",
        "**3. Splitting into Heads:**\n",
        "\n",
        "* The `split_heads()` method reshapes the projected Q, K, and V matrices into smaller matrices for each head.\n",
        "* Each head has a shape of (`batch_size`, `num_heads`, `seq_len`, `depth`), allowing parallel computation of attention for different parts of the input.\n",
        "\n",
        "**4. Scaled Dot-Product Attention:**\n",
        "\n",
        "* The `scaled_dot_product_attention()` function is applied to each head to compute the attention output.\n",
        "* The result is transposed back to match the original input shape.\n",
        "\n",
        "**5. Concatenation:**\n",
        "\n",
        "* The outputs of all heads are concatenated together.\n",
        "* This step aggregates the different perspectives each attention head has learned.\n",
        "\n",
        "**6. Final Linear Layer:**\n",
        "\n",
        "* A dense layer (`self.dense`) projects the concatenated output back to the desired output dimensions (`d_model`).\n",
        "* This allows the multi-head output to be in the same format as the original input, making it compatible for subsequent layers in a Transformer model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsNdo-ibAphs"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class MultiHeadAttention(Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Ensure that the d_model is divisible by num_heads\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        self.depth = d_model // num_heads\n",
        "\n",
        "        # Dense layers to project inputs to queries, keys, and values\n",
        "        self.wq = Dense(d_model)\n",
        "        self.wk = Dense(d_model)\n",
        "        self.wv = Dense(d_model)\n",
        "\n",
        "        # Dense layer to combine all the attention heads\n",
        "        self.dense = Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"\n",
        "        Splits the input tensor into multiple heads.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "            batch_size: Size of the input batch.\n",
        "\n",
        "        Returns:\n",
        "            x: Transposed tensor of shape (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        # Step 1: Linear projections for Q, K, V\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # Step 2: Split into multiple heads\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len, depth)\n",
        "\n",
        "        # Step 3: Apply scaled dot-product attention\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        # Step 4: Concatenate the heads\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "\n",
        "        # Step 5: Final linear layer\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        return output, attention_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN46iJuU4PSn"
      },
      "source": [
        "## Example Usage\n",
        "Let's implement a simplified example of a multihead attention layer using dummy data, and then visualize the attention weights to see how the model distributes attention across the sequence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEbX-pIe45w2"
      },
      "source": [
        "### 1. Create Dummy Data\n",
        "\n",
        "Generate some simple sequences as input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7SyX23L5FaL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Layer\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n",
        "dummy_data = np.random.rand(1, 10, 64)  # (batch_size, seq_len, d_model)\n",
        "dummy_data = tf.convert_to_tensor(dummy_data, dtype=tf.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Jxrsu0i5Lkh"
      },
      "source": [
        "### 2. Instantiate and Apply the Multi-Head Attention layer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vnil6MtT5bB7"
      },
      "outputs": [],
      "source": [
        "num_heads = 4\n",
        "d_model = 64\n",
        "mha = MultiHeadAttention(d_model, num_heads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAFETSsh5cBx"
      },
      "source": [
        "### 3. Forward Pass\n",
        "\n",
        "Pass the dummy data through the attention layer to get the attention weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXcQVn-W5oax"
      },
      "outputs": [],
      "source": [
        "# Query, Key, and Value are all the same in this dummy example\n",
        "output, attention_weights = mha(dummy_data, dummy_data, dummy_data, mask=None)\n",
        "\n",
        "# Attention weights shape: (batch_size, num_heads, seq_len, seq_len)\n",
        "print(f\"Attention weights shape: {attention_weights.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGsvgtKN3nq2"
      },
      "source": [
        "### Visualizing the Attention Weights:\n",
        "* The output of each subplot shows how each attention head focuses on different positions in the sequence.\n",
        "* Each heatmap displays the attention distribution, with brighter colors indicating higher attention weights.\n",
        "* Rows represent query positions, and columns represent key positions.\n",
        "\n",
        "This visualization helps us understand how attention heads distribute focus across different parts of the input sequence, making multihead attention a powerful mechanism for capturing relationships in sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Klxb5oueBKSf"
      },
      "outputs": [],
      "source": [
        "# Visualize the attention weights for each head\n",
        "fig, axes = plt.subplots(1, num_heads, figsize=(20, 5))\n",
        "for i in range(num_heads):\n",
        "    ax = axes[i]\n",
        "    ax.matshow(attention_weights[0, i].numpy(), cmap='viridis')\n",
        "    ax.set_title(f'Head {i + 1}')\n",
        "    ax.set_xlabel('Key Positions')\n",
        "    ax.set_ylabel('Query Positions')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0epVr3Xd55-4"
      },
      "source": [
        "## Approach 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi05i33I6oI6"
      },
      "source": [
        "Although, the above approach gives you more control in designing your attention mechanism, there is an alternate and more simpler way to do that.\n",
        "\n",
        "```python\n",
        "tf.keras.layers.MultiHeadAttention(\n",
        "    num_heads,\n",
        "    key_dim,\n",
        "    value_dim=None,\n",
        "    dropout=0.0,\n",
        "    use_bias=True)\n",
        "```\n",
        "\n",
        "**Arguments**\n",
        "* **num_heads:** (Integer) The number of attention heads. This determines how many parallel attention mechanisms will be used.\n",
        "* **key_dim:** (Integer) The dimension of the key, query, and value vectors. This is the size of each attention head.\n",
        "* **value_dim:** (Optional Integer) The dimension of the value vectors. If not specified, it defaults to key_dim.\n",
        "* **dropout:** (Optional Float) The dropout rate to apply to the attention weights.\n",
        "* **use_bias:** (Optional Boolean) Whether to use bias vectors in the dense layers. Defaults to True.\n",
        "\n",
        "**Call arguments**\n",
        "* **query:** Tensor of shape (batch_size, seq_len_q, depth) representing the query.\n",
        "* **value:** Tensor of shape (batch_size, seq_len_v, depth) representing the value.\n",
        "* **key:** Tensor of shape (batch_size, seq_len_k, depth) representing the key.\n",
        "* **attention_mask:** (Optional) A mask to apply to the attention weights.\n",
        "* **return_attention_scores:** (Optional Boolean) Whether to return the attention scores along with the output. Defaults to False.\n",
        "\n",
        "**Outputs:**\n",
        "\n",
        "* **output:** Tensor of shape (batch_size, seq_len_q, value_dim) representing the output of the multi-head attention.\n",
        "* **attention_scores:** (Optional) If return_attention_scores=True, a tensor of shape (batch_size, num_heads, seq_len_q, seq_len_k) representing the attention weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQKGXXwIChJK"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D68PoJiN8IHA"
      },
      "source": [
        "# SMS Spam Classification using Muli-Head Attention\n",
        "\n",
        "<img src='https://miro.medium.com/v2/resize:fit:1400/1*oLszsXod-dV2c1NBqa8k7w.jpeg' width=500>\n",
        "\n",
        "SMS spam classification is a crucial task to filter unwanted and potentially harmful messages. Traditional methods often rely on hand-crafted features and classical machine learning algorithms. In this approach, we will leverage the power of Multi-Head Attention to build a robust SMS spam classifier.\n",
        "\n",
        "## Dataset Description\n",
        "\n",
        "The [SMS Spam Collection](https://www.kaggle.com/datasets/thedevastator/sms-spam-collection-a-more-diverse-dataset) dataset contains 5574 English, real, and non-encoded messages. The SMS messages are thought-provoking and eye-catching. The dataset is useful for mobile phone spam research.\n",
        "\n",
        "**Key characteristics:**\n",
        "\n",
        "* **Content:** Upto 5000 records of original text messages classified as spam or not spam.\n",
        "* **Size:** The full dataset contains around 5574 english, real and non-encoded messages.\n",
        "* **Format:** The data is stored as text, and the code uses a tokenizer to convert words into numerical representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmR3V7HDxY6M"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization, Embedding, Dense, GlobalAveragePooling1D, MultiHeadAttention, Dropout, LayerNormalization, Input, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md---3l29gnr"
      },
      "source": [
        "### 1. Load and Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8r2X77ZxY4C"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('sms_spam.csv')\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['sms'], data['label'], test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9XT0e549ljx"
      },
      "source": [
        "### 2. text Vectorization\n",
        "\n",
        "We'll use `TextVectorization` to convert the text into integer tokens for the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYhz7uAMxY1Z"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "vocab_size = 10000\n",
        "sequence_length = 100\n",
        "embedding_dim = 64\n",
        "\n",
        "# Text vectorization layer\n",
        "vectorizer = TextVectorization(max_tokens=vocab_size, output_mode='int', output_sequence_length=sequence_length)\n",
        "vectorizer.adapt(X_train.values)\n",
        "\n",
        "# Example vectorization\n",
        "X_train_vectorized = vectorizer(X_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh5LadvE9tuR"
      },
      "source": [
        "### 3. Build the model with Muli-Head Attention\n",
        "\n",
        "We'll be utilizing the `tf.keras.layers.MultiHeadAttention` module to build our model. We'll be assigning **4 attention heads**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxCPISgSxYzn"
      },
      "outputs": [],
      "source": [
        "def create_model(vocab_size, embedding_dim, sequence_length, num_heads):\n",
        "    # Input layer\n",
        "    inputs = Input(shape=(sequence_length,), dtype=tf.int32)\n",
        "\n",
        "    # Embedding layer\n",
        "    x = Embedding(vocab_size, embedding_dim)(inputs)\n",
        "\n",
        "    # Multihead Attention\n",
        "    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)(x, x)\n",
        "    attention_output = Dropout(0.1)(attention_output)\n",
        "    attention_output = LayerNormalization(epsilon=1e-6)(attention_output + x)  # Residual connection\n",
        "\n",
        "    # Global pooling to reduce sequence length\n",
        "    pooled_output = GlobalAveragePooling1D()(attention_output)\n",
        "\n",
        "    # Dense layers for classification\n",
        "    dense_output = Dense(64, activation='relu')(pooled_output)\n",
        "    dense_output = Dropout(0.1)(dense_output)\n",
        "    outputs = Dense(1, activation='sigmoid')(dense_output)\n",
        "\n",
        "    # Create the model\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Create the model\n",
        "num_heads = 4\n",
        "model = create_model(vocab_size=vocab_size, embedding_dim=embedding_dim, sequence_length=sequence_length, num_heads=num_heads)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_33AkCq8-ED7"
      },
      "source": [
        "**Key Features**\n",
        "\n",
        "* **Input Layer:** It takes an input tensor representing the vectorized SMS messages, with a shape of (sequence_length,).\n",
        "Embedding Layer: It utilizes an Embedding layer to convert the integer-encoded words into dense vector representations. This allows the model to capture semantic relationships between words.\n",
        "* **Multi-Head Attention:** The core of the model is the MultiHeadAttention layer, which enables it to attend to different parts of the input sequence simultaneously. It helps in capturing complex relationships and dependencies within the text.\n",
        "* **Residual Connection and Layer Normalization:** A residual connection is implemented by adding the attention output to the original input, and then applying layer normalization. This aids in training deeper networks by mitigating the vanishing gradient problem and improving information flow.\n",
        "* **Global Average Pooling:** A GlobalAveragePooling1D layer is used to reduce the sequence length to a fixed-size vector, capturing the overall essence of the message.\n",
        "* **Dense Layers:** Two dense layers are employed for classification, with the first using a ReLU activation and the second using a sigmoid activation for binary classification (spam or ham).\n",
        "* **Dropout:** Dropout layers are added after the attention and dense layers to prevent overfitting and improve generalization performance.\n",
        "* **Output:** The model outputs a single value between 0 and 1, representing the probability of the input message being spam.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DcwnbJg-dyj"
      },
      "source": [
        "### 4. Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DALj9X0xYwk"
      },
      "outputs": [],
      "source": [
        "# Prepare the training data\n",
        "X_train_vectorized = vectorizer(X_train)\n",
        "X_test_vectorized = vectorizer(X_test)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_vectorized, y_train, epochs=5, batch_size=32, validation_data=(X_test_vectorized, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0n6SuftvFc2"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test_vectorized, y_test)\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o16GgFW7-gv9"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCsZwx7yvFal"
      },
      "outputs": [],
      "source": [
        "# Predict on new samples\n",
        "new_messages = [\n",
        "    \"Congratulations! You've won a free ticket to Bahamas!\",\n",
        "    \"Hey, can we reschedule our meeting to tomorrow?\"\n",
        "]\n",
        "new_messages_vectorized = vectorizer(new_messages)\n",
        "predictions = model.predict(new_messages_vectorized)\n",
        "\n",
        "# Print the predictions\n",
        "for i, message in enumerate(new_messages):\n",
        "    print(f\"Message: '{message}' - Prediction: {'Spam' if predictions[i] > 0.5 else 'Not Spam'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rEu94Fm99xM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yprAXTuVLPfB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23M_FcMcycNF"
      },
      "source": [
        "<center>\n",
        "    <h1>Long Short Term Memory (LSTM)</h1>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueuU1vBNycNG"
      },
      "source": [
        "# Brief Recap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Long Short-Term Memory (LSTM) networks are a specialized type of recurrent neural network (RNN) designed to capture long-term dependencies in sequential data. LSTMs were introduced by Hochreiter & Schmidhuber in 1997 to address the vanishing gradient problem faced by traditional RNNs. In other words, they have memory of the past inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LSTM Architecture Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSgau3NdD4CU"
      },
      "source": [
        "LSTM is a type of Recurrent Neural Network (RNN) architecture. RNNs are designed to handle sequential data by processing each input based on the previous inputs. In other words, they have memory of the past inputs.\n",
        "\n",
        "LSTM takes this concept further by introducing a cell state that can keep information over long periods of time. This cell state is controlled by three gates: the input gate, the forget gate, and the output gate. These gates determine what information to keep or discard from the cell state.\n",
        "\n",
        "- **Input Gate**: Decides which values from the input should be used to update the cell state.\n",
        "- **Forget Gate**: Determines what information should be discarded from the cell state.\n",
        "- **Output Gate**: Controls what information from the cell state should be used as output.\n",
        "- **Cell State**: The memory of the network that runs through the entire chain, with only minor linear interactions.\n",
        "\n",
        "<center>\n",
        "    <img src=\"static/image1.jpeg\" alt=\"LSTM Architecture\" style=\"width:50%;\">\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIKfyzKWD69s"
      },
      "source": [
        "## Advantages of LSTMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Long-term Dependencies**: LSTMs can effectively capture and learn from long-range dependencies in sequential data.\n",
        "- **Mitigates Vanishing Gradient**: The gating mechanism helps prevent the vanishing gradient problem common in traditional RNNs.\n",
        "- **Selective Memory**: LSTMs can selectively remember or forget information, making them more efficient at processing sequences.\n",
        "- **Versatility**: Effective for various sequential data tasks like natural language processing, speech recognition, and time series forecasting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL3OGNSpycNG"
      },
      "source": [
        "# Implementing LSTM with TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TensorFlow provides an easy way to implement LSTM layers using the `tf.keras.layers.LSTM` class. Here's an overview of the key components:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- `units`: Number of LSTM units (neurons) in the layer.\n",
        "- `activation`: Activation function for the output. Default is `'tanh'`.\n",
        "- `recurrent_activation`: Activation function for the recurrent step. Default is `'sigmoid'`.\n",
        "- `return_sequences`: If `True`, returns the full sequence of outputs for each sample. If `False`, returns only the last output.\n",
        "- `return_state`: If `True`, returns the last state in addition to the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Some other important arguments are:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- `dropout`: Float between 0 and 1. Fraction of units to drop for the linear transformation of the inputs.\n",
        "- `recurrent_dropout`: Float between 0 and 1. Fraction of units to drop for the linear transformation of the recurrent state.\n",
        "\n",
        "For more detailed information, refer to the [TensorFlow documentation on LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rv6hkRJQycNG",
        "outputId": "2d511782-4b2d-4485-c6e6-3e7a1c95441c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Create input data (batch_size, timesteps, features)\n",
        "inputs = np.random.random((32, 10, 8))\n",
        "\n",
        "# Example 1: Basic LSTM\n",
        "lstm = keras.layers.LSTM(4)\n",
        "output = lstm(inputs)\n",
        "print(\"Basic LSTM output shape:\", output.shape)\n",
        "\n",
        "# Example 2: LSTM with return_sequences and return_state\n",
        "lstm = keras.layers.LSTM(4, return_sequences=True, return_state=True)\n",
        "whole_seq_output, final_memory_state, final_carry_state = lstm(inputs)\n",
        "print(\"Whole sequence output shape:\", whole_seq_output.shape)\n",
        "print(\"Final memory state shape:\", final_memory_state.shape)\n",
        "print(\"Final carry state shape:\", final_carry_state.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_eF6ku_ycNH"
      },
      "source": [
        "## Basic LSTM (Example 1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\n",
        "output = lstm(inputs)\n",
        "print(\"Basic LSTM output shape:\", output.shape)\n",
        "```\n",
        "\n",
        "**Output:** `(32, 4)`\n",
        "\n",
        "**Explanation:** By default, LSTM returns only the last output for each sample in the batch. Here, we get 32 samples (batch size), each with 4 features (number of LSTM units)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "827WfLn0DLdl"
      },
      "source": [
        "## LSTM with **return_sequences=True** and **return_state=True** (Example 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\n",
        "whole_seq_output, final_memory_state, final_carry_state = lstm(inputs)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### a. Whole Sequence Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\n",
        "print(\"Whole sequence output shape:\", whole_seq_output.shape)\n",
        "```\n",
        "\n",
        "**Output:** `(32, 10, 4)`\n",
        "\n",
        "**Explanation:** With `return_sequences=True`, we get outputs for all timesteps. This results in 32 samples, each with 10 timesteps, and 4 features per timestep."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### b. Final Memory State"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "```python\n",
        "print(\"Final memory state shape:\", final_memory_state.shape)\n",
        "```\n",
        "\n",
        "**Output:** `(32, 4)`\n",
        "\n",
        "**Explanation:** This is the final memory state (hidden state) of the LSTM for each sample in the batch. It has 32 samples, each with 4 features (matching the number of LSTM units)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### c. Final Carry State"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\n",
        "print(\"Final carry state shape:\", final_carry_state.shape)\n",
        "```\n",
        "\n",
        "**Output:** `(32, 4)`\n",
        "\n",
        "**Explanation:** This is the final carry state (cell state) of the LSTM for each sample. Like the memory state, it has 32 samples with 4 features each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- When `return_sequences=False` (default), you get only the last output of the sequence.\n",
        "- When `return_sequences=True`, you get outputs for all timesteps.\n",
        "- When `return_state=True`, you get the final memory (hidden) state and carry (cell) state in addition to the outputs.\n",
        "- The number of features in the output always matches the number of LSTM units specified."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preparing Data for LSTM in TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preparing data for Long Short-Term Memory (LSTM) networks in TensorFlow involves several key steps to ensure that your sequential data is in the right format for training and evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_J2hvqr_LkG"
      },
      "source": [
        "## Sequence Padding and Truncation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When working with sequence data, it's common to have sequences of varying lengths. To feed this data into an LSTM, we need to ensure all sequences have the same length. This is achieved through padding (adding values to shorter sequences) and truncation (cutting off longer sequences).\n",
        "\n",
        "TensorFlow provides the `tf.keras.preprocessing.sequence.pad_sequences` function for this purpose:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZbX3bBd-t4n",
        "outputId": "78b38fb4-9722-4ec1-95d2-086531080e1c"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Example sequences\n",
        "sequences = [[1, 2, 3], [4, 5], [6, 7, 8, 9]]\n",
        "\n",
        "# Pad sequences\n",
        "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    sequences,\n",
        "    maxlen=4,  # Maximum sequence length\n",
        "    padding='post',  # Add padding at the end of sequences\n",
        "    truncating='post',  # Truncate from the end if sequence is too long\n",
        "    value=0  # Padding value\n",
        ")\n",
        "\n",
        "print(padded_sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bl6FXEmycNH"
      },
      "source": [
        "\n",
        "### Key Parameters of pad_sequences:\n",
        "\n",
        "- `maxlen`: Maximum sequence length. Sequences longer than this will be truncated.\n",
        "- `padding`: 'pre' or 'post' (default). Add padding either before or after each sequence.\n",
        "- `truncating`: 'pre' or 'post' (default). Remove values from sequences longer than `maxlen`, either from the beginning or end of the sequence.\n",
        "- `value`: Float or String, padding value.\n",
        "\n",
        "For more detailed information, refer to the official TensorFlow documentation:\n",
        "- [tf.keras.preprocessing.sequence.pad_sequences](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Khc0w0Lj_e4b"
      },
      "source": [
        "## Creating Input Sequences and Labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For time series or sequential data, we often need to create input sequences and corresponding labels. Here's an example of how to do this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce3Q1cMj_jpM",
        "outputId": "6dcadf66-603f-481e-db08-033482f60b0a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        seq = data[i:i+seq_length]\n",
        "        label = data[i+seq_length]\n",
        "        sequences.append(seq)\n",
        "        labels.append(label)\n",
        "    return np.array(sequences), np.array(labels)\n",
        "\n",
        "# Example data\n",
        "data = np.arange(100)\n",
        "seq_length = 10\n",
        "\n",
        "X, y = create_sequences(data, seq_length)\n",
        "print(\"Input shape:\", X.shape)\n",
        "print(\"Label shape:\", y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfszkWJ1_sJ9"
      },
      "source": [
        "This function creates sequences of length `seq_length` and uses the next value as the label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpPlKFenAQef"
      },
      "source": [
        "## Splitting Data into Training and Validation Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To evaluate our model's performance, we need to split our data into training and validation sets. We can use the `train_test_split` function from scikit-learn for this purpose:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oaEuChNATWJ",
        "outputId": "85545224-9d25-4921-da6a-bd0af1bb84d8"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming X and y are your input sequences and labels\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Validation set shape:\", X_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M9ANhJwAYH9"
      },
      "source": [
        "This code splits the data into 80% training and 20% validation sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDKoZDvFF17A"
      },
      "source": [
        "## Handling Variable Length Sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If your sequences have variable lengths and you want to avoid padding, you can use TensorFlow's `tf.data.Dataset` with `padded_batch`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vd8O25nfGBnw",
        "outputId": "8dd23518-d9f0-4a05-95b7-669cee68266a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Generate Sample Data\n",
        "texts = [\n",
        "    \"I love programming in Python\",\n",
        "    \"Deep learning is fascinating\",\n",
        "    \"Machine learning can be fun\",\n",
        "    \"I enjoy solving complex problems\",\n",
        "    \"Data science combines multiple disciplines\",\n",
        "    \"Artificial intelligence is the future\",\n",
        "    \"Natural language processing is amazing\",\n",
        "    \"I like building models with TensorFlow\",\n",
        "    \"Understanding algorithms is essential\",\n",
        "    \"Python is great for data analysis\"\n",
        "]\n",
        "\n",
        "# Sample labels (for example, 0-2 for different categories)\n",
        "labels = [0, 1, 1, 0, 2, 2, 1, 0, 1, 2]\n",
        "\n",
        "# 2. Tokenization\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences_encoded = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# 3. Padding Sequences\n",
        "max_length = 10\n",
        "X = pad_sequences(sequences_encoded, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# 4. Preparing Labels\n",
        "num_classes = len(np.unique(labels))\n",
        "y = tf.keras.utils.to_categorical(labels, num_classes=num_classes)\n",
        "\n",
        "# Convert y to float32 to avoid type issues\n",
        "y = y.astype(np.float32)\n",
        "\n",
        "# 5. Splitting the Dataset\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 6. Creating the Dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "\n",
        "# 7. Padded Batching\n",
        "train_padded_dataset = train_dataset.padded_batch(\n",
        "    batch_size=2,\n",
        "    padded_shapes=([None], [num_classes]),  # X: variable length, y: fixed length for one-hot\n",
        "    padding_values=(0, 0.0)  # Use 0 for X and 0.0 for y (float)\n",
        ")\n",
        "\n",
        "val_padded_dataset = val_dataset.padded_batch(\n",
        "    batch_size=2,\n",
        "    padded_shapes=([None], [num_classes]),\n",
        "    padding_values=(0, 0.0)\n",
        ")\n",
        "\n",
        "# Sample output to verify\n",
        "for batch in train_padded_dataset.take(1):\n",
        "    print(batch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74uKzaFOGXf-"
      },
      "source": [
        "This approach allows TensorFlow to handle padding dynamically during training, which can be more memory-efficient for variable-length sequences.\n",
        "\n",
        "For more detailed information, refer to the official TensorFlow documentation:\n",
        "[tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB1Ps-2uGqzM"
      },
      "source": [
        "## Understanding Masking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When using padding, it's important to understand masking. Masking allows the model to ignore padded values during computation. TensorFlow's LSTM layers automatically support masking when the input layer uses the `mask_zero=True` parameter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "L7pa0rpwIK7b",
        "outputId": "e02f546a-e2b8-4724-807a-06b838559ea8"
      },
      "outputs": [],
      "source": [
        "# 8. Define Vocabulary Size\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Add 1 for padding\n",
        "\n",
        "# 9. Build the LSTM Model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=64, mask_zero=True),\n",
        "    tf.keras.layers.LSTM(64),\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')  # Softmax for multi-class classification\n",
        "])\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1nwJM87HoC9"
      },
      "source": [
        "This ensures that the LSTM layer ignores padded values during its computations.\n",
        "In the output, the model summary shows that the model is unbuilt as of nof, hence the 'Param' and 'Output Shape' are not available in the summary. Let's go ahead and compile the model to visualize the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr-6XGZBIYAe"
      },
      "source": [
        "# Training the LSTM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1oKUWGKJBuC"
      },
      "source": [
        "## Setting up Training Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before training your LSTM model, you need to set up various training parameters. These parameters will affect the learning process and the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "iH9HchqBKgIm"
      },
      "outputs": [],
      "source": [
        "# Define Hyperparameters\n",
        "batch_size = 2\n",
        "learning_rate = 0.001\n",
        "epochs = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve7EkWS1JBkx"
      },
      "source": [
        "### Key Parameters:\n",
        "\n",
        "- **`batch_size`**: Number of samples processed before the model is updated. A smaller batch size often leads to noisier gradient estimates, while a larger batch size provides a more stable estimate but may require more memory.\n",
        "\n",
        "- **`epochs`**: Number of complete passes through the training dataset. More epochs allow the model to learn better, but too many can lead to overfitting, where the model performs well on training data but poorly on unseen data.\n",
        "\n",
        "- **`learning_rate`**: Step size at each iteration while moving toward a minimum of the loss function. A small learning rate ensures steady convergence but may take longer to train, while a large learning rate can speed up training but risks overshooting the minimum.\n",
        "\n",
        "- **`optimizer`**: Algorithm used to update model parameters based on the computed gradients. Common optimizers include Adam, SGD, and RMSprop, each with its own characteristics and suitable use cases.\n",
        "\n",
        "- **`loss function`**: A measure of how well the model's predictions match the actual labels. For classification tasks, common choices include categorical cross-entropy and binary cross-entropy.\n",
        "\n",
        "- **`metrics`**: Quantitative measures used to evaluate the performance of the model during training and validation. Common metrics for classification include accuracy, precision, recall, and F1-score.\n",
        "\n",
        "- **`num_classes`**: The total number of distinct categories in the target variable. This is crucial for setting up the output layer and choosing the appropriate loss function.\n",
        "\n",
        "- **`max_length`**: The maximum length of input sequences after padding. This ensures uniform input sizes for the model and can impact the model's ability to learn patterns effectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "9o1XCVZrMhJn"
      },
      "outputs": [],
      "source": [
        "# Compile the Model\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',  # Loss function for multi-class classification\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),  # Adam optimizer\n",
        "    metrics=['accuracy']  # Metric to monitor\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivYBGjJyMaMv"
      },
      "source": [
        "## Fitting the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Important Arguments:**\n",
        "- `X_train`, `y_train`: Training data and labels.\n",
        "- `batch_size`: Number of samples per gradient update.\n",
        "- `epochs`: Number of epochs to train the model.\n",
        "- `validation_data`: Data on which to evaluate the loss and any model metrics at the end of each epoch.\n",
        "- `callbacks`: List of callbacks to apply during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtSO8EVhLW9l",
        "outputId": "47bd4b12-7bd9-4f04-b252-f4bf3a22a322"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_padded_dataset, validation_data=val_padded_dataset, epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "11xQDLe4LaRM",
        "outputId": "33202fa2-a073-4cf3-8a27-cc0785cac586"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rLL5-HH8cXT"
      },
      "source": [
        "# Visualizing Training History"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After training, you can plot the training history to get a clear picture of how your model performed over time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "WJ58zN1JM-Cu",
        "outputId": "4e5ee14f-51bb-42f9-bbee-6c9940220e1e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Plot training & validation accuracy\n",
        "plt.subplot(121)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Plot training & validation loss\n",
        "plt.subplot(122)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7VytPlmNTyv"
      },
      "source": [
        "### Other Metrics that can be used to visualize the performance of the model\n",
        "\n",
        "- **Accuracy**: Gives an overall idea of model performance but may be misleading for imbalanced datasets.\n",
        "- **Confusion Matrix**: Helps visualize the model's performance for each class.\n",
        "- **Precision**: Indicates the proportion of positive identifications that were actually correct.\n",
        "- **Recall**: Indicates the proportion of actual positives that were identified correctly.\n",
        "- **F1-score**: The harmonic mean of precision and recall, providing a single score that balances both metrics.\n",
        "- **ROC Curve and AUC**: Useful for binary classification, showing the model's ability to distinguish between classes at various thresholds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Let's Build a Real world Project to understand the concept of LSTMs better"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stock Price Prediction using LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this project, you will develop a Long Short-Term Memory (LSTM) neural network model to predict stock prices. Given historical stock price data for a company, your task is to build an LSTM model that can forecast the closing price of the stock for the next day. The model will use the past 60 days of stock prices to predict the price on the 61st day."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll be using historical stock price data for Apple Inc. (AAPL) from Yahoo Finance. The dataset includes daily stock information such as opening price, closing price, high, low, volume, and adjusted close."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How to Download the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll use the historical stock price data for Apple Inc. (AAPL) from Yahoo Finance. \n",
        "\n",
        "You can manually download the dataset from this link to visualize it:\n",
        "https://query1.finance.yahoo.com/v7/finance/download/AAPL?period1=1420070400&period2=1631577600&interval=1d&events=history&includeAdjustedClose=true\n",
        "\n",
        "Save this CSV file as \"AAPL_stock_data.csv\" in your working directory.\n",
        "\n",
        "---\n",
        "\n",
        "**In this walkthrough notebook,** the dataset will be automatically downloaded using the `yfinance` library when you run the provided code. Here's how it works:\n",
        "\n",
        "1. The code defines a function `download_stock_data()` that uses `yfinance` to fetch the data.\n",
        "2. It downloads approximately 4 years of historical data (1500 days) up to the current date.\n",
        "3. The data is then saved as a CSV file named \"AAPL_stock_data.csv\" in your working directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import yfinance as yf\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Create a directory for storing the dataset if it does not exist\n",
        "data_dir = 'data'\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "\n",
        "# Automatic dataset download and save\n",
        "def download_stock_data(symbol, start_date, end_date, filename):\n",
        "    stock = yf.Ticker(symbol)\n",
        "    data = stock.history(start=start_date, end=end_date)\n",
        "    data.to_csv(filename)\n",
        "    print(f\"Dataset downloaded and saved as {filename}\")\n",
        "\n",
        "# Set the parameters for data download\n",
        "symbol = \"AAPL\"\n",
        "end_date = datetime.now().strftime('%Y-%m-%d')\n",
        "start_date = (datetime.now() - timedelta(days=1500)).strftime('%Y-%m-%d')\n",
        "filename = os.path.join(data_dir, 'AAPL_stock_data.csv')\n",
        "\n",
        "# Download and save the dataset\n",
        "download_stock_data(symbol, start_date, end_date, filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We import the yfinance library and datetime for date handling.\n",
        "\n",
        "* The download_stock_data() function takes four parameters:\n",
        "    1. symbol: The stock symbol (e.g., \"AAPL\" for Apple)\n",
        "    2. start_date: The start date for the historical data\n",
        "    3. end_date: The end date for the historical data\n",
        "    4. filename: The name of the file to save the data\n",
        "\n",
        "* We set the parameters for data download:\n",
        "    1. end_date is set to the current date\n",
        "    2. start_date is set to 1500 days before the current date (about 4 years of data)\n",
        "    3. filename is set to \"AAPL_stock_data.csv\"\n",
        "\n",
        "* We call the download_stock_data() function to download and save the dataset before proceeding with the rest of the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Necessary Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import yfinance as yf\n",
        "from datetime import datetime, timedelta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Load the downloaded CSV file into a pandas DataFrame.\n",
        "- Convert the 'Date' column to datetime and sort the data chronologically.\n",
        "- Select the 'Close' price for our prediction task.\n",
        "- Normalize the data using MinMaxScaler to scale values between 0 and 1.\n",
        "- Create sequences of 60 days for input and 1 day for output.\n",
        "- Split the data into training (80%) and testing (20%) sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('data/AAPL_stock_data.csv')\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df = df.sort_values('Date')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select the 'Close' price for our prediction task\n",
        "data = df['Close'].values.reshape(-1, 1)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "data_normalized = scaler.fit_transform(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sequences of 60 days for input and 1 day for output\n",
        "def create_sequences(data, seq_length):\n",
        "    X = []\n",
        "    y = []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:(i + seq_length)])\n",
        "        y.append(data[i + seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 60\n",
        "X, y = create_sequences(data_normalized, seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Building the LSTM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Create a Sequential model using TensorFlow/Keras.\n",
        "- Add two LSTM layers with 50 units each and ReLU activation.\n",
        "- Add a Dense layer for the output.\n",
        "- Compile the model using Adam optimizer and Mean Squared Error loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.LSTM(50, activation='relu', input_shape=(seq_length, 1), return_sequences=True),\n",
        "    tf.keras.layers.LSTM(50, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Fit the model on the training data.\n",
        "- Use 50 epochs and a batch size of 32.\n",
        "- Set aside 10% of the training data for validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training the Model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Making Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Use the trained model to make predictions on the test set.\n",
        "- Inverse transform the predictions and actual values to get real stock prices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Inverse transform the predictions and actual values\n",
        "y_pred = scaler.inverse_transform(y_pred)\n",
        "y_test = scaler.inverse_transform(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Evaluating Model Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Calculate the Root Mean Squared Error (RMSE) between predicted and actual prices.\n",
        "- Plot the actual vs. predicted stock prices over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluating Model Performance\n",
        "\n",
        "mse = np.mean((y_pred - y_test)**2)\n",
        "rmse = np.sqrt(mse)\n",
        "print(f\"Root Mean Squared Error: {rmse}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotting the results\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df['Date'].values[-len(y_test):], y_test, label='Actual Price')\n",
        "plt.plot(df['Date'].values[-len(y_pred):], y_pred, label='Predicted Price')\n",
        "plt.title('AAPL Stock Price Prediction')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Stock Price')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview of the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The graph shows the performance of an LSTM (Long Short-Term Memory) model in predicting Apple Inc. (AAPL) stock prices over a period from December 2023 to October 2024. The blue line represents the actual stock price, while the orange line shows the model's predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Observations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "1. **Overall Trend Capture**: The LSTM model successfully captures the general trend of AAPL stock prices. It follows both upward and downward movements in the stock price over time.\n",
        "\n",
        "2. **Lag in Predictions**: There's a noticeable lag in the predicted prices compared to actual prices. The orange line (predictions) often trails slightly behind the blue line (actual prices), especially during sharp price changes.\n",
        "\n",
        "3. **Accuracy in Stable Periods**: The model performs well during periods of relative price stability, closely matching the actual price movements.\n",
        "\n",
        "4. **Difficulty with Sudden Changes**: The LSTM struggles to predict sudden, sharp price movements accurately. This is evident in areas where there are quick spikes or drops in the actual price.\n",
        "\n",
        "5. **Long-term Trend Alignment**: Despite short-term discrepancies, the model aligns well with long-term price trends, indicating good capture of underlying patterns.\n",
        "\n",
        "6. **Price Range**: The model accurately predicts the general price range, with predictions mostly falling within the same range as actual prices (approximately $165 to $235)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Strengths of the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Good at capturing overall market trends\n",
        "- Accurate in predicting the general price range\n",
        "- Performs well during periods of stable or gradual price changes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Limitations of the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Struggles with sudden, sharp price movements\n",
        "- Exhibits a lag in responding to price changes\n",
        "- May underestimate extreme highs and lows"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

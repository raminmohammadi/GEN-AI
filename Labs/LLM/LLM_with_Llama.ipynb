{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Large Language Models (LLMs)</h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief Recap of Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Large Language Models are advanced AI systems trained on vast amounts of text data to understand and generate human-like text. \n",
    "\n",
    "- These models have revolutionized natural language processing by demonstrating remarkable capabilities in tasks like text generation, translation, and question-answering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Characteristics**\n",
    "\n",
    "- Pre-trained on massive text datasets\n",
    "\n",
    "- Utilize deep learning and transformer architecture\n",
    "- Can perform various language tasks without task-specific training\n",
    "- Generate contextually relevant and coherent responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evolution and Impact**\n",
    "\n",
    "- The development of LLMs has transformed from simple rule-based systems to sophisticated neural networks capable of handling billions of parameters. \n",
    "\n",
    "- Modern LLMs like GPT-3 (175 billion parameters) and similar models demonstrate remarkable capabilities in tasks ranging from translation to creative writing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"static/image1.gif\" alt=\"LLMs\" style=\"width:50%;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Architecture Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The foundation of modern LLMs is built on the Transformer architecture, introduced in 2017's \"Attention is All You Need\" paper.\n",
    "\n",
    "- The architecture of modern LLMs is based on the transformer model, which processes text through several sophisticated mechanisms:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input Processing Layer**\n",
    "\n",
    "- Embedding Layer: Converts text tokens into numerical vectors\n",
    "\n",
    "- Positional Encoding: Adds sequence information to maintain word order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformer Block**\n",
    "\n",
    "- Self-Attention Mechanism: Weighs word importance and relationships\n",
    "\n",
    "- Multi-Head Attention: Processes multiple attention patterns simultaneously\n",
    "\n",
    "- Feed-Forward Networks: Applies additional transformations to processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output Processing**\n",
    "\n",
    "- Layer Normalization\n",
    "\n",
    "- Residual Connections\n",
    "- Final Linear Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Architectural Features**\n",
    "\n",
    "- **Parallel Processing**: Unlike traditional sequential models, Transformers process entire sequences simultaneously\n",
    "\n",
    "- **Attention Mechanism**: Enables understanding of long-range dependencies and contextual relationships between words\n",
    "\n",
    "- **Scalability**: Architecture supports models of varying sizes, from millions to billions of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture's efficiency comes from its ability to process text in parallel and maintain context through sophisticated attention mechanisms, making it particularly effective for large-scale language tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"static/image2.webp\" alt=\"LLMs\" style=\"width:50%;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Major Advantages of Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Natural Language Understanding\n",
    "    - Advanced comprehension of human language\n",
    "    - Context-aware responses\n",
    "    - Sentiment analysis capabilities\n",
    "    - Understanding of nuanced expressions\n",
    "\n",
    "- Task Versatility\n",
    "    - Document summarization\n",
    "    - Multi-language translation\n",
    "    - Code generation and debugging\n",
    "    - Content creation and editing\n",
    "    - Question-answering systems\n",
    "\n",
    "- Efficiency Improvements\n",
    "    - Automated task completion\n",
    "    - Rapid document processing\n",
    "    - Parallel processing capabilities\n",
    "    - Reduced manual intervention\n",
    "\n",
    "- Business Applications\n",
    "    - Data analysis and insights extraction\n",
    "    - Market research automation\n",
    "    - Customer service enhancement\n",
    "    - Content generation at scale\n",
    "\n",
    "- Learning Capabilities\n",
    "    - In-context learning\n",
    "    - Few-shot learning adaptation\n",
    "    - Continuous improvement\n",
    "    - Transfer learning across domains\n",
    "\n",
    "- Cost Benefits\n",
    "    - Reduced operational costs\n",
    "    - Automated workflows\n",
    "    - Faster time-to-market\n",
    "    - Scalable solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with LLMs (Llama Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: Using Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create a Hugging Face Account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't already have a Hugging Face account, go to the [Hugging Face website](https://huggingface.co/) and click on \"Sign Up\" to create an account. Follow the verification process, including verifying your email address."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Generate a User Access Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Log in to your Hugging Face account.\n",
    "- Navigate to your profile by clicking on your profile photo in the navigation bar.\n",
    "- Open the settings menu and select \"Access Tokens\" from the sidebar.\n",
    "- Click the \"New token\" button.\n",
    "- Provide a name for your token and select an appropriate role:\n",
    "  - `read`: Allows downloading models and datasets.\n",
    "  - `write`: Allows downloading and uploading models and datasets.\n",
    "- Click the \"Generate a token\" button.\n",
    "- Copy the generated token by clicking the \"Show\" and then \"Copy\" buttons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Request Access to Gated Models (If Necessary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For gated models like the Llama family models, you need to request access:\n",
    "- Go to the model page on the Hugging Face Hub.\n",
    "- You will be prompted to share your username and email address with the model authors.\n",
    "- Fill out any additional fields requested by the model authors.\n",
    "- Click \"Agree\" to send the access request.\n",
    "- If the model uses automatic approval, you will gain access immediately. Otherwise, you will need to wait for manual approval from the model authors.\n",
    "\n",
    "**Link to Llama Family Models:** https://huggingface.co/meta-llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Authenticate Using the Access Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can authenticate using the access token in several ways:\n",
    "\n",
    "- Using the `huggingface_hub` Library\n",
    "```python\n",
    "from huggingface_hub import login\n",
    "\n",
    "access_token = \"YOUR TOKEN\"\n",
    "login(token=access_token)\n",
    "```\n",
    "\n",
    "- Setting the Environment Variable\n",
    "You can set the `HF_TOKEN` environment variable:\n",
    "```bash\n",
    "export HF_TOKEN= YOUR_TOKEN\n",
    "```\n",
    "Or in your code:\n",
    "```python\n",
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"YOUR_TOKEN\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Initializing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `initialize_model()` function sets up a LLaMA 3 (Large Language Model) with specific configurations.\n",
    "\n",
    "**Core Components**\n",
    "\n",
    "```python\n",
    "def initialize_model():\n",
    "    try:\n",
    "        model = models.Llama3CausalLM.from_preset(\n",
    "            \"hf://meta-llama/Llama-3.2-1B-Instruct\",\n",
    "            dtype=\"bfloat16\"\n",
    "        )\n",
    "        model.compile(sampler=TopKSampler(k=50))\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing model: {e}\")\n",
    "        return None\n",
    "```\n",
    "\n",
    "**Parameters Breakdown**\n",
    "\n",
    "1. **Model Preset Path**\n",
    "   - Required parameter: `\"hf://meta-llama/Llama-3.2-1B-Instruct\"`\n",
    "   - Specifies the HuggingFace model path for LLaMA 3\n",
    "   - The \"1B\" indicates this is the 1 billion parameter version\n",
    "   - \"Instruct\" suggests it's fine-tuned for instruction-following tasks\n",
    "\n",
    "2. **Data Type (dtype)**\n",
    "   - Optional parameter: `dtype=\"bfloat16\"`\n",
    "   - Sets the numerical precision for model weights\n",
    "   - \"bfloat16\" is a memory-efficient format that maintains good numerical stability\n",
    "   - Alternative options could include \"float32\" or \"float16\"\n",
    "\n",
    "3. **Sampler Configuration**\n",
    "   - Optional parameter: `k=50` in `TopKSampler`\n",
    "   - Controls the randomness in text generation\n",
    "   - Selects the top 50 most likely next tokens during generation\n",
    "   - Higher k values increase diversity but may reduce quality\n",
    "   - Lower k values make outputs more deterministic\n",
    "\n",
    "**Error Handling**\n",
    "- The try-except block catches any initialization errors\n",
    "- Returns None if initialization fails\n",
    "- Prints the specific error message for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Generating the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `generate_text` function uses a pre-initialized LLaMA model to generate text responses based on a given prompt.\n",
    "\n",
    "```python\n",
    "def generate_text(model, prompt, max_length=200):\n",
    "    try:\n",
    "        response = model.generate(\n",
    "            prompt,\n",
    "            max_length=max_length\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error generating text: {e}\"\n",
    "```\n",
    "\n",
    "**Required Parameters**\n",
    "\n",
    "1. **model**\n",
    "   - The initialized LLaMA model instance\n",
    "   - Must be passed as the first argument\n",
    "   - Should be a valid model object returned from `initialize_model()`\n",
    "\n",
    "2. **prompt**\n",
    "   - The input text that guides the model's response\n",
    "   - Can be a question, instruction, or any text context\n",
    "   - Should be a string format\n",
    "\n",
    "**Optional Parameters**\n",
    "\n",
    "1. **max_length**\n",
    "   - Default value: 200 tokens\n",
    "   - Controls the maximum length of the generated response\n",
    "   - One token roughly equals 4 characters in English\n",
    "   - Can be adjusted based on needs:\n",
    "     - Lower values (50-100) for short responses\n",
    "     - Higher values (500+) for longer content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Working Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # Set TensorFlow as backend\n",
    "\n",
    "from keras_hub import models\n",
    "from keras_nlp.samplers import TopKSampler\n",
    "from huggingface_hub import login\n",
    "\n",
    "def setup_authentication():\n",
    "    try:\n",
    "        login(token=os.environ[\"HF_TOKEN\"])\n",
    "        print(\"Successfully authenticated with Hugging Face\")\n",
    "    except Exception as e:\n",
    "        print(f\"Authentication error: {e}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def initialize_model():\n",
    "    try:\n",
    "        # Initialize LLaMA 3 model using keras_hub\n",
    "        model = models.Llama3CausalLM.from_preset(\n",
    "            \"hf://meta-llama/Llama-3.2-1B-Instruct\",\n",
    "            dtype=\"bfloat16\"\n",
    "        )\n",
    "        \n",
    "        # Configure the sampler for text generation\n",
    "        model.compile(sampler=TopKSampler(k=50))  # You can adjust k value\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing model: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_text(model, prompt, max_length=200):\n",
    "    try:\n",
    "        # Generate response (tokenization is handled automatically)\n",
    "        response = model.generate(\n",
    "            prompt,\n",
    "            max_length=max_length\n",
    "        )\n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error generating text: {e}\"\n",
    "\n",
    "def main():\n",
    "    # Setup authentication\n",
    "    if not setup_authentication():\n",
    "        print(\"Failed to authenticate. Please check your HF token.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize model\n",
    "    model = initialize_model()\n",
    "    if model is not None:\n",
    "        prompt = \"Explain what is machine learning:\"\n",
    "        response = generate_text(model, prompt)\n",
    "        print(f\"Generated Response:\\n{response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your actual Hugging Face token\n",
    "    os.environ[\"HF_TOKEN\"] = \"FILL_YOUR_HUGGING_FACE_TOKEN\"  \n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: Using LlamaAPI client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Getting API Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Create an Account:**\n",
    "   - Visit https://www.llama-api.com\n",
    "   - Click on \"Log In\" → \"Sign up\"\n",
    "   - Complete the registration process\n",
    "\n",
    "2. **Join Waitlist and Get Approval:**\n",
    "   - After signup, you'll be added to the waitlist\n",
    "   - Wait for the approval email (usually takes a few days)\n",
    "   - Once approved, proceed to the next step\n",
    "\n",
    "3. **Obtain API Token:**\n",
    "   - Log in to your Llama API account\n",
    "   - Navigate to the \"API Token\" section\n",
    "   - Find your API token on the page\n",
    "   - Click the clipboard icon to copy your token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Setting Up Development Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **Install Required Package:**\n",
    "```bash\n",
    "pip install llamaapi\n",
    "```\n",
    "\n",
    "2. **Create a New Python File:**\n",
    "   - Create a new .py file in your preferred IDE\n",
    "   - Import required libraries at the top of your file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implementing the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Initialize the Client:**\n",
    "```python\n",
    "from llamaapi import LlamaAPI\n",
    "import json\n",
    "\n",
    "# Replace with your actual API token\n",
    "llama = LlamaAPI(\"your_api_token\")\n",
    "```\n",
    "\n",
    "2. **Create the API Request:**\n",
    "```python\n",
    "api_request = {\n",
    "    \"model\": \"llama3.1-70b\",    # Specify the model\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the concept of machine learning\"\n",
    "        }\n",
    "    ],\n",
    "    \"stream\": False,            # Disable streaming\n",
    "    \"max_length\": 500,          # Maximum response length\n",
    "    \"temperature\": 0.7          # Control randomness (0.0 to 1.0)\n",
    "}\n",
    "```\n",
    "\n",
    "3. **Execute the Request and Handle Response:**\n",
    "```python\n",
    "try:\n",
    "    response = llama.run(api_request)\n",
    "    print(json.dumps(response.json(), indent=2))\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Additional Configuration Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can customize your request with these parameters[3]:\n",
    "```python\n",
    "api_request = {\n",
    "    \"model\": \"llama3.1-70b\",\n",
    "    \"messages\": [...],\n",
    "    \"max_token\": 500,           # Maximum tokens to generate\n",
    "    \"temperature\": 0.1,         # Lower = more focused outputs\n",
    "    \"top_p\": 1.0,              # Nucleus sampling parameter\n",
    "    \"frequency_penalty\": 1.0,   # Reduce repetition\n",
    "    \"stream\": False            # Enable/disable streaming\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Running the Complete Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamaapi import LlamaAPI\n",
    "import json\n",
    "\n",
    "# Initialize client\n",
    "llama = LlamaAPI(\"your_api_token\")\n",
    "\n",
    "# Create request\n",
    "api_request = {\n",
    "    \"model\": \"llama3.1-70b\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Explain the concept of machine learning\"}\n",
    "    ],\n",
    "    \"stream\": False,\n",
    "    \"max_length\": 500,\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Get response\n",
    "    response = llama.run(api_request)\n",
    "    \n",
    "    # Print formatted response\n",
    "    print(json.dumps(response.json(), indent=2))\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Notes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Rate Limits:**\n",
    "   - Limited to 20 questions per 60-second window\n",
    "   - Exceeding this limit triggers a cooldown period\n",
    "\n",
    "2. **Common Errors:**\n",
    "   - 429: Too Many Requests (rate limit exceeded)\n",
    "   - 408: Request Timeout\n",
    "   - 401: Unauthorized (invalid credentials)\n",
    "\n",
    "3. **Model Selection:**\n",
    "   Available models include:\n",
    "   - Mixtral models: mixtral-8x22b-instruct, mixtral-8x7b-instruct\n",
    "   - Mistral models: mistral-7b-instruct\n",
    "   - Qwen models: Various sizes from 0.5B to 110B\n",
    "\n",
    "4. **Pricing:**\n",
    "   - Costs vary based on the model used\n",
    "   - Pricing is per 1 million tokens\n",
    "   - Separate costs for input and output tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

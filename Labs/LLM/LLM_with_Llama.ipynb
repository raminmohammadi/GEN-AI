{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Large Language Models (LLMs)</h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief Recap of Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Large Language Models are advanced AI systems trained on vast amounts of text data to understand and generate human-like text. \n",
    "\n",
    "- These models have revolutionized natural language processing by demonstrating remarkable capabilities in tasks like text generation, translation, and question-answering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Characteristics**\n",
    "\n",
    "- Pre-trained on massive text datasets\n",
    "\n",
    "- Utilize deep learning and transformer architecture\n",
    "- Can perform various language tasks without task-specific training\n",
    "- Generate contextually relevant and coherent responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evolution and Impact**\n",
    "\n",
    "- The development of LLMs has transformed from simple rule-based systems to sophisticated neural networks capable of handling billions of parameters. \n",
    "\n",
    "- Modern LLMs like GPT-3 (175 billion parameters) and similar models demonstrate remarkable capabilities in tasks ranging from translation to creative writing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"static/image1.gif\" alt=\"LLMs\" style=\"width:50%;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Architecture Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The foundation of modern LLMs is built on the Transformer architecture, introduced in 2017's \"Attention is All You Need\" paper.\n",
    "\n",
    "- The architecture of modern LLMs is based on the transformer model, which processes text through several sophisticated mechanisms:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input Processing Layer**\n",
    "\n",
    "- Embedding Layer: Converts text tokens into numerical vectors\n",
    "\n",
    "- Positional Encoding: Adds sequence information to maintain word order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformer Block**\n",
    "\n",
    "- Self-Attention Mechanism: Weighs word importance and relationships\n",
    "\n",
    "- Multi-Head Attention: Processes multiple attention patterns simultaneously\n",
    "\n",
    "- Feed-Forward Networks: Applies additional transformations to processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output Processing**\n",
    "\n",
    "- Layer Normalization\n",
    "\n",
    "- Residual Connections\n",
    "- Final Linear Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Architectural Features**\n",
    "\n",
    "- **Parallel Processing**: Unlike traditional sequential models, Transformers process entire sequences simultaneously\n",
    "\n",
    "- **Attention Mechanism**: Enables understanding of long-range dependencies and contextual relationships between words\n",
    "\n",
    "- **Scalability**: Architecture supports models of varying sizes, from millions to billions of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture's efficiency comes from its ability to process text in parallel and maintain context through sophisticated attention mechanisms, making it particularly effective for large-scale language tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"static/image2.webp\" alt=\"LLMs\" style=\"width:50%;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Major Advantages of Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Natural Language Understanding\n",
    "    - Advanced comprehension of human language\n",
    "    - Context-aware responses\n",
    "    - Sentiment analysis capabilities\n",
    "    - Understanding of nuanced expressions\n",
    "\n",
    "- Task Versatility\n",
    "    - Document summarization\n",
    "    - Multi-language translation\n",
    "    - Code generation and debugging\n",
    "    - Content creation and editing\n",
    "    - Question-answering systems\n",
    "\n",
    "- Efficiency Improvements\n",
    "    - Automated task completion\n",
    "    - Rapid document processing\n",
    "    - Parallel processing capabilities\n",
    "    - Reduced manual intervention\n",
    "\n",
    "- Business Applications\n",
    "    - Data analysis and insights extraction\n",
    "    - Market research automation\n",
    "    - Customer service enhancement\n",
    "    - Content generation at scale\n",
    "\n",
    "- Learning Capabilities\n",
    "    - In-context learning\n",
    "    - Few-shot learning adaptation\n",
    "    - Continuous improvement\n",
    "    - Transfer learning across domains\n",
    "\n",
    "- Cost Benefits\n",
    "    - Reduced operational costs\n",
    "    - Automated workflows\n",
    "    - Faster time-to-market\n",
    "    - Scalable solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with LLMs (Llama Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: Using Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create a Hugging Face Account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't already have a Hugging Face account, go to the [Hugging Face website](https://huggingface.co/) and click on \"Sign Up\" to create an account. Follow the verification process, including verifying your email address."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Generate a User Access Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Log in to your Hugging Face account.\n",
    "- Navigate to your profile by clicking on your profile photo in the navigation bar.\n",
    "- Open the settings menu and select \"Access Tokens\" from the sidebar.\n",
    "- Click the \"New token\" button.\n",
    "- Provide a name for your token and select an appropriate role:\n",
    "  - `read`: Allows downloading models and datasets.\n",
    "  - `write`: Allows downloading and uploading models and datasets.\n",
    "- Click the \"Generate a token\" button.\n",
    "- Copy the generated token by clicking the \"Show\" and then \"Copy\" buttons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Request Access to Gated Models (If Necessary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For gated models like the Llama family models, you need to request access:\n",
    "- Go to the model page on the Hugging Face Hub.\n",
    "- You will be prompted to share your username and email address with the model authors.\n",
    "- Fill out any additional fields requested by the model authors.\n",
    "- Click \"Agree\" to send the access request.\n",
    "- If the model uses automatic approval, you will gain access immediately. Otherwise, you will need to wait for manual approval from the model authors.\n",
    "\n",
    "**Link to Llama Family Models:** https://huggingface.co/meta-llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Authenticate Using the Access Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can authenticate using the access token in several ways:\n",
    "\n",
    "- Using the `huggingface_hub` Library\n",
    "```python\n",
    "from huggingface_hub import login\n",
    "\n",
    "access_token = \"YOUR TOKEN\"\n",
    "login(token=access_token)\n",
    "```\n",
    "\n",
    "- Setting the Environment Variable\n",
    "You can set the `HF_TOKEN` environment variable:\n",
    "```bash\n",
    "export HF_TOKEN= YOUR_TOKEN\n",
    "```\n",
    "Or in your code:\n",
    "```python\n",
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"YOUR_TOKEN\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Load the Model and Tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AutoTokenizer.from_pretrained()**\n",
    "\n",
    "The AutoTokenizer automatically selects and loads the appropriate tokenizer for a given pre-trained model[4]. Here are the key parameters:\n",
    "\n",
    "**Required Parameters:**\n",
    "- `model_id`: The name or path of the pre-trained model (e.g., \"meta-llama/Llama-3.1-8B\")\n",
    "\n",
    "**Optional Parameters:**\n",
    "- `use_auth_token`: Boolean or string to use Hugging Face token for authentication\n",
    "- `trust_remote_code`: Boolean to allow loading remote code\n",
    "- `padding`: Boolean or string ('max_length' or 'longest') for input padding[5]\n",
    "- `truncation`: Boolean or string to truncate inputs to max_length\n",
    "- `max_length`: Integer for maximum sequence length\n",
    "- `return_tensors`: Format of the returned tensors (\"pt\" for PyTorch, \"tf\" for TensorFlow)[5]\n",
    "\n",
    "Example with additional parameters:\n",
    "```python\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=True,\n",
    "    trust_remote_code=True,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AutoModelForCausalLM.from_pretrained()**\n",
    "\n",
    "This class is specifically designed for casual language modeling tasks, where the model generates text in a conversational manner[4]. Here are the key parameters:\n",
    "\n",
    "**Required Parameters:**\n",
    "- `model_id`: The name or path of the pre-trained model\n",
    "\n",
    "**Optional Parameters:**\n",
    "- `use_auth_token`: For authentication with Hugging Face\n",
    "- `trust_remote_code`: Allow loading remote code\n",
    "- `torch_dtype`: Specify the model's data type (e.g., torch.float16 for half precision)\n",
    "- `device_map`: Specify how to distribute the model across available hardware\n",
    "- `low_cpu_mem_usage`: Boolean to enable low CPU memory usage\n",
    "- `cache_dir`: Directory for storing downloaded models\n",
    "- `local_files_only`: Boolean to use only local files[3]\n",
    "- `revision`: Specific model version to use\n",
    "\n",
    "Example with additional parameters:\n",
    "```python\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    cache_dir=\"./model_cache\",\n",
    "    local_files_only=False,\n",
    "    revision=\"main\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `AutoTokenizer` and `AutoModelForCausalLM` from the `transformers` library to load the model and tokenizer, ensuring you use the access token for authentication:\n",
    "```python\n",
    "model_id = \"meta-llama/Llama-3.1-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "\n",
    "You can also load the model from your local directory if you have already downloaded it.\n",
    "\n",
    "```python\n",
    "# Load from local directory\n",
    "local_model_path = \"./saved_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    local_model_path,\n",
    "    local_files_only=True\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Use the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1. Input Preparation\n",
    "```python\n",
    "# Prepare input\n",
    "prompt = \"Write a short story about:\"\n",
    "inputs = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\",      # Return PyTorch tensors\n",
    "    padding=True,             # Optional: pad sequences\n",
    "    truncation=True,          # Optional: truncate sequences\n",
    "    max_length=512,           # Optional: maximum sequence length\n",
    ").to(model.device)           # Move to GPU if available\n",
    "```\n",
    "\n",
    "**Additional Tokenizer Parameters:**\n",
    "- `add_special_tokens`: Boolean to add model's special tokens (default: True)\n",
    "- `return_attention_mask`: Boolean to return attention mask (default: True)\n",
    "- `return_token_type_ids`: Boolean to return token type IDs\n",
    "- `return_overflowing_tokens`: Boolean to handle texts longer than max_length\n",
    "- `return_special_tokens_mask`: Boolean to return special tokens mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2. Text Generation\n",
    "```python\n",
    "with torch.no_grad():  # Disable gradient calculation for inference\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=200,           # Maximum length of generated text\n",
    "        min_length=10,            # Minimum length of generated text\n",
    "        temperature=0.7,          # Controls randomness (0.0 to 1.0)\n",
    "        do_sample=True,           # Enable sampling\n",
    "        num_beams=1,              # Number of beams for beam search\n",
    "        no_repeat_ngram_size=2,   # Prevent repetition of n-grams\n",
    "        pad_token_id=tokenizer.eos_token_id,  # Padding token ID\n",
    "        eos_token_id=tokenizer.eos_token_id,  # End of sequence token ID\n",
    "        attention_mask=inputs[\"attention_mask\"],  # Optional attention mask\n",
    "        top_k=50,                 # Top-k sampling parameter\n",
    "        top_p=0.95,               # Nucleus sampling parameter\n",
    "        repetition_penalty=1.2,   # Penalty for repeated tokens\n",
    "        length_penalty=1.0,       # Length penalty (>1.0 favors longer sequences)\n",
    "        num_return_sequences=1    # Number of sequences to generate\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detailed Parameter Explanation:**\n",
    "\n",
    "1. **Sampling Parameters:**\n",
    "```python\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    # Sampling strategy\n",
    "    do_sample=True,          # Enable sampling (vs greedy decoding)\n",
    "    temperature=0.7,         # Higher = more random, Lower = more focused\n",
    "    top_k=50,               # Limit to top k tokens during sampling\n",
    "    top_p=0.95,             # Nucleus sampling probability threshold\n",
    ")\n",
    "```\n",
    "\n",
    "2. **Length Control:**\n",
    "```python\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    # Length parameters\n",
    "    max_length=200,         # Maximum sequence length\n",
    "    min_length=10,          # Minimum sequence length\n",
    "    length_penalty=1.0,     # Penalty for sequence length\n",
    "    early_stopping=True,    # Stop when conditions are met\n",
    ")\n",
    "```\n",
    "\n",
    "3. **Beam Search Parameters:**\n",
    "```python\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    # Beam search parameters\n",
    "    num_beams=4,           # Number of beams for beam search\n",
    "    no_repeat_ngram_size=2,# Prevent repetition of n-grams\n",
    "    num_beam_groups=1,     # Number of groups for diverse beam search\n",
    "    diversity_penalty=0.0  # Penalty for diverse beam search\n",
    ")\n",
    "```\n",
    "\n",
    "4. **Repetition Control:**\n",
    "```python\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    # Repetition control\n",
    "    repetition_penalty=1.2,    # Penalty for repeated tokens\n",
    "    bad_words_ids=None,        # List of token IDs to prevent\n",
    "    force_words_ids=None       # List of token IDs to force\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3. Response Decoding\n",
    "```python\n",
    "# Basic decoding\n",
    "response = tokenizer.decode(\n",
    "    outputs[0],                 # First generated sequence\n",
    "    skip_special_tokens=True,   # Skip special tokens like [PAD], [CLS], etc.\n",
    "    clean_up_tokenization_spaces=True  # Clean up spaces from tokenization\n",
    ")\n",
    "\n",
    "# Batch decoding\n",
    "responses = tokenizer.batch_decode(\n",
    "    outputs,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=True\n",
    ")\n",
    "\n",
    "# Advanced decoding with additional parameters\n",
    "response = tokenizer.decode(\n",
    "    outputs[0],\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=True,\n",
    "    spaces_between_special_tokens=True,\n",
    "    truncate_before_pattern=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Example with Advanced Parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "prompt = \"Write a short story about:\"\n",
    "inputs = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=200,\n",
    "        min_length=50,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        num_beams=4,\n",
    "        no_repeat_ngram_size=2,\n",
    "        repetition_penalty=1.2,\n",
    "        length_penalty=1.0,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        attention_mask=inputs[\"attention_mask\"]\n",
    "    )\n",
    "\n",
    "responses = tokenizer.batch_decode(\n",
    "    outputs,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=True\n",
    ")\n",
    "\n",
    "for response in responses:\n",
    "    print(response)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Working Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3feddd77140447e891cbdf100b38154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def initialize_model_and_tokenizer(model_id):\n",
    "    # Initialize tokenizer with padding token\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        use_auth_token=True,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Set padding token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Initialize model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        use_auth_token=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_text(prompt, model, tokenizer):\n",
    "    # Prepare input\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_attention_mask=True\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=200,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Usage\n",
    "model_id = 'meta-llama/Llama-3.1-8B'\n",
    "model, tokenizer = initialize_model_and_tokenizer(model_id)\n",
    "prompt = \"Explain the meaning of life.\"\n",
    "response = generate_text(prompt, model, tokenizer)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: Using LlamaAPI client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Getting API Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Create an Account:**\n",
    "   - Visit https://www.llama-api.com\n",
    "   - Click on \"Log In\" → \"Sign up\"\n",
    "   - Complete the registration process[4]\n",
    "\n",
    "2. **Join Waitlist and Get Approval:**\n",
    "   - After signup, you'll be added to the waitlist\n",
    "   - Wait for the approval email (usually takes a few days)\n",
    "   - Once approved, proceed to the next step[4]\n",
    "\n",
    "3. **Obtain API Token:**\n",
    "   - Log in to your Llama API account\n",
    "   - Navigate to the \"API Token\" section\n",
    "   - Find your API token on the page\n",
    "   - Click the clipboard icon to copy your token[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Setting Up Development Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **Install Required Package:**\n",
    "```bash\n",
    "pip install llamaapi\n",
    "```\n",
    "\n",
    "2. **Create a New Python File:**\n",
    "   - Create a new .py file in your preferred IDE\n",
    "   - Import required libraries at the top of your file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implementing the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Initialize the Client:**\n",
    "```python\n",
    "from llamaapi import LlamaAPI\n",
    "import json\n",
    "\n",
    "# Replace with your actual API token\n",
    "llama = LlamaAPI(\"your_api_token\")\n",
    "```\n",
    "\n",
    "2. **Create the API Request:**\n",
    "```python\n",
    "api_request = {\n",
    "    \"model\": \"llama3.1-70b\",    # Specify the model\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the concept of machine learning\"\n",
    "        }\n",
    "    ],\n",
    "    \"stream\": False,            # Disable streaming\n",
    "    \"max_length\": 500,          # Maximum response length\n",
    "    \"temperature\": 0.7          # Control randomness (0.0 to 1.0)\n",
    "}\n",
    "```\n",
    "\n",
    "3. **Execute the Request and Handle Response:**\n",
    "```python\n",
    "try:\n",
    "    response = llama.run(api_request)\n",
    "    print(json.dumps(response.json(), indent=2))\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Additional Configuration Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can customize your request with these parameters[3]:\n",
    "```python\n",
    "api_request = {\n",
    "    \"model\": \"llama3.1-70b\",\n",
    "    \"messages\": [...],\n",
    "    \"max_token\": 500,           # Maximum tokens to generate\n",
    "    \"temperature\": 0.1,         # Lower = more focused outputs\n",
    "    \"top_p\": 1.0,              # Nucleus sampling parameter\n",
    "    \"frequency_penalty\": 1.0,   # Reduce repetition\n",
    "    \"stream\": False            # Enable/disable streaming\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Running the Complete Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamaapi import LlamaAPI\n",
    "import json\n",
    "\n",
    "# Initialize client\n",
    "llama = LlamaAPI(\"your_api_token\")\n",
    "\n",
    "# Create request\n",
    "api_request = {\n",
    "    \"model\": \"llama3.1-70b\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Explain the concept of machine learning\"}\n",
    "    ],\n",
    "    \"stream\": False,\n",
    "    \"max_length\": 500,\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Get response\n",
    "    response = llama.run(api_request)\n",
    "    \n",
    "    # Print formatted response\n",
    "    print(json.dumps(response.json(), indent=2))\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Notes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Rate Limits:**\n",
    "   - Limited to 20 questions per 60-second window\n",
    "   - Exceeding this limit triggers a cooldown period\n",
    "\n",
    "2. **Common Errors:**\n",
    "   - 429: Too Many Requests (rate limit exceeded)\n",
    "   - 408: Request Timeout\n",
    "   - 401: Unauthorized (invalid credentials)\n",
    "\n",
    "3. **Model Selection:**\n",
    "   Available models include:\n",
    "   - Mixtral models: mixtral-8x22b-instruct, mixtral-8x7b-instruct\n",
    "   - Mistral models: mistral-7b-instruct\n",
    "   - Qwen models: Various sizes from 0.5B to 110B\n",
    "\n",
    "4. **Pricing:**\n",
    "   - Costs vary based on the model used\n",
    "   - Pricing is per 1 million tokens\n",
    "   - Separate costs for input and output tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **MultiHead Attention for Image Classification**\n",
        "\n",
        "\n",
        "The primary goal of this notebook is to demonstrate the use of Multi-Head Attention within a CNN for image classification. By combining convolutional layers with attention mechanisms, the model aims to achieve improved accuracy in classifying handwritten digits. Specifically, the notebook aims to:\n",
        "\n",
        "  * **Implement Multi-Head Attention**: The notebook provides a detailed implementation of Multi-Head Attention using TensorFlow and Keras.\n",
        "  * **Build a CNN with Attention**: Integrate the Multi-Head Attention model into a CNN architecture for image classification.\n",
        "  * **Train and Evaluate the Model**: Train the model on the MNIST dataset and evaluate its performance using appropriate metrics.\n",
        "  * **Visualize Predictions**: Display predictions on sample images to showcase the model's classification capabilities.\n",
        "\n",
        "By achieving these goals, the notebook effectively illustrates the benefits and application of Multi-Head Attention in improving the accuracy of image classification tasks."
      ],
      "metadata": {
        "id": "bOcQv8WkF6az"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **MultiHead Attention**\n",
        "\n",
        "<img src= 'multihead_att.png' width=500>\n",
        "\n",
        "Firstly, we shall be defining our own MultiHead Attention model. Its going to perform the following:\n",
        "\n",
        "**1. Input Processing:**\n",
        "\n",
        "  * The function takes three inputs: q1, k1, and v1, representing the query, key, and value tensors, respectively.\n",
        "  * These inputs are passed through dense layers with ReLU activation to project them into a higher-dimensional space. The output tensors are named q2, k2, and v2.\n",
        "  \n",
        "**2. Reshaping for Multi-Head Attention:**\n",
        "\n",
        "  * The projected tensors (q2, k2, v2) are reshaped using `tf.keras.layers.Reshape` to create separate heads.\n",
        "  * The reshaping operation divides the tensors into nv (number of heads) along the last dimension.\n",
        "  * This results in tensors q, k, and v with shape [-1, l, nv, dv], where l is the sequence length, nv is the number of heads, and dv is the dimension of each head.\n",
        "\n",
        "**3. Scaled Dot-Product Attention:**\n",
        "\n",
        "  * The core attention mechanism is implemented using a `tf.keras.layers.Lambda` layer.\n",
        "  * Inside the lambda function, the query and key tensors (q and k) are multiplied using tf.matmul to calculate the attention weights.\n",
        "  * The attention weights are then scaled down by the square root of the head dimension (dv) to prevent them from becoming too large.\n",
        "  * A softmax function is applied to the scaled attention weights to normalize them into probabilities.\n",
        "\n",
        "**4.Weighted Value Aggregation:**\n",
        "\n",
        "  * The normalized attention weights (probabilities) are multiplied with the value tensor (v) using another tf.keras.layers.Lambda layer.\n",
        "  * This weighted sum of values produces the output of the attention mechanism.\n",
        "\n",
        "**5.Output Reshaping and Projection:**\n",
        "\n",
        "  * The output tensor is reshaped back to the original shape using `tf.keras.layers.Reshape`.\n",
        "  * The reshaped output is then passed through a final dense layer with ReLU activation to project it to the desired output dimension.\n",
        "\n",
        "**6. Residual Connection and Normalization:**\n",
        "\n",
        "  * The MultiHeadsAttModel function incorporates a residual connection by adding the initial input (q1) to the output of the multi-head attention. This residual connection has the benefits of stabilizing training and allowing information to flow more easily.\n",
        "\n",
        "  * Finally, the output is normalized using the NormL function, which implements Layer Normalization. Layer Normalization scales and shifts the input data independently for each feature, which can make training faster and more stable.\n",
        "\n"
      ],
      "metadata": {
        "id": "0uFPfEZY8Co3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def MultiHeadsAttModel(l=8*8, d=512, dv=64, dout=512, nv=8):\n",
        "    def model(inputs):\n",
        "        q1, k1, v1 = inputs\n",
        "\n",
        "        v2 = tf.keras.layers.Dense(dv*nv, activation=\"relu\")(v1)\n",
        "        q2 = tf.keras.layers.Dense(dv*nv, activation=\"relu\")(q1)\n",
        "        k2 = tf.keras.layers.Dense(dv*nv, activation=\"relu\")(k1)\n",
        "\n",
        "        v = tf.keras.layers.Reshape([-1, l, nv, dv])(v2)\n",
        "        q = tf.keras.layers.Reshape([-1, l, nv, dv])(q2)\n",
        "        k = tf.keras.layers.Reshape([-1, l, nv, dv])(k2)\n",
        "\n",
        "        # Wrap TensorFlow operations within a Lambda layer\n",
        "        att = tf.keras.layers.Lambda(lambda x: tf.matmul(x[0], x[1], transpose_b=True) / tf.sqrt(tf.cast(dv, tf.float32)))([q, k])\n",
        "        att = tf.keras.layers.Lambda(lambda x: tf.nn.softmax(x))(att)\n",
        "\n",
        "        out = tf.keras.layers.Lambda(lambda x: tf.matmul(x[0], x[1]))([att, v])\n",
        "        out = tf.keras.layers.Reshape([-1, l, d])(out)\n",
        "\n",
        "        out = out + q1 # Ensure shapes are compatible for addition\n",
        "\n",
        "        out = tf.keras.layers.Dense(dout, activation=\"relu\")(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "kEh_p9Ld8Fpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Layer Normalization**\n",
        "\n",
        "The `NormL` class implements Layer Normalization, a technique used to normalize the activations within a layer. This normalization helps stabilize and speed up the training process of neural networks.\n",
        "\n",
        "\n",
        "**1. Initialization:**\n",
        "\n",
        "  * The `__init__` method initializes the layer and creates two trainable weights: a and b. These weights are used for scaling and shifting the normalized data.\n",
        "\n",
        "**2. Building the Layer:**\n",
        "\n",
        "  * The build method is called when the layer is first used. It sets the shapes of the weights a and b based on the input shape.\n",
        "\n",
        "**3. Applying Normalization:**\n",
        "\n",
        "  * The call method is where the actual normalization happens.\n",
        "  * It calculates the mean (`mu`) and standard deviation (`sigma`) of the input data (`x`) along the last dimension (axis=-1).\n",
        "  * It then normalizes the data by subtracting the mean and dividing by the standard deviation. A small value (`eps`) is added to the standard deviation to avoid division by zero.\n",
        "  * Finally, the normalized data is scaled by a and shifted by b before being returned."
      ],
      "metadata": {
        "id": "nrexrdJsBzJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NormL(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(NormL, self).__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.a = self.add_weight(name='a', shape=(1, input_shape[-1]),\n",
        "                                 initializer='ones', trainable=True)\n",
        "        self.b = self.add_weight(name='b', shape=(1, input_shape[-1]),\n",
        "                                 initializer='zeros', trainable=True)\n",
        "\n",
        "    def call(self, x):\n",
        "        eps = 0.000001\n",
        "        mu = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
        "        sigma = tf.math.reduce_std(x, axis=-1, keepdims=True)\n",
        "        ln_out = (x - mu) / (sigma + eps)\n",
        "        return ln_out * self.a + self.b\n"
      ],
      "metadata": {
        "id": "ta4VAx-K8IBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Preparation**\n",
        "\n",
        "We are going to load the MNIST dataset, reshape the images, normalize the pixel values, and convert the labels into a suitable format for training the model. This preprocessing is essential for ensuring that the data is in the correct format and range for the model to learn effectively."
      ],
      "metadata": {
        "id": "0Vu6X5vl8PT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(60000, 28, 28, 1).astype('float32') / 255\n",
        "X_test = X_test.reshape(10000, 28, 28, 1).astype('float32') / 255\n",
        "\n",
        "Y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "Y_test = tf.keras.utils.to_categorical(y_test, 10)\n"
      ],
      "metadata": {
        "id": "BIMbFxIg8J0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Building**\n",
        "\n",
        "We are going to define a Convolutional Neural Network (CNN) with Multi-Head Attention for classifying MNIST handwritten digits. It starts with an input layer shaped for MNIST images. Then, add convolutional layers to extract features, followed by pooling to reduce dimensionality. Next, it utilizes Multi-Head Attention to capture relationships within the features. After reshaping and normalization, flatten the output and feed it through dense layers, ultimately producing a probability distribution over the 10 digit classes (0-9). This structure enables the model to learn complex patterns and achieve accurate digit classification.\n",
        "\n",
        "**In simpler terms:**\n",
        "\n",
        "Imagine the model as a series of steps:\n",
        "\n",
        "* **See:** It takes an image of a handwritten digit as input.\n",
        "Extract features: It uses convolutional layers to identify important features like edges and curves.\n",
        "* **Focus:** Multi-Head Attention helps the model focus on the most relevant features for classification.\n",
        "* **Process:** It flattens the features and uses dense layers to analyze them.\n",
        "* **Classify:** Finally, it outputs a probability distribution, predicting the digit in the image.\n",
        "\n",
        "This model architecture leverages the strengths of both convolutional and attention mechanisms for effective digit classification."
      ],
      "metadata": {
        "id": "MeRjtKWJ8UIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the model\n",
        "inputs = tf.keras.Input(shape=(28, 28, 1))\n",
        "x = tf.keras.layers.Conv2D(32, (2, 2), activation='relu', padding='same')(inputs)\n",
        "x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = tf.keras.layers.Conv2D(64, (2, 2), activation='relu')(x)\n",
        "x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='same')(x)\n",
        "x = tf.keras.layers.Conv2D(64*3, (2, 2), activation='relu')(x)\n",
        "\n",
        "x = tf.keras.layers.Reshape([-1, 6*6, 64*3])(x) # Use TF Keras Reshape layer\n",
        "att = MultiHeadsAttModel(l=6*6, d=64*3, dv=8*3, dout=32, nv=8)\n",
        "x = att([x, x, x])\n",
        "x = tf.keras.layers.Reshape([-1, 6, 6, 32])(x)  # Use TF Keras Reshape layer\n",
        "x = NormL()(x)\n",
        "\n",
        "x = tf.keras.layers.Flatten()(x)\n",
        "x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
        "outputs = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n"
      ],
      "metadata": {
        "id": "tb3Kt3uM8L7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Compiling & Training**"
      ],
      "metadata": {
        "id": "ncsD3MAj8Ys7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "\n",
        "model.fit(X_train, Y_train,\n",
        "          batch_size=128,\n",
        "          epochs=5,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, Y_test))\n"
      ],
      "metadata": {
        "id": "1M4kBLxNH7zU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Inference**"
      ],
      "metadata": {
        "id": "PifHani8ida3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(Y_test, axis=1)\n",
        "\n",
        "# Plot some images with actual and predicted values\n",
        "num_images_to_plot = 5\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i in range(num_images_to_plot):\n",
        "    plt.subplot(1, num_images_to_plot, i+1)\n",
        "    plt.imshow(X_test[i].reshape(28, 28), cmap='gray')\n",
        "    plt.title(f\"True: {y_true[i]}\\nPredicted: {y_pred_classes[i]}\")\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7DKwz-lpgpY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jkNVC63Oics6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CgVSVaOctqQ"
      },
      "source": [
        "# MultiHead-Attention Mechanism (Graded)\n",
        "\n",
        "Welcome to your MultiHead-Attention (required) programming assignment! You will build a **Sentiment Analysis** model using MultiHead-Attention. You will be using [Coronavirus tweets](https://www.kaggle.com/datasets/alincijov/bilingual-sentence-pairs) dataset which contains 40k+ tweets from the twitter about the corona virus.\n",
        "\n",
        "Your goal is to build a text classifier to classify the sentiment of each tweet if they are positive, neutral or negative.\n",
        "\n",
        "**Instructions:**\n",
        "* Do not modify any of the codes.\n",
        "* Only write code when prompted. For example in some sections you will find the following,\n",
        "```\n",
        "# YOUR CODE GOES HERE\n",
        "# YOUR CODE STARTS HERE\n",
        "# TODO\n",
        "```\n",
        "Only modify those sections of the code.\n",
        "* You will find **REFLECTION** under few code cells where you are asked to write your thoughts or interpretations on the outputs.\n",
        "\n",
        "\n",
        "**You will learn to:**\n",
        "* Explore the [Corona Virus Tweets](https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification/data?select=Corona_NLP_train.csv) dataset.\n",
        "* Clean the dataset before using it for training.\n",
        "* Build a robust text classification model using just MultiHead-Attention mechanism.\n",
        "* Inference using trained model to make predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ueepFGkfeci"
      },
      "source": [
        "# Corona Virus Tweet Classification using MultiHead-Attention Mechanism\n",
        "\n",
        "Download the train/test csv files from [here](https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification/data?select=Corona_NLP_train.csv)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4PAaOltrDXc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "from tests import *\n",
        "\n",
        "# Download the train/test csv files and load them\n",
        "df_train = #TODO: Load using Pandas\n",
        "df_test = #TODO: Load using Pandas\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hS7XJIQrW22"
      },
      "outputs": [],
      "source": [
        "df_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqOuxa-kriAV"
      },
      "outputs": [],
      "source": [
        "# Train and test set\n",
        "x_train = df_train['OriginalTweet']\n",
        "y_train = df_train['Sentiment']\n",
        "\n",
        "x_test = df_test['OriginalTweet']\n",
        "y_test = df_test['Sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GTFwMe8rTbY"
      },
      "outputs": [],
      "source": [
        "# TODO: plot a bar plot to check the count of Sentiment values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FczDVeOAZmIw"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "<img src=\"assets/distribution.png\" width=400>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvnyOppwjarw"
      },
      "source": [
        "# Data Preparation\n",
        "\n",
        "The data is prepared by carrying out the following steps:\n",
        "\n",
        "* **Data cleaning:** Lowercasing, removing punctuations, URLs, HTML tags, excluding special characters etc.\n",
        "* **Tokenizing and Padding:** Padding input and target tensors to a uniform length.\n",
        "* **Sentiment Mapping and OneHot Encoding:** Map the sentiment labels to numerical values using a dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQq_UemFj5tl"
      },
      "source": [
        "## Data Cleaning\n",
        "\n",
        "Example tweet from the dataset:\n",
        "```\n",
        "My food stock is not the only one which is empty...\\r\\r\\n\\r\\r\\nPLEASE, don't panic, THERE WILL BE ENOUGH FOOD FOR EVERYONE if you do not take more than you need. \\r\\r\\nStay calm, stay safe.\\r\\r\\n\\r\\r\\n#COVID19france #COVID_19 #COVID19 #coronavirus #confinement #Confinementotal #ConfinementGeneral https://t.co/zrlG0Z520j\n",
        "```\n",
        "\n",
        "We'll be performing some data cleaning steps to remove the following,\n",
        "* **URLs**: This ensures that web addresses don't interfere with the sentiment analysis.\n",
        "\n",
        "* **HTML Tags**: It removes any HTML tags that might be present in the tweets.\n",
        "\n",
        "* **Digits**: Numerical digits usually don't carry much sentimental value.\n",
        "\n",
        "* **Hashtags and Mentions**: Hashtags (e.g., #COVID19) and mentions (e.g., @WHO)\n",
        "\n",
        "* **Stop Words**: common words like \"the,\" \"a,\" \"is,\" etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0_GNiYOrh9x"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "\n",
        "def text_cleaner(tweet):\n",
        "    # TODO: remove urls\n",
        "\n",
        "    # TODO: remove html tags\n",
        "\n",
        "    # TODO: remove digits\n",
        "\n",
        "    # TODO: remove hashtags\n",
        "\n",
        "    # TODO: remove mentions\n",
        "\n",
        "    #TODO: remove stop words\n",
        "\n",
        "    return tweet\n",
        "\n",
        "stop_words = #TODO: Load English stopwords\n",
        "\n",
        "#TODO: Clean tweets using text_cleaner on x_train and x_test\n",
        "x_train_clean =\n",
        "x_test_clean =\n",
        "\n",
        "x_train_clean.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uFUG1I9o98P"
      },
      "source": [
        "## Tokenizing and Padding\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "Sentence:\n",
        "Due COVID- retail store classroom Atlanta open walk-in business classes next two weeks, beginning Monday, March . We continue process online phone orders normal! Thank understanding!\n",
        "\n",
        "After tokenizing :\n",
        "[34, 1, 69, 4, 11239, 4874, 153, 665, 39, 104, 2637, 174, 172, 146, 812, 766, 186, 25, 267, 1487, 13, 802, 450, 326, 102, 2185]\n",
        "\n",
        "After padding :\n",
        "[   34     1    69     4 11239  4874   153   665    39   104  2637   174\n",
        "   172   146   812   766   186    25   267  1487    13   802   450   326\n",
        "   102  2185     0     0     0     0     0     0     0     0     0     0\n",
        "     0     0     0     0     0     0     0     0     0     0     0     0\n",
        "     0     0     0     0     0     0]\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TC-HawuMR73s"
      },
      "outputs": [],
      "source": [
        "len(x_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnJWCMF8rh5J"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "\n",
        "# TODO: Import Tokenizer and pad_sequences\n",
        "\n",
        "#TODO: Define tokenizer and fit on clean input text\n",
        "tokenizer =\n",
        "\n",
        "\n",
        "#TODO: Convert the clean text to sequences\n",
        "x_train_seqs =\n",
        "x_test_seqs =\n",
        "\n",
        "# TODO: Pad the train and test sequences\n",
        "x_train_seqs =\n",
        "x_test_seqs =\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Asdk44yoSILz"
      },
      "outputs": [],
      "source": [
        "x_test_seqs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVqozZPl-YLW"
      },
      "source": [
        "## Sentiment Mapping and One Hot Encoding\n",
        "\n",
        "Build a sentiment map to create 3 classes:\n",
        "\n",
        "* **Negative [0]** -> Extremely Negative, Negative\n",
        "* **Neutral [1]**\n",
        "* **Positive [2]** -> Extemely Positive, Positive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOcRi-fRrhan"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "\n",
        "# TODO: Map the sentiment labels to numerical values using a dictionary.\n",
        "sentiments =\n",
        "\n",
        "# TODO: Map the train and test target labels to above sentiments dictionary\n",
        "y_train =\n",
        "y_test =\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEpHHE0BzYhl"
      },
      "outputs": [],
      "source": [
        "# TODO: One hot encode target and test target\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train =\n",
        "y_test =\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AvRQ81MvEfe"
      },
      "source": [
        "**REFLECTION**\n",
        "\n",
        "\\<Why do you think we have to one hot encode our target values?>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwOYkHoblNUo"
      },
      "outputs": [],
      "source": [
        "validate_data_preparation(x_train_seqs, x_test_seqs, y_train, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BbGHOs9E6b6"
      },
      "source": [
        "# Model Training and Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wr01I9RyIIz1"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "\n",
        "#TODO: Import the necessary layers from the TensorFlow\n",
        "\n",
        "# Define the model\n",
        "def multihead_attention_model(vocab_size, embedding_dim, num_heads, ff_dim):\n",
        "    # TODO: Define an input layer\n",
        "\n",
        "    # TODO: Define an Embedding layer\n",
        "\n",
        "    # TODO: Define a Multi-head attention layer\n",
        "\n",
        "    # TODO: Define a feed forward layer\n",
        "\n",
        "\n",
        "    # TODO: Define a LayerNormalization layer\n",
        "\n",
        "    # TODO: Define a Global Average Pooling layer to get a fixed-length representation\n",
        "\n",
        "    # TODO: Define Output layer with 3 units corresponding to the number of classes\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwLHFkzKwgEL"
      },
      "outputs": [],
      "source": [
        "# TODO: Define model parameters\n",
        "vocab_size =\n",
        "embedding_dim =\n",
        "num_heads =\n",
        "ff_dim =\n",
        "\n",
        "# TODO: Build the model\n",
        "model =\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSZ4dbfquVSW"
      },
      "source": [
        "**REFLECTION**\n",
        "\n",
        "\\<Write your thoughts about the model structure here>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSL1Dt-ry8dR"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "\n",
        "def main(model):\n",
        "\n",
        "  # TODO: Compile the model with a categorical crossentropy loss\n",
        "\n",
        "\n",
        "  # TODO: Train the model\n",
        "  history =\n",
        "\n",
        "  return model, history\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, history = main(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP7sKBi_uF-f"
      },
      "source": [
        "**REFLECTION**\n",
        "\n",
        "\\<Write your observations here. Why is model behaving the way its behaving?>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-8QajHqsj3n"
      },
      "source": [
        "In order to pass this test, **achieve atleast 85% test accuracy**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFdRtjsatTvX"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Evaluate the model\n",
        "loss, test_accuracy = model.evaluate(x_test_seqs, y_test)\n",
        "print('Test accuracy:', accuracy)\n",
        "\n",
        "# Get predictions for the test set\n",
        "y_pred = model.predict(x_test_seqs)\n",
        "\n",
        "# Convert predictions to class labels (0, 1, 2)\n",
        "y_pred_labels = numpy.argmax(y_pred, axis=1)\n",
        "\n",
        "# Convert true labels (y_test) to class labels\n",
        "y_true_labels = numpy.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate classification report\n",
        "# You can also use other metrics like precision, recall, F1-score\n",
        "print(classification_report(y_true_labels, y_pred_labels))\n",
        "\n",
        "test_model_accuracy(test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihvsVoS6t21Z"
      },
      "source": [
        "**REFLECTION**\n",
        "\n",
        "\\<Write your observations here>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Rp7vW0omQk8"
      },
      "outputs": [],
      "source": [
        "plot_metrics(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKgQfnguuCrf"
      },
      "source": [
        "**REFLECTION**\n",
        "\n",
        "\\<Write your observations here>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY3EbsDGxPRJ"
      },
      "source": [
        "# Improvement Strategies\n",
        "\n",
        "Here are some model improvement strategies you can consider to improve the model:\n",
        "\n",
        "1. **Embedding Dimension (embedding_dim)**: Try values like 64, 128, 256, or 512.\n",
        "2. **Number of Heads (num_heads)**: Experiment with values like 4, 8, or 16.\n",
        "3. **Feedforward Dimension (ff_dim)**: Try values like 128, 256, or 512.\n",
        "4. **Add More Layers**: Consider adding more multi-head attention layers and feedforward layers to increase the model's capacity to learn complex patterns.\n",
        "5. **Learned Positional Embeddings**: Add a trainable embedding layer to represent the position of each word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpzNrxDbQdFP"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14Nphq3BrhVl"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "labels = ['Negative', 'Neutral', 'Positive']\n",
        "sentence = \"I dont know what to do!\"\n",
        "\n",
        "predicted_sentiment = run_inference(model, text_cleaner, tokenizer, sentence, labels)\n",
        "\n",
        "print(f\"Input Sentence: {sentence}\")\n",
        "print(f\"Predicted Sentiment: {predicted_sentiment}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EjubyHZrhTD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02tYFcS2rhQ9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYlZXQEFrhOc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DE_WbvJ3rhJu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Self Attention Mechanism</h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief Recap of Self-Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Self-attention, also known as intra-attention, is a powerful mechanism in deep learning that allows a model to focus on different parts of its input when processing a sequence. \n",
    "\n",
    "- Unlike traditional attention mechanisms that operate between different sequences, self-attention computes relationships within a single sequence. \n",
    "\n",
    "- This enables the model to capture long-range dependencies and contextual information more effectively than traditional sequential processing methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"static/image1.gif\" alt=\"Self Attention Mechanism\" style=\"width:50%;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture of Self-Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Query, Key, and Value Matrices:** The input sequence is transformed into three separate representations:\n",
    "    - Query (Q): Represents the current element's search query\n",
    "\n",
    "    - Key (K): Represents elements to be matched against\n",
    "    - Value (V): Represents the actual content to be aggregated\n",
    "\n",
    "- **Attention Scores:** Computed by taking the dot product of the Query with all Keys, measuring how much focus to place on other parts of the input.\n",
    "\n",
    "- **Scaling:** The attention scores are divided by the square root of the dimension of the Key vectors to stabilize gradients.\n",
    "\n",
    "- **Softmax:** Applied to the scaled attention scores to obtain attention weights.\n",
    "\n",
    "- **Weighted Sum:** The final output is computed by taking a weighted sum of the Value vectors, using the attention weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mathematical representation of self-attention can be expressed as:\n",
    "\n",
    "$$Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "Where $d_k$ is the dimension of the Key vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"static/image2.webp\" alt=\"Self Attention Mechanism\" style=\"width:50%;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of Self-Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention has revolutionized various AI domains:\n",
    "\n",
    "- **Natural Language Processing (NLP)**\n",
    "  - Machine Translation\n",
    "  - Text Summarization\n",
    "  - Question Answering\n",
    "\n",
    "- **Computer Vision**\n",
    "  - Image Classification\n",
    "  - Object Detection\n",
    "  - Image Captioning\n",
    "\n",
    "- **Speech Recognition**\n",
    "\n",
    "- **Recommendation Systems**\n",
    "\n",
    "- **Bioinformatics**\n",
    "  - Protein Structure Prediction\n",
    "  - DNA Sequence Analysis\n",
    "\n",
    "- **Time Series Analysis**\n",
    "  - Financial Forecasting\n",
    "  - Anomaly Detection\n",
    "\n",
    "- **Graph Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Self Attention Mechanism with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Self Attention Mechanism can be designed by defining a custom class which can be used as a TensorFlow layer. Let's get to know how to design it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Definition and Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class SelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, attention_units):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.attention_units = attention_units\n",
    "```\n",
    "\n",
    "- We define a class `SelfAttention` that inherits from `tf.keras.layers.Layer`.\n",
    "- The `__init__` method initializes the layer with a specified number of `attention_units`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def build(self, input_shape):\n",
    "    self.W_q = self.add_weight(\"W_q\", shape=(input_shape[-1], self.attention_units))\n",
    "    self.W_k = self.add_weight(\"W_k\", shape=(input_shape[-1], self.attention_units))\n",
    "    self.W_v = self.add_weight(\"W_v\", shape=(input_shape[-1], self.attention_units))\n",
    "```\n",
    "\n",
    "- The `build` method is called once the layer knows its input shape.\n",
    "- We create three weight matrices: `W_q`, `W_k`, and `W_v` for query, key, and value transformations respectively.\n",
    "- Each weight matrix transforms the input from its last dimension to `attention_units`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def call(self, inputs):\n",
    "    q = tf.matmul(inputs, self.W_q)\n",
    "    k = tf.matmul(inputs, self.W_k)\n",
    "    v = tf.matmul(inputs, self.W_v)\n",
    "```\n",
    "\n",
    "- In the `call` method, we first compute query (q), key (k), and value (v) by multiplying the input with their respective weight matrices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Score Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "attention_scores = tf.matmul(q, k, transpose_b=True)\n",
    "attention_scores = attention_scores / tf.math.sqrt(tf.cast(self.attention_units, tf.float32))\n",
    "```\n",
    "\n",
    "- We compute attention scores by multiplying q and k (transposed).\n",
    "- The scores are then scaled by dividing by the square root of `attention_units`. This scaling helps stabilize gradients during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Weights and Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "attention_weights = tf.nn.softmax(attention_scores, axis=-1)\n",
    "output = tf.matmul(attention_weights, v)\n",
    "\n",
    "return output, attention_weights\n",
    "```\n",
    "\n",
    "- We apply softmax to get attention weights, which sum to 1 along the last axis.\n",
    "- The final output is computed by multiplying attention weights with the value (v).\n",
    "- The method returns both the output and the attention weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This self-attention mechanism allows the model to weigh the importance of different parts of the input sequence when processing each element. It's particularly useful for capturing long-range dependencies in sequential data. Let's combine it all together to see how the final class looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Self-Attention Layer\n",
    "class SelfAttention(tf.keras.layers.Layer):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, attention_units):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.attention_units = attention_units\n",
    "    \n",
    "    # Build method to define the weights\n",
    "    def build(self, input_shape):\n",
    "        self.W_q = self.add_weight(\"W_q\", shape=(input_shape[-1], self.attention_units))\n",
    "        self.W_k = self.add_weight(\"W_k\", shape=(input_shape[-1], self.attention_units))\n",
    "        self.W_v = self.add_weight(\"W_v\", shape=(input_shape[-1], self.attention_units))\n",
    "    \n",
    "    # Call method to perform the calculations\n",
    "    def call(self, inputs):\n",
    "        # Calculate the query, key, and value matrices\n",
    "        q = tf.matmul(inputs, self.W_q)\n",
    "        k = tf.matmul(inputs, self.W_k)\n",
    "        v = tf.matmul(inputs, self.W_v)\n",
    "        \n",
    "        # Calculate the attention scores\n",
    "        attention_scores = tf.matmul(q, k, transpose_b=True)\n",
    "        attention_scores = attention_scores / tf.math.sqrt(tf.cast(self.attention_units, tf.float32))\n",
    "        \n",
    "        # Apply the softmax activation function\n",
    "        attention_weights = tf.nn.softmax(attention_scores, axis=-1)\n",
    "        output = tf.matmul(attention_weights, v)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although, the above approach gives you more control in designing your self-attention mechanism, there is an alternate and more simpler way to do that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow provides built-in layers for implementing attention mechanisms, including self-attention. Here's how you can implement self-attention using Keras APIs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Create the Attention layer\n",
    "attention_layer = tf.keras.layers.Attention(use_scale=True, score_mode='dot')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Inputs:**\n",
    "- query: A tensor of shape (batch_size, Tq, dim), where Tq is the query sequence length.\n",
    "\n",
    "- value: A tensor of shape (batch_size, Tv, dim), where Tv is the value sequence length.\n",
    "- key: An optional tensor of shape (batch_size, Tv, dim). If not provided, value is used as the key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output:** \n",
    "- The layer returns attention outputs of shape (batch_size, Tq, dim).\n",
    "\n",
    "- Optionally, it can return attention scores after masking and softmax with shape (batch_size, Tq, Tv)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Arguments:**\n",
    "- use_scale (default: False): If True, creates a scalar variable to scale the attention scores.\n",
    "- score_mode (default: 'dot'): The scoring function to use. Options are 'dot', 'general', or 'additive'.\n",
    "- dropout (default: 0.0): Dropout rate for attention weights.\n",
    "- seed (default: None): Random seed for dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about Tensorflow's Attention layeer, refer to this link: [Tensorflow Attention Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define model parameters\n",
    "sequence_length = 20  # Length of input sequences\n",
    "input_dim = 10        # Dimensionality of each element in the sequence\n",
    "num_classes = 5       # Number of output classes\n",
    "\n",
    "# Define input shape\n",
    "input_shape = (sequence_length, input_dim)\n",
    "\n",
    "# Create inputs\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "# Create query, key, and value through dense layers\n",
    "query = Dense(64, name='query_layer')(inputs)\n",
    "key = Dense(64, name='key_layer')(inputs)\n",
    "value = Dense(64, name='value_layer')(inputs)\n",
    "\n",
    "# Apply attention\n",
    "attention_output = Attention(name='attention_layer')([query, key, value])\n",
    "\n",
    "# Further processing\n",
    "x = Dense(32, activation='relu', name='dense_layer')(attention_output)\n",
    "outputs = Dense(num_classes, activation='softmax', name='output_layer')(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Points:**\n",
    "\n",
    "- We create the input layer using Input(shape=input_shape). This layer expects input data with shape (batch_size, sequence_length, input_dim).\n",
    "\n",
    "- We create separate dense layers for query, key, and value:\n",
    "    - These layers transform the input into representations suitable for attention mechanism.\n",
    "    - Each dense layer has 64 units, but you can adjust this based on your needs.\n",
    "\n",
    "- We apply the attention mechanism using the Attention() layer:\n",
    "    - This layer takes query, key, and value as inputs and computes the attention output.\n",
    "\n",
    "- After the attention layer, we add a dense layer with 32 units and ReLU activation for further processing.\n",
    "\n",
    "- The final output layer uses softmax activation for multi-class classification with num_classes units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis of IMDB Reviews with Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to build a sentiment analysis model using self-attention to classify movie reviews as positive or negative. This project will demonstrate the effectiveness of the self-attention mechanism in capturing important features from text data for sentiment classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Decription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The IMDB dataset consists of 50,000 movie reviews, split evenly into 25,000 training and 25,000 testing samples. \n",
    "\n",
    "- Each review is labeled as either positive (1) or negative (0). \n",
    "\n",
    "- The dataset has been preprocessed to contain only the most frequent 10,000 words.\n",
    "\n",
    "- Key features of the dataset:\n",
    "    - 50,000 movie reviews (25,000 for training, 25,000 for testing)\n",
    "    - Binary sentiment classification (positive or negative)\n",
    "    - Preprocessed to include only the top 10,000 most frequent words\n",
    "    - Variable length reviews\n",
    "\n",
    "- For more information about the dataset, refer to this link: [IMDB TF Documentation](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb) || [IMDB Stanford Dataset](https://ai.stanford.edu/%7Eamaas/data/sentiment/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "vocab_size = 10000\n",
    "max_length = 200\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")\n",
    "print(f\"Shape of training data: {X_train.shape}\")\n",
    "print(f\"Shape of testing data: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding the Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step prepares our text data for input into our neural network. Here's why we do this:\n",
    "\n",
    "- **Uniform Length**: Neural networks typically require input data to have a consistent shape. However, movie reviews can vary in length.\n",
    "\n",
    "- **pad_sequences Function**: This Keras utility helps us standardize the length of our sequences.\n",
    "\n",
    "- **How it Works**:\n",
    "   - For sequences shorter than `max_length`, it adds padding (usually zeros) at the beginning.\n",
    "   - For sequences longer than `max_length`, it truncates them, keeping the last `max_length` elements.\n",
    "\n",
    "- **Preserving Recent Information**: By keeping the end of longer sequences, we retain the most recent (and often most relevant) parts of the reviews.\n",
    "\n",
    "- **Efficiency**: Having fixed-length inputs allows for more efficient batch processing during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=max_length)\n",
    "X_test = pad_sequences(X_test, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to float32 prepares our labels for efficient processing by the neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of training data: {X_train.shape}\")\n",
    "print(f\"Shape of testing data: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below code, while designing the layers, we use the following layers:\n",
    "- **Input Layer**: Accepts sequences of `max_length`.\n",
    "\n",
    "- **Embedding Layer**: Converts word indices to dense vectors. Uses L2 regularization to prevent overfitting.\n",
    "\n",
    "- **Attention Layer**: Applies self-attention mechanism to focus on important parts of the input.\n",
    "\n",
    "- **Layer Normalization**: Normalizes the outputs, helping with training stability.\n",
    "\n",
    "- **Global Average Pooling**: Reduces the sequence dimension to a fixed size.\n",
    "\n",
    "- **Dropout Layers**: Randomly drops 50% of neurons during training to prevent overfitting.\n",
    "\n",
    "- **Dense Layers**: Fully connected layers for feature extraction and final prediction. Use L2 regularization and ReLU activation.\n",
    "\n",
    "- **Output Layer**: Single neuron with sigmoid activation for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, GlobalAveragePooling1D, Attention, LayerNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "# Model Building\n",
    "def create_model(vocab_size, max_length):\n",
    "    inputs = Input(shape=(max_length,))\n",
    "    embedding = Embedding(vocab_size, 128, embeddings_regularizer=l2(1e-5))(inputs)\n",
    "    attention = Attention()([embedding, embedding])\n",
    "    normalized = LayerNormalization(epsilon=1e-6)(attention + embedding)\n",
    "    pooled = GlobalAveragePooling1D()(normalized)\n",
    "    x = Dropout(0.5)(pooled)\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(1, activation='sigmoid', kernel_regularizer=l2(1e-4))(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is then compiled with Adam optimizer, binary crossentropy loss (suitable for binary classification), and accuracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(vocab_size, max_length)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks for Training Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These callbacks help optimize the training process:\n",
    "- Early Stopping:\n",
    "    - Monitors validation loss\n",
    "\n",
    "    - Stops training if no improvement for 3 epochs\n",
    "    - Restores the best weights to prevent overfitting\n",
    "\n",
    "- ReduceLROnPlateau:\n",
    "    - Reduces learning rate when progress plateaus\n",
    "    \n",
    "    - Decreases learning rate by 20% if no improvement for 2 epochs\n",
    "    - Helps fine-tune the model and overcome local minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code initiates the training process for our sentiment analysis model:\n",
    "\n",
    "- Input Data: Uses X_train (input sequences) and y_train (labels) for training.\n",
    "\n",
    "- Epochs: Sets a maximum of 20 training cycles through the entire dataset.\n",
    "\n",
    "- Batch Size: Processes 32 samples at a time, balancing between speed and memory usage.\n",
    "\n",
    "- Validation Split: Reserves 20% of the training data for validation.\n",
    "\n",
    "- Callbacks:\n",
    "    - Applies Early Stopping to prevent overfitting\n",
    "    - Uses ReduceLROnPlateau to adjust learning rate dynamically\n",
    "\n",
    "- Verbose Mode: Set to 1 for detailed progress output during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit() method returns a history object containing training metrics, which can be used later for performance analysis and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,  # Increased max epochs\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Testing and Evaluation\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss Plot:**\n",
    "\n",
    "- Left subplot shows training and validation loss over epochs.\n",
    "\n",
    "- Helps identify overfitting (if validation loss increases while training loss decreases).\n",
    "\n",
    "**Accuracy Plot:**\n",
    "\n",
    "- Right subplot displays training and validation accuracy over epochs.\n",
    "\n",
    "- Indicates how well the model is learning and generalizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Accuracy Plot\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below visualizes how the model's attention mechanism focuses on different parts of the input:\n",
    "\n",
    "1. **Extract Attention Layer**: Gets the attention layer from the trained model.\n",
    "\n",
    "2. **Create Attention Model**: Builds a new model that outputs the attention weights.\n",
    "\n",
    "3. **Sample Input**: Uses the first sample from the test set.\n",
    "\n",
    "4. **Predict Attention Weights**: Applies the sample input to get attention weights.\n",
    "\n",
    "5. **Visualization**:\n",
    "   - Creates a heatmap of attention weights.\n",
    "   - X and Y axes represent sequence positions.\n",
    "   - Brighter colors indicate higher attention weights.\n",
    "\n",
    "6. **Interpretation**:\n",
    "   - Vertical bright lines show words the model focuses on across the sequence.\n",
    "   - This helps understand which parts of the input are most influential in the model's decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Weights Visualization\n",
    "attention_layer = model.get_layer('attention_2')\n",
    "attention_model = Model(inputs=model.input, outputs=attention_layer.output)\n",
    "\n",
    "sample_input = X_test[:1]\n",
    "attention_weights = attention_model.predict(sample_input)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(attention_weights[0], cmap='viridis')\n",
    "plt.title('Attention Weights')\n",
    "plt.xlabel('Sequence Position')\n",
    "plt.ylabel('Sequence Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Prediction\n",
    "def predict_sentiment(review_text):\n",
    "    word_index = imdb.get_word_index()\n",
    "    review_sequence = [word_index.get(word, 0) for word in review_text.lower().split()]\n",
    "    review_sequence = pad_sequences([review_sequence], maxlen=max_length)\n",
    "    prediction = model.predict(review_sequence)[0][0]\n",
    "    return \"Positive\" if prediction > 0.5 else \"Negative\", prediction\n",
    "\n",
    "sample_review = \"This movie was fantastic! The acting was great and the plot was engaging.\"\n",
    "sentiment, score = predict_sentiment(sample_review)\n",
    "print(f\"Sample review: {sample_review}\")\n",
    "print(f\"Predicted sentiment: {sentiment}\")\n",
    "print(f\"Sentiment score: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

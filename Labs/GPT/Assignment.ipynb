{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2-jmbtTJvLR"
      },
      "source": [
        "# GPT (Graded)\n",
        "\n",
        "Welcome to your GPT (required) programming assignment! You will build a GPT model that will generate **Shakesperean like text**. You will be using [Tiny Shakespeare](https://huggingface.co/datasets/karpathy/tiny_shakespeare) dataset which contains 40,000 lines from a variety of Shakespeare's plays.\n",
        "\n",
        "Imagine you have a student (the GPT-2 model) who is already quite good at writing in general. You want to teach this student to write like Shakespeare (fine-tuning). So, you provide the student with a lot of Shakespeare's work (the dataset) and have them practice writing in a similar style (training). Once the student has learned enough, you give them a starting phrase (the prompt) and ask them to write a continuation in Shakespearean style (inference). The student uses their knowledge gained during training to create new text that resembles Shakespeare's writing.\n",
        "\n",
        "\n",
        "**Instructions:**\n",
        "* Do not modify any of the codes.\n",
        "* Only write code when prompted. For example in some sections you will find the following,\n",
        "```\n",
        "# YOUR CODE GOES HERE\n",
        "# YOUR CODE STARTS HERE\n",
        "# TODO\n",
        "```\n",
        "Only modify those sections of the code.\n",
        "\n",
        "**You will learn to:**\n",
        "* Explore the [Tiny Shakespeare](https://huggingface.co/datasets/karpathy/tiny_shakespeare) dataset\n",
        "* Implementation of a GPT-2 model for text generation\n",
        "* Fine-tuning pre-trained models on specific datasets\n",
        "* Working with the Transformers library and TensorFlow\n",
        "* Managing model parameters and hyperparameters\n",
        "* Balancing between model performance and computational resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0X_imJIURaKQ"
      },
      "source": [
        "# Data Preparation\n",
        "\n",
        "\n",
        "In this section, You will transform raw text data into a numerical format suitable for training a GPT-2 model using TensorFlow. It involves loading the dataset, tokenizing the text, and creating a TensorFlow dataset that can be fed into the model during the training process.\n",
        "\n",
        "You shall be performing the following steps:\n",
        "\n",
        "1. **Loading the Dataset:**\n",
        "\n",
        "  * Begin by using the `load_dataset` function from the datasets library to load the \"tiny_shakespeare\" dataset.\n",
        "  * The training texts are extracted and stored in the `train_texts` attribute.\n",
        "2. **Loading and Configuring Tokenizer:**\n",
        "\n",
        "  * Load the \"GPT-2\" tokenizer using `GPT2Tokenizer.from_pretrained(\"gpt2\")`.\n",
        "  * Configure the tokenizer by setting the padding token to the end-of-sequence token (`eos_token`). This ensures that padding is treated as the end of a sequence during training.\n",
        "3. **Tokenizing the Texts:**\n",
        "\n",
        "  * Tokenize the training texts using the loaded tokenizer.\n",
        "  * Parameters like `truncation`, `padding`, `max_length`, and `return_tensors` are used to control the tokenization process. This converts the text into numerical representations that the model can understand.\n",
        "4. **Creating TensorFlow Dataset:**\n",
        "\n",
        "  * Create a TensorFlow dataset from the tokenized input IDs, attention mask, and input IDs again. This dataset will be used for training the model.\n",
        "5. **Preparing Final Training Dataset:**\n",
        "\n",
        "  * Shuffle the dataset using `shuffle(1000)` to randomize the order of training examples.\n",
        "  * Then create batches using `batch(self.batch_size)` to divide the data into smaller groups for efficient training.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMu2HmIqQp_7"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "\n",
        "from tests import *\n",
        "from helpers import *\n",
        "\n",
        "import tensorflow as tf\n",
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "\n",
        "class ShakespeareGenerator:\n",
        "    def __init__(self, seed=42, max_length=128, batch_size=8):\n",
        "        self.max_length = max_length\n",
        "        self.batch_size = batch_size\n",
        "        tf.random.set_seed(seed)\n",
        "\n",
        "\n",
        "    def prepare_data(self):\n",
        "        \"\"\"Data preparation phase: Load and process the dataset\"\"\"\n",
        "        # TODO: 1. Load the dataset\n",
        "        dataset = \n",
        "        self.train_texts = \n",
        "\n",
        "        # TODO: 2. Load and configure tokenizer\n",
        "        self.tokenizer = \n",
        "        self.tokenizer.pad_token = \n",
        "\n",
        "        # TODO: 3. Tokenize the texts\n",
        "        self.train_encodings = self.tokenizer(\n",
        "            \n",
        "        )\n",
        "\n",
        "        # TODO: 4. Create TensorFlow dataset\n",
        "        self.train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "\n",
        "        ))\n",
        "\n",
        "        # TODO: 5. Prepare final training dataset\n",
        "        self.train_dataset = \n",
        "\n",
        "        # DO NOT MODFIY THIS CODE\n",
        "        validate_shakespeare_generator(self)\n",
        "\n",
        "        return self.train_dataset, self.tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix3R8ewARgFY"
      },
      "source": [
        "# Modelling\n",
        "\n",
        "\n",
        "Load the `TFGPT2LMHeadModel` from the `transformers` library, which is a pre-trained GPT-2 model. This model is then fine-tuned on the Shakespeare dataset you prepared in the previous section.\n",
        "\n",
        "You shall be performing the following steps:\n",
        "\n",
        "1. **Model Setup:**\n",
        "\n",
        "  * Load the pre-trained GPT-2 model using `TFGPT2LMHeadModel.from_pretrained(\"gpt2\")`.\n",
        "  * Configure the model for training using an Adam optimizer and `SparseCategoricalCrossentropy` loss function.\n",
        "  * Compile the model with the chosen optimizer and loss function. This prepares the model for training.\n",
        "2. **Training:**\n",
        "\n",
        "  * Call the `model.fit` method with the training dataset and specified number of epochs. This starts the fine-tuning process, where the model's parameters are adjusted to better fit the Shakespeare dataset.\n",
        "  * The training history is stored and can be used to monitor the model's performance during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Nda5N25QsRB"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "\n",
        "def setup_model(generator, learning_rate=3e-5):\n",
        "    \"\"\"Model setup phase: Initialize and configure the model\"\"\"\n",
        "    # TODO: 1. Load pre-trained model\n",
        "    model = \n",
        "\n",
        "    # TODO: Configure the model for training\n",
        "    optimizer = \n",
        "    loss = \n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=[loss, *[None] * model.config.n_layer]\n",
        "    )\n",
        "\n",
        "    # DO NOT MODIFY THIS CODE\n",
        "    check_model_setup(generator, model)\n",
        "\n",
        "    return model\n",
        "\n",
        "def train(model, train_dataset, epochs=3):\n",
        "    # TODO: Fit the model\n",
        "    history = \n",
        "    \n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuVLrhmiRh5S"
      },
      "source": [
        "# Inference\n",
        "\n",
        "This is where the fine-tuned GPT-2 model is used to generate Shakespearean-like text based on a given prompt.\n",
        "\n",
        "Here are the steps to complete the inference code:\n",
        "\n",
        "\n",
        "1. Take a `prompt` (the starting text) and `max_length` (the maximum length of the generated text) as input.\n",
        "2. Use the `tokenizer` to encode the `prompt` into a numerical format that the model can understand.\n",
        "3. Call the `model.generate` method to generate text based on the encoded `prompt` and other parameters such as `max_length`, `num_return_sequences`, and `no_repeat_ngram_size`.\n",
        "4. Decode the generated output back into text using the `tokenizer`.\n",
        "5. Return the generated text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dr-gdaWQyw4"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "\n",
        "def generate(prompt, model, tokenizer, max_length=100):\n",
        "    \"\"\"Inference phase: Generate text from prompt\"\"\"\n",
        "    input_ids = \n",
        "    output = \n",
        "    \n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llrkut6mRi6-"
      },
      "source": [
        "# Driver code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "nBuaZZ0M6cq_",
        "outputId": "f1139b59-8f69-4e91-b0e0-33f424e10c19"
      },
      "outputs": [],
      "source": [
        "\n",
        "def main(prompt):\n",
        "    # Initialize the data generator\n",
        "    shakespeare_gen = ShakespeareGenerator()\n",
        "\n",
        "    # Data preparation\n",
        "    print(\"Preparing data...\")\n",
        "    train_dataset, tokenizer = shakespeare_gen.prepare_data()\n",
        "\n",
        "    validate_shakespeare_generator(shakespeare_gen)\n",
        "\n",
        "    # Model setup\n",
        "    print(\"Setting up model...\")\n",
        "    model=setup_model(shakespeare_gen)\n",
        "\n",
        "    # Training\n",
        "    print(\"Training model...\")\n",
        "    history = train(model, train_dataset, epochs=1)\n",
        "\n",
        "    # Inference\n",
        "    print(\"\\nGenerating text from prompt:\", prompt)\n",
        "    generated_text = generate(prompt, model, tokenizer)\n",
        "    print(f\"\\nGenerated text:\\n{generated_text}\")\n",
        "\n",
        "\n",
        "if __name__=='__main__':\n",
        "  main(prompt = \"To be or not to be\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWPJ5TEUkbF0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7LtYtBgkbDq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zp7PwR0bkbBh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdKXf4Epka_T"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_hZjfTYywXF"
   },
   "source": [
    "# VAE (Graded)\n",
    "\n",
    "Welcome to your Variational Auto Encoders (required) programming assignment! You will build a **face generation** model using VAE. You will be using [CelebA](https://www.kaggle.com/datasets/jessicali9530/celeba-dataset) dataset which contains around 30k+ images of males and females.\n",
    "\n",
    "Your goal is to build a robust face image generator model using Variational Auto Encoders.\n",
    "\n",
    "**Instructions:**\n",
    "* Do not modify any of the codes.\n",
    "* Only write code when prompted. For example in some sections you will find the following,\n",
    "  ```\n",
    "  # YOUR CODE GOES HERE\n",
    "  # YOUR CODE STARTS HERE\n",
    "  # TODO\n",
    "  ```\n",
    "Only modify those sections of the code.\n",
    "\n",
    "**You will learn to:**\n",
    "* Understanding VAEs including encoder, decoder, latent space and reparameterization trick.\n",
    "* Preprocessing the images for training\n",
    "  * Practicing data preprocessing steps like resizing, normalization, and batching.\n",
    "* Implementing the VAE loss function which consists of reconstruction and KL Divergence loss.\n",
    "* Training and Generating Images:\n",
    "  * Training the VAE model on the CelebA dataset.\n",
    "  * You will learn how to generate new images by sampling random points from the latent space and using the decoder to create corresponding images.\n",
    "* Using Convolutional Layers:\n",
    "  * You will learn how to use Convolutional and Transposed Convolutional layers for effective image processing within the encoder and decoder networks.\n",
    "\n",
    "\n",
    "  <img src='https://user-images.githubusercontent.com/17472642/69832523-e1720c00-11fc-11ea-96a3-df39b8c73a4c.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLkuz_9_2SUd"
   },
   "source": [
    "# Data Loading and Preprocessing\n",
    "\n",
    "**Instructions:**\n",
    "1. Set up the training directory path\n",
    "2. Load the dataset using `tf.keras.preprocessing.image_dataset_from_directory`\n",
    "    * Set `image_size` and `batch_size`\n",
    "3. Create a normalization function to scale pixel values to [0, 1]\n",
    "4. Apply normalization and set up dataset prefetching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_HF0QevMp8i9",
    "outputId": "b9d0d9a0-83c4-4f0b-a93c-5fbc2c9f7b36"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "import tensorflow as tf\n",
    "from tests import *\n",
    "from helpers import * \n",
    "\n",
    "validator = VAEValidator()\n",
    "\n",
    "# (TODO)1. Setup the training directory path\n",
    "train_dir = 'celeba_hq/train'\n",
    "\n",
    "# (TODO)2. Load the training dataset\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels=None,  # Set labels=None since we only care about the images\n",
    "    image_size=   # Resize images (For eg: (64, 64))\n",
    "    batch_size= # Set the batch size\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# (TODO)3. Normalize the images to [0, 1] range\n",
    "def normalize(image):\n",
    "    # YOUR CODE GOES HERE\n",
    "    return image\n",
    "\n",
    "# (TODO)3. Apply the normalization\n",
    "train_dataset = \n",
    "\n",
    "# 4.Prefetch to optimize dataset loading\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "validator.validate_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Wv7SgR53nBp"
   },
   "source": [
    "### But why do we prefetch the data?\n",
    "\n",
    "Prefetching Optimization for Data Loading\n",
    "\n",
    "* **Overlapping Computation and Data Loading:** Prefetching aims to overlap the preprocessing and model execution of a training step. While your model is busy training on a batch of data, the input pipeline in the background is already preparing the next batch. This helps reduce idle time for the GPU or CPU, leading to faster training.\n",
    "\n",
    "* **Improved Efficiency:** By keeping a buffer of prefetched data, prefetching ensures that your model doesn't have to wait for data to be loaded from disk or processed before it can start training on the next batch. This can significantly improve the overall training efficiency.\n",
    "\n",
    "* `tf.data.AUTOTUNE:` When you use tf.data.AUTOTUNE as the argument for prefetch, TensorFlow automatically determines the optimal buffer size for prefetching based on the characteristics of your dataset and hardware. This further optimizes the data loading process.\n",
    "\n",
    "* **In simpler terms:** Imagine a restaurant kitchen. Prefetching is like having the chefs prepare some ingredients in advance while the current dishes are being cooked. This way, when an order is ready, the chefs can quickly grab the prepped ingredients and start cooking the next dish without delay. It keeps the kitchen running smoothly and efficiently.\n",
    "\n",
    "[Learn more](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/data_performance.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P47bQjAV3he3"
   },
   "source": [
    "# Model Building\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_bLvWuq5VNA"
   },
   "source": [
    "## Encoder Network\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Define the latent dimension for the VAE\n",
    "2. Lets suppose your image_size is 64, create an encoder that:\n",
    "   - Takes `64x64x3` images as input\n",
    "   - Uses `Conv2D` layers with increasing filters (32, 64, 128)\n",
    "   - Outputs `z_mean` and `z_log_var` for the latent space\n",
    "3. The architecture should follow atleast 3 `Conv2D` layers followed by a `Flatten` and a final `Dense` layer:\n",
    "```\n",
    "Conv2D -> Conv2D -> Conv2D -> Flatten -> Dense\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RjVUIQskH68Q"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# (TODO)Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# (TODO)Define the encoder\n",
    "def build_encoder(input_shape, latent_dim):\n",
    "    # YOUR CODE GOES HERE\n",
    "    inputs = \n",
    "\n",
    "    z_mean = \n",
    "    z_log_var = \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return models.Model(inputs, [z_mean, z_log_var], name=\"encoder\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ywhY73hIf3y"
   },
   "source": [
    "## Sampling Layer\n",
    "\n",
    "In a VAE, we represent each latent variable $z$ as a Gaussian distribution with a mean $\\mu$ and a standard deviation \n",
    "$\\sigma$ (which can be parameterized by $log(\\sigma^2)$ or log variance for numerical stability). The formula for sampling is:\n",
    "\n",
    "$$\n",
    "z=\\mu+\\sigma.\\epsilon\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\mu$ and $log(\\sigma^2)$ (log variance) are outputs from the encoder network.\n",
    "* $\\sigma$ = $exp((0.5).log(\\sigma^2))$ is the standard deviation\n",
    "* $\\epsilon$ is a random noise sampled from a standard normal distribution. $\\epsilon \\approx N(0,I)$ where $I$ is the identity matrix. \n",
    "\n",
    "**Instructions:**\n",
    "1. Create a custom layer that implements the reparameterization trick\n",
    "2. The layer should take `z_mean` and `z_log_var` as inputs\n",
    "3. Return sampled points from the latent space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqKhbPRyIo7g"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# (TODO) Complete the following code\n",
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        # YOUR CODE GOES HERE\n",
    "        z_mean, z_log_var = \n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon =  # Sample epsilon\n",
    "        return  # Reparameterization trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7ZQ3eK4JGEE"
   },
   "source": [
    "## Decoder Network\n",
    "\n",
    "**Instructions:**\n",
    "1. Create a decoder that:\n",
    "   - Takes latent vectors as input\n",
    "   - Uses `Conv2DTranspose` layers to upsample\n",
    "   - Outputs reconstructed images of size `64x64x3`\n",
    "2. The architecture should mirror the encoder in reverse\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1JieM9TKaLc"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "def build_decoder():\n",
    "    # YOUR CODE GOES HERE\n",
    "    inputs = \n",
    "    outputs = \n",
    "    return models.Model(inputs, outputs, name=\"decoder\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ifd4QtEtKcjx"
   },
   "source": [
    "# VAE Loss function\n",
    "\n",
    "The loss function in a Variational Autoencoder (VAE) combines two components:\n",
    "\n",
    "**Reconstruction Loss:** This measures how well the VAE can reconstruct the input data from the latent space, ensuring that the generated output is similar to the input.\n",
    "$$\n",
    "Reconstruction Loss = MSE(x, \\hat{x})\n",
    "$$\n",
    "where: \n",
    "* $x$ is the input data\n",
    "* $\\hat{x}$ is the reconstructed data\n",
    "\n",
    "**KL Divergence Loss:** This regularizes the latent space to follow a standard normal distribution, encouraging the model to generate meaningful samples.\n",
    "$$\n",
    "KL Divergence Loss = -\\frac{1}{2} \\sum_{i=1}^{d} \\left( 1 + \\log(\\sigma_i^2) - \\mu_i^2 - \\sigma_i^2 \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\mu$ and $\\sigma^2$(or equivalently, $log(\\sigma^2)$) are the mean and variance of the latent distribution for a given input, as produced by the encoder.\n",
    "\n",
    "**Instructions:**\n",
    "1. Implement both **reconstruction loss (MSE) and KL divergence loss**\n",
    "2. Combine both losses to create the final VAE loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N9WfooV5Kvnh"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# VAE Loss Function\n",
    "def vae_loss(inputs, outputs, z_mean, z_log_var):\n",
    "    reconstruction_loss = \n",
    "    \n",
    "    kl_loss = \n",
    "    return reconstruction_loss + kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OX0ALDssLB6G"
   },
   "source": [
    "## Build VAE model\n",
    "\n",
    "**Instructions:**\n",
    "1. Combine the encoder, sampling layer, and decoder\n",
    "2. Implement the custom training step in `VAEModel` class. Inside the following steps inside `train_method`:\n",
    "  - `tf.GradientTape`: Record operations for automatic differentiation to calculate gradients.\n",
    "  - `Forward Pass`: Pass the input data through the encoder, sampling layer, and decoder to get the model's output.\n",
    "  - `Loss Calculation`: Compute the VAE loss (reconstruction loss + KL divergence).\n",
    "  - `Gradient Calculation`: Use tape.gradient to obtain gradients of the loss with respect to the model's trainable variables.\n",
    "  - `Optimizer Update`: Apply the gradients to update the model's weights using the chosen optimizer.\n",
    "  - **Return loss:** A dictionary containing the **loss** value for monitoring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBqbUmCPMV6C"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# (TODO)Define the VAE hyperparameters\n",
    "input_shape = \n",
    "latent_dim =  # Start with 50 and increase it gradually\n",
    "\n",
    "# (TODO)Build Encoder\n",
    "encoder = \n",
    "validator.validate_encoder(encoder, input_shape, latent_dim)\n",
    "\n",
    "# (TODO)Build Decoder\n",
    "decoder = \n",
    "validator.validate_decoder(decoder, latent_dim, input_shape)\n",
    "\n",
    "inputs = layers.Input(shape=input_shape)\n",
    "z_mean, z_log_var = encoder(inputs)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "outputs = decoder(z)\n",
    "\n",
    "# (TODO)Custom model training\n",
    "class VAEModel(tf.keras.Model):\n",
    "    def train_step(self, data):\n",
    "      # (TODO) Perform all the operations mentioned above\n",
    "        with tf.GradientTape() as tape:\n",
    "            # YOUR CODE GOES HERE\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return {\"loss\": }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9M--mpBPMONf"
   },
   "source": [
    "# Model Training\n",
    "\n",
    "**Instructions:**\n",
    "1. Create the VAE model instance\n",
    "2. Compile the model\n",
    "3. Train the model for as many epochs as you wish\n",
    "4. Monitor the loss during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "H0oK7cQMpmNF",
    "outputId": "12267ddb-7c4c-4185-a2a6-1801a53b1158"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "num_epochs = \n",
    "\n",
    "# (TODO)1. Create the VAE model instance\n",
    "\n",
    "\n",
    "# (TODO)2. Compile the model with appropriate optimizer\n",
    "\n",
    "\n",
    "# (TODO)Train the VAE model on CelebA\n",
    "history = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection**\n",
    "\n",
    "Write your observations here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5FVhVyt1pmK3"
   },
   "outputs": [],
   "source": [
    "plot_faces(latent_dim, decoder, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyaNlMDNepsB"
   },
   "source": [
    "# Improvement Strategies\n",
    "\n",
    "Here are some improvement strategies you can consider to improve the model.\n",
    "\n",
    "1. **Enhanced VAE Architecture**\n",
    "  * **Deeper and Wider Network Layers**:\n",
    "\n",
    "    * Increase the number of convolutional layers in the encoder and decoder.\n",
    "    * Use higher filter sizes for earlier layers to capture more facial details.\n",
    "    * Example: Start with 32 filters in the first layer and double the number of filters with each additional layer (e.g., 32 → 64 → 128 → 256).\n",
    "\n",
    "  * **Residual Connections:**\n",
    "\n",
    "    * Use residual blocks (skip connections) in both encoder and decoder to help the model capture finer details.\n",
    "    * This prevents loss of information as data passes through deeper layers, aiding better reconstruction.\n",
    "\n",
    "  * **Leverage Upsampling Layers in Decoder:**\n",
    "\n",
    "    * Instead of Conv2DTranspose, use a combination of UpSampling2D and Conv2D layers. This can often improve the quality of generated images, especially for faces, as it helps retain image detail.\n",
    "2. **Optimize the Latent Space**\n",
    "  * **Increase Latent Dimension:**\n",
    "    * Increase the latent dimension size from 50 to a higher value like 100 or 128, giving the model more capacity to encode complex face details.\n",
    "\n",
    "  * **Beta-VAE:**\n",
    "    * Scale the KL-divergence term with a coefficient (beta) in the VAE loss function to control the trade-off between reconstruction quality and latent space regularization. A larger beta (e.g., 4 to 10) enforces a more structured latent space but may reduce reconstruction quality. Find a balance through experimentation.\n",
    "3. **Regularization Techniques**\n",
    "  * **Dropout:**\n",
    "  Add dropout layers in both the encoder and decoder to prevent overfitting. Dropout rates between 0.3 to 0.5 can help generalize better, especially for large datasets like CelebA.\n",
    "\n",
    "  * **Batch Normalization:**\n",
    "  Add batch normalization after each convolutional layer, particularly in the encoder. This can stabilize training and help the model learn faster.\n",
    "\n",
    "4. **Improving Loss Functions**\n",
    "  * **Perceptual Loss (Content Loss):**\n",
    "\n",
    "    * Instead of relying solely on pixel-wise mean squared error for reconstruction, use a perceptual loss. Compute this loss using a pre-trained model (like VGG) on the reconstructed images and the original images, comparing them at higher feature levels.\n",
    "\n",
    "  This helps the model capture high-level features, like facial structures and expressions.\n",
    "\n",
    "5. **Training Techniques**\n",
    "  * **Learning Rate Scheduler:**\n",
    "\n",
    "    * Use a learning rate scheduler that decays the learning rate as training progresses, such as the cosine decay or exponential decay scheduler.\n",
    "  * **Data Augmentation:**\n",
    "\n",
    "    * Add slight data augmentations (like horizontal flipping, random cropping, or brightness adjustments) to introduce diversity and help the model generalize better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwYlWlPQraYu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AvuM6y6rraWz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTViQrGvraT9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJzNGzJ1mOoC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "history_visible": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>Large Language Models (LLMs) with OpenAI API</h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with LLMs (OpenAI API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create an OpenAI Account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Visit OpenAI's website (https://openai.com)\n",
    "2. Click on \"Sign Up\" or \"Get Started\"\n",
    "3. Enter your email address\n",
    "4. Create a secure password\n",
    "5. Verify your email through the confirmation link sent to your inbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Complete Account Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fill in your user profile information\n",
    "2. Provide your full name\n",
    "3. Set up two-factor authentication (recommended)\n",
    "4. Complete any additional verification steps if prompted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Set Up Billing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Navigate to the API section in your dashboard\n",
    "2. Click on \"Billing\" in the left sidebar\n",
    "3. Click \"Set up paid account\"\n",
    "4. Add a payment method (credit card required)\n",
    "5. Set usage limits (recommended to avoid unexpected charges)\n",
    "6. Consider setting a monthly spending cap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate API Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Go to the API Keys section in your dashboard\n",
    "2. Click \"Create new secret key\"\n",
    "3. Give your key a descriptive name\n",
    "4. Copy and save the API key immediately (it won't be shown again)\n",
    "5. Store the key securely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build a simple chatbot that takes user input and fetches the response from an LLM Model through OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Imports\n",
    "import os\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Add API Key to the environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code adds the code to the environment variables. You need to add you API key here generated in Step 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = 'ADD YOUR OPEN API KEY HERE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Interacting through code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ChatAssistant` class provides a wrapper for interacting with OpenAI's Chat API. Here's a detailed breakdown of its components and available parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class ChatAssistant:\n",
    "    def __init__(self):\n",
    "        self.client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "```\n",
    "\n",
    "The constructor initializes the OpenAI client using an API key stored in environment variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Method: get_chat_response()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def get_chat_response(self, prompt, model=\"gpt-3.5-turbo\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Current Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `prompt`: The user's input text (required)\n",
    "- `model`: The GPT model to use (default: \"gpt-3.5-turbo\")\n",
    "- `temperature`: Controls randomness (0.7 in current code)\n",
    "- `max_tokens`: Maximum response length (150 in current code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Available Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Messages Parameters**\n",
    "```python\n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "```\n",
    "- `role`: Can be \"system\", \"user\", or \"assistant\"\n",
    "- `content`: The message content\n",
    "- `name`: Optional identifier for the message sender\n",
    "\n",
    "2. **Generation Parameters**\n",
    "```python\n",
    "# Optional parameters you can add to chat.completions.create()\n",
    "response = self.client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    temperature=0.7,           # 0 to 2 (default: 1)\n",
    "    top_p=1,                   # 0 to 1 (default: 1)\n",
    "    n=1,                       # Number of completions to generate\n",
    "    stream=False,              # Stream responses or not\n",
    "    stop=None,                 # Custom stop sequences\n",
    "    presence_penalty=0,        # -2.0 to 2.0\n",
    "    frequency_penalty=0,       # -2.0 to 2.0\n",
    "    logit_bias={},            # Token bias dictionary\n",
    "    user=\"user_123\",          # User identifier\n",
    "    response_format=None,     # Specify response format\n",
    "    seed=None                 # Random seed for reproducibility\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Available Parameter Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Temperature\n",
    "- **Range**: 0 to 2\n",
    "- **Default**: 1\n",
    "- **Purpose**: Controls randomness in responses\n",
    "  - 0: More focused and deterministic\n",
    "  - 1: Balanced\n",
    "  - 2: More random and creative\n",
    "\n",
    "2. Top P (nucleus sampling)\n",
    "- **Range**: 0 to 1\n",
    "- **Default**: 1\n",
    "- **Purpose**: Alternative to temperature for controlling response diversity\n",
    "\n",
    "3. Presence Penalty\n",
    "- **Range**: -2.0 to 2.0\n",
    "- **Default**: 0\n",
    "- **Purpose**: Penalizes new tokens based on their presence in the text so far\n",
    "  - Positive values encourage talking about new topics\n",
    "  - Negative values encourage focusing on existing topics\n",
    "\n",
    "4. Frequency Penalty\n",
    "- **Range**: -2.0 to 2.0\n",
    "- **Default**: 0\n",
    "- **Purpose**: Penalizes tokens based on their frequency in the text so far\n",
    "  - Positive values decrease repetition\n",
    "  - Negative values may increase repetition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gpt-4\n",
    "- gpt-4-turbo-preview\n",
    "- gpt-3.5-turbo\n",
    "- gpt-3.5-turbo-16k\n",
    "etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the working code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatAssistant:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize OpenAI client with API key from environment\"\"\"\n",
    "        self.client = OpenAI(\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "        )\n",
    "        \n",
    "    def get_chat_response(self, prompt, model=\"gpt-3.5-turbo\"):\n",
    "        \"\"\"Generate a chat completion response\"\"\"\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.7,\n",
    "                max_tokens=150\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Initialize assistant\n",
    "    assistant = ChatAssistant()\n",
    "    \n",
    "    # Query\n",
    "    prompt = \"Which team won the 2019 cricket world cup?\"\n",
    "    response = assistant.get_chat_response(prompt)\n",
    "    print(f\"User: {prompt}\")\n",
    "    print(f\"Assistant: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

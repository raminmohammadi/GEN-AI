{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Advanced Question-Answering System: Integrating RAG with Context-Aware Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: Why Combine RAG and Context Awareness?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine having a conversation with someone who not only has access to a vast library of information (RAG) but also remembers and learns from your ongoing conversation (Context Awareness). By combining Retrieval-Augmented Generation (RAG) with a context-aware chatbot, we create a system that can provide both accurate, source-based answers and maintain coherent, contextual conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore how these two powerful approaches complement each other:\n",
    "\n",
    "- **RAG**: Provides accurate, source-based information retrieval and response generation\n",
    "- **Context Awareness**: Maintains conversation history and understands ongoing dialogue flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Combined Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our integrated system will have the following components:\n",
    "\n",
    "1. RAG Components (from previous implementation)\n",
    "   - Document Processor\n",
    "   - Embedding Engine\n",
    "   - Retrieval System\n",
    "   \n",
    "2. Context-Aware Components (new additions)\n",
    "   - Memory Cache\n",
    "   - Context Manager\n",
    "   - Response Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement this integrated system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Dict, List, Optional, Any, Tuple\n",
    "from datetime import datetime, timedelta\n",
    "from openai import OpenAI\n",
    "from hashlib import md5\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    \"\"\"\n",
    "    Handles document preprocessing and chunking for the RAG system.\n",
    "    This component prepares documents for efficient retrieval and context management.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 chunk_size: int = 500,\n",
    "                 chunk_overlap: int = 50,\n",
    "                 min_chunk_size: int = 100):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.min_chunk_size = min_chunk_size\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and normalize text content for better processing.\n",
    "        Handles common text issues like extra whitespace and formatting.\n",
    "        \"\"\"\n",
    "        # Remove extra whitespace and normalize line endings\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'\\n+', '\\n', text)\n",
    "        \n",
    "        # Remove special characters while preserving essential punctuation\n",
    "        text = re.sub(r'[^\\w\\s.,!?;:()\\[\\]{}\"\\'`-]', '', text)\n",
    "        \n",
    "        # Normalize sentence endings\n",
    "        text = re.sub(r'([.!?])\\s*', r'\\1\\n', text)\n",
    "        \n",
    "        return text.strip()\n",
    "\n",
    "    def create_chunks(self, \n",
    "                     document: Dict[str, str], \n",
    "                     respect_sentences: bool = True) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Break document into chunks while preserving semantic meaning.\n",
    "        \"\"\"\n",
    "        text = self.clean_text(document['text'])\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        \n",
    "        # Split into sentences if respecting sentence boundaries\n",
    "        sentences = text.split('\\n') if respect_sentences else [text]\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "                \n",
    "            sentence_length = len(sentence)\n",
    "            \n",
    "            # Check if adding this sentence exceeds chunk size\n",
    "            if current_length + sentence_length > self.chunk_size:\n",
    "                if current_length >= self.min_chunk_size:\n",
    "                    chunk_text = ' '.join(current_chunk)\n",
    "                    chunks.append({\n",
    "                        'text': chunk_text,\n",
    "                        'document_id': document.get('id', 'unknown'),\n",
    "                        'chunk_size': len(chunk_text),\n",
    "                        'position': len(chunks)\n",
    "                    })\n",
    "                \n",
    "                # Start new chunk with overlap\n",
    "                if current_chunk and self.chunk_overlap > 0:\n",
    "                    overlap_text = ' '.join(current_chunk[-2:])\n",
    "                    current_chunk = [overlap_text, sentence]\n",
    "                    current_length = len(overlap_text) + sentence_length\n",
    "                else:\n",
    "                    current_chunk = [sentence]\n",
    "                    current_length = sentence_length\n",
    "            else:\n",
    "                current_chunk.append(sentence)\n",
    "                current_length += sentence_length\n",
    "        \n",
    "        # Add final chunk if it meets minimum size\n",
    "        if current_chunk and current_length >= self.min_chunk_size:\n",
    "            chunk_text = ' '.join(current_chunk)\n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'document_id': document.get('id', 'unknown'),\n",
    "                'chunk_size': len(chunk_text),\n",
    "                'position': len(chunks)\n",
    "            })\n",
    "            \n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Explanation:**\n",
    "\n",
    "Let us break down the DocumentProcessor class and explain each component in detail:\n",
    "\n",
    "1. **Class Purpose**: \n",
    "    The DocumentProcessor class is responsible for preparing documents for use in the RAG system. Its main job is to:\n",
    "    - Clean up text documents\n",
    "    - Break large documents into manageable pieces (chunks)\n",
    "    - Maintain semantic coherence while chunking\n",
    "\n",
    "2. **Constructor (__init__)**: \n",
    "    ```python\n",
    "    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50, min_chunk_size: int = 100):\n",
    "    ```\n",
    "    Parameters:\n",
    "    - `chunk_size`: Maximum size of each text chunk (default 500 characters)\n",
    "    - `chunk_overlap`: How much text should overlap between chunks (default 50 characters)\n",
    "    - `min_chunk_size`: Minimum acceptable chunk size (default 100 characters)\n",
    "\n",
    "3. **Text Cleaning Method (clean_text)**:\n",
    "    ```python\n",
    "    def clean_text(self, text: str) -> str:\n",
    "    ```\n",
    "    This method sanitizes the input text through several steps:\n",
    "\n",
    "    a. Whitespace Normalization:\n",
    "    ```python\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Converts multiple spaces into single space\n",
    "    text = re.sub(r'\\n+', '\\n', text) # Normalizes multiple newlines\n",
    "    ```\n",
    "\n",
    "    b. Character Cleaning:\n",
    "    ```python\n",
    "    text = re.sub(r'[^\\w\\s.,!?;:()\\[\\]{}\"\\'`-]', '', text)\n",
    "    ```\n",
    "    - Keeps: alphanumeric characters, spaces, common punctuation\n",
    "    - Removes: special characters, emojis, unusual symbols\n",
    "\n",
    "    c. Sentence Normalization:\n",
    "    ```python\n",
    "    text = re.sub(r'([.!?])\\s*', r'\\1\\n', text)  # Ensures each sentence ends with newline\n",
    "    ```\n",
    "\n",
    "4. **Chunking Method (create_chunks)**:\n",
    "    ```python\n",
    "    def create_chunks(self, document: Dict[str, str], respect_sentences: bool = True)\n",
    "    ```\n",
    "\n",
    "    This method implements a sophisticated chunking strategy:\n",
    "\n",
    "    a. Initial Setup:\n",
    "    ```python\n",
    "    text = self.clean_text(document['text'])\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    ```\n",
    "\n",
    "    b. Sentence Processing:\n",
    "    ```python\n",
    "    sentences = text.split('\\n') if respect_sentences else [text]\n",
    "    ```\n",
    "    - Splits text into sentences if respect_sentences is True\n",
    "    - Keeps text as one piece if False\n",
    "\n",
    "    c. Chunk Creation Logic:\n",
    "    ```python\n",
    "    if current_length + sentence_length > self.chunk_size:\n",
    "        if current_length >= self.min_chunk_size:\n",
    "            chunk_text = ' '.join(current_chunk)\n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'document_id': document.get('id', 'unknown'),\n",
    "                'chunk_size': len(chunk_text),\n",
    "                'position': len(chunks)\n",
    "            })\n",
    "    ```\n",
    "    - Checks if adding new sentence exceeds chunk_size\n",
    "    - Creates new chunk if current chunk is large enough\n",
    "    - Includes metadata (document ID, size, position)\n",
    "\n",
    "    d. Overlap Handling:\n",
    "    ```python\n",
    "    if current_chunk and self.chunk_overlap > 0:\n",
    "        overlap_text = ' '.join(current_chunk[-2:])\n",
    "        current_chunk = [overlap_text, sentence]\n",
    "    ```\n",
    "    - Maintains context between chunks\n",
    "    - Takes last two sentences from previous chunk\n",
    "    - Adds them to beginning of new chunk\n",
    "\n",
    "5. **Key Features**:\n",
    "- **Context Preservation**: Overlapping chunks maintain contextual connections\n",
    "- **Metadata Tracking**: Each chunk knows its source and position\n",
    "- **Flexible Sizing**: Configurable chunk sizes and overlaps\n",
    "- **Sentence Respect**: Avoids breaking sentences in middle (unless specified)\n",
    "\n",
    "6. **Usage Example**:\n",
    "    ```python\n",
    "    processor = DocumentProcessor(chunk_size=300, chunk_overlap=30)\n",
    "    document = {\n",
    "        'id': 'doc1',\n",
    "        'text': 'Your long document text here...'\n",
    "    }\n",
    "    chunks = processor.create_chunks(document)\n",
    "    ```\n",
    "\n",
    "This class is crucial for RAG systems because it:\n",
    "- Makes large documents manageable\n",
    "- Preserves context across chunks\n",
    "- Maintains clean, consistent text format\n",
    "- Enables efficient retrieval operations\n",
    "\n",
    "The careful text processing and chunking ensure that the RAG system can effectively store and retrieve relevant information while maintaining semantic coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingEngine:\n",
    "    \"\"\"\n",
    "    Creates and manages vector representations of text.\n",
    "    Provides methods for computing embeddings and measuring text similarity.\n",
    "    \"\"\"\n",
    "    def __init__(self, dimensions: int = 768, cache_size: int = 10000):\n",
    "        self.dimensions = dimensions\n",
    "        self.cache_size = cache_size\n",
    "        self.cache = {}\n",
    "        self.word_vectors = {}\n",
    "    \n",
    "    def _create_word_vector(self, word: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create a deterministic vector for a word using hashing.\n",
    "        This ensures consistent vector representations across sessions.\n",
    "        \"\"\"\n",
    "        if word in self.word_vectors:\n",
    "            return self.word_vectors[word]\n",
    "        \n",
    "        # Create deterministic seed from word hash\n",
    "        word_hash = int(md5(word.encode()).hexdigest()[:8], 16)\n",
    "        np.random.seed(word_hash)\n",
    "        \n",
    "        # Create and normalize vector\n",
    "        vector = np.random.randn(self.dimensions)\n",
    "        vector = vector / np.linalg.norm(vector)\n",
    "        \n",
    "        self.word_vectors[word] = vector\n",
    "        return vector\n",
    "    \n",
    "    def compute_embedding(self, text: str, use_cache: bool = True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert text into a vector representation using weighted word vectors.\n",
    "        \"\"\"\n",
    "        if use_cache and text in self.cache:\n",
    "            return self.cache[text]\n",
    "        \n",
    "        words = text.lower().split()\n",
    "        if not words:\n",
    "            return np.zeros(self.dimensions)\n",
    "        \n",
    "        # Initialize embedding with position-weighted word vectors\n",
    "        embedding = np.zeros(self.dimensions)\n",
    "        word_count = {}\n",
    "        \n",
    "        for position, word in enumerate(words):\n",
    "            word_count[word] = word_count.get(word, 0) + 1\n",
    "            word_vector = self._create_word_vector(word)\n",
    "            position_weight = 1.0 / (1 + np.log1p(position))\n",
    "            embedding += word_vector * position_weight\n",
    "        \n",
    "        # Apply IDF-like scaling based on word frequency\n",
    "        for word, count in word_count.items():\n",
    "            scaling = 1.0 / np.sqrt(count)\n",
    "            word_vector = self._create_word_vector(word)\n",
    "            embedding += word_vector * scaling\n",
    "        \n",
    "        # Normalize the final embedding\n",
    "        embedding_norm = np.linalg.norm(embedding)\n",
    "        if embedding_norm > 0:\n",
    "            embedding = embedding / embedding_norm\n",
    "        \n",
    "        # Cache the result if enabled\n",
    "        if use_cache:\n",
    "            if len(self.cache) >= self.cache_size:\n",
    "                self.cache.pop(next(iter(self.cache)))\n",
    "            self.cache[text] = embedding\n",
    "        \n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Explanation:**\n",
    "\n",
    "Let us break down the EmbeddingEngine class and explain each component:\n",
    "\n",
    "1. **Class Initialization**:\n",
    "    ```python\n",
    "    def __init__(self, dimensions: int = 768, cache_size: int = 10000):\n",
    "        self.dimensions = dimensions    # Size of embedding vectors\n",
    "        self.cache_size = cache_size    # Maximum number of cached embeddings\n",
    "        self.cache = {}                 # Storage for computed embeddings\n",
    "        self.word_vectors = {}          # Storage for individual word vectors\n",
    "    ```\n",
    "    - `dimensions`: Controls the size of embedding vectors (default 768, similar to BERT)\n",
    "    - `cache_size`: Limits memory usage by storing only recent embeddings\n",
    "    - Two caching systems:\n",
    "    * `cache`: Stores complete text embeddings\n",
    "    * `word_vectors`: Stores individual word vectors\n",
    "\n",
    "2. **Word Vector Creation (_create_word_vector)**:\n",
    "    ```python\n",
    "    def _create_word_vector(self, word: str) -> np.ndarray:\n",
    "    ```\n",
    "    This private method creates consistent vector representations for words:\n",
    "\n",
    "    a. Caching Check:\n",
    "    ```python\n",
    "    if word in self.word_vectors:\n",
    "        return self.word_vectors[word]\n",
    "    ```\n",
    "    - Returns cached vector if available\n",
    "\n",
    "    b. Vector Generation:\n",
    "    ```python\n",
    "    word_hash = int(md5(word.encode()).hexdigest()[:8], 16)\n",
    "    np.random.seed(word_hash)\n",
    "    ```\n",
    "    - Creates deterministic hash from word\n",
    "    - Uses hash as random seed for consistency\n",
    "\n",
    "    c. Vector Creation and Normalization:\n",
    "    ```python\n",
    "    vector = np.random.randn(self.dimensions)\n",
    "    vector = vector / np.linalg.norm(vector)\n",
    "    ```\n",
    "    - Generates random normal vector\n",
    "    - Normalizes to unit length\n",
    "    - Caches for future use\n",
    "\n",
    "3. **Text Embedding Computation (compute_embedding)**:\n",
    "    ```python\n",
    "    def compute_embedding(self, text: str, use_cache: bool = True) -> np.ndarray:\n",
    "    ```\n",
    "    This main method converts text into vector representation:\n",
    "\n",
    "    a. Cache Checking:\n",
    "    ```python\n",
    "    if use_cache and text in self.cache:\n",
    "        return self.cache[text]\n",
    "    ```\n",
    "    - Returns cached embedding if available\n",
    "\n",
    "    b. Text Processing:\n",
    "    ```python\n",
    "    words = text.lower().split()\n",
    "    if not words:\n",
    "        return np.zeros(self.dimensions)\n",
    "    ```\n",
    "    - Converts text to lowercase words\n",
    "    - Handles empty text case\n",
    "\n",
    "    c. Position-Weighted Embedding:\n",
    "    ```python\n",
    "    for position, word in enumerate(words):\n",
    "        word_count[word] = word_count.get(word, 0) + 1\n",
    "        word_vector = self._create_word_vector(word)\n",
    "        position_weight = 1.0 / (1 + np.log1p(position))\n",
    "        embedding += word_vector * position_weight\n",
    "    ```\n",
    "    Key features:\n",
    "    - Tracks word frequency\n",
    "    - Applies position-based weighting (earlier words get higher weight)\n",
    "    - Accumulates weighted vectors\n",
    "\n",
    "    d. Frequency Scaling:\n",
    "    ```python\n",
    "    for word, count in word_count.items():\n",
    "        scaling = 1.0 / np.sqrt(count)\n",
    "        word_vector = self._create_word_vector(word)\n",
    "        embedding += word_vector * scaling\n",
    "    ```\n",
    "    - Applies IDF-like scaling\n",
    "    - Reduces impact of frequent words\n",
    "    - Enhances impact of rare words\n",
    "\n",
    "    e. Final Normalization:\n",
    "    ```python\n",
    "    embedding_norm = np.linalg.norm(embedding)\n",
    "    if embedding_norm > 0:\n",
    "        embedding = embedding / embedding_norm\n",
    "    ```\n",
    "    - Normalizes final vector to unit length\n",
    "    - Ensures consistent scaling\n",
    "\n",
    "    f. Cache Management:\n",
    "    ```python\n",
    "    if use_cache:\n",
    "        if len(self.cache) >= self.cache_size:\n",
    "            self.cache.pop(next(iter(self.cache)))\n",
    "        self.cache[text] = embedding\n",
    "    ```\n",
    "    - Maintains cache size limit\n",
    "    - Removes oldest entries when full\n",
    "    - Stores new embedding\n",
    "\n",
    "4. **Key Features and Benefits**:\n",
    "- **Deterministic**: Same text always produces same embedding\n",
    "- **Position-Aware**: Considers word order importance\n",
    "- **Frequency-Aware**: Balances common and rare words\n",
    "- **Memory-Efficient**: Uses caching with size limits\n",
    "- **Normalized Output**: Consistent vector magnitudes\n",
    "- **Fast Retrieval**: Cached results for repeated texts\n",
    "\n",
    "5. **Usage Example**:\n",
    "    ```python\n",
    "    engine = EmbeddingEngine(dimensions=300)\n",
    "    text = \"Example text for embedding\"\n",
    "    vector = engine.compute_embedding(text)\n",
    "    ```\n",
    "\n",
    "This class is crucial for RAG because it:\n",
    "- Converts text to numerical form for similarity comparison\n",
    "- Maintains consistency in vector representations\n",
    "- Optimizes performance through caching\n",
    "- Considers both word position and frequency\n",
    "- Enables efficient semantic search operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryCache:\n",
    "    \"\"\"\n",
    "    Manages conversation history and context for the chatbot.\n",
    "    Implements time-based memory management and context retrieval.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_age_hours: int = 24):\n",
    "        self.max_age = timedelta(hours=max_age_hours)\n",
    "        self.memories = []\n",
    "        \n",
    "    def add_memory(self, \n",
    "                   user_input: str, \n",
    "                   system_response: str,\n",
    "                   retrieved_context: Optional[List[Dict]] = None) -> None:\n",
    "        \"\"\"\n",
    "        Store a new conversation interaction with its context.\n",
    "        \"\"\"\n",
    "        memory = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'user_input': user_input,\n",
    "            'system_response': system_response,\n",
    "            'retrieved_context': retrieved_context\n",
    "        }\n",
    "        self.memories.append(memory)\n",
    "        self._cleanup_old_memories()\n",
    "        self._save_memories()\n",
    "        \n",
    "    def get_recent_context(self, limit: int = 5) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Retrieve recent conversations for context.\n",
    "        \"\"\"\n",
    "        self._cleanup_old_memories()\n",
    "        return self.memories[-limit:]\n",
    "        \n",
    "    def _cleanup_old_memories(self) -> None:\n",
    "        \"\"\"Remove memories older than max_age.\"\"\"\n",
    "        current_time = datetime.now()\n",
    "        self.memories = [\n",
    "            mem for mem in self.memories \n",
    "            if (current_time - mem['timestamp']) < self.max_age\n",
    "        ]\n",
    "        \n",
    "    def _save_memories(self) -> None:\n",
    "        \"\"\"Save memories to persistent storage.\"\"\"\n",
    "        try:\n",
    "            with open('memories.json', 'w') as f:\n",
    "                serializable_memories = []\n",
    "                for mem in self.memories:\n",
    "                    mem_copy = mem.copy()\n",
    "                    mem_copy['timestamp'] = mem_copy['timestamp'].isoformat()\n",
    "                    serializable_memories.append(mem_copy)\n",
    "                json.dump({'memories': serializable_memories}, f)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving memories: {str(e)}\")\n",
    "\n",
    "    def _load_memories(self) -> None:\n",
    "        \"\"\"Load memories from persistent storage.\"\"\"\n",
    "        try:\n",
    "            with open('memories.json', 'r') as f:\n",
    "                data = json.load(f)\n",
    "                self.memories = []\n",
    "                for mem in data['memories']:\n",
    "                    mem['timestamp'] = datetime.fromisoformat(mem['timestamp'])\n",
    "                    self.memories.append(mem)\n",
    "        except FileNotFoundError:\n",
    "            self.memories = []\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading memories: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Explanation:**\n",
    "\n",
    "Let us break down the MemoryCache class and explain each component in detail:\n",
    "\n",
    "1. **Class Initialization**:\n",
    "    ```python\n",
    "    def __init__(self, max_age_hours: int = 24):\n",
    "        self.max_age = timedelta(hours=max_age_hours)\n",
    "        self.memories = []\n",
    "    ```\n",
    "    Purpose:\n",
    "    - Sets maximum age for stored memories (default 24 hours)\n",
    "    - Initializes empty list for storing conversations\n",
    "    - `max_age` determines how long conversations are kept\n",
    "\n",
    "2. **Adding New Memories**:\n",
    "    ```python\n",
    "    def add_memory(self, user_input: str, system_response: str, \n",
    "                retrieved_context: Optional[List[Dict]] = None):\n",
    "    ```\n",
    "    Purpose:\n",
    "    - Stores new conversation interactions\n",
    "    - Records:\n",
    "    * User's question/input\n",
    "    * System's response\n",
    "    * Retrieved context used for response\n",
    "    * Timestamp of interaction\n",
    "    - Triggers cleanup and saving processes\n",
    "\n",
    "3. **Getting Recent Context**:\n",
    "    ```python\n",
    "    def get_recent_context(self, limit: int = 5):\n",
    "    ```\n",
    "    Purpose:\n",
    "    - Retrieves most recent conversations\n",
    "    - Default returns last 5 interactions\n",
    "    - Used for maintaining conversation flow\n",
    "    - Cleans up old memories before retrieval\n",
    "\n",
    "4. **Memory Cleanup**:\n",
    "    ```python\n",
    "    def _cleanup_old_memories(self):\n",
    "    ```\n",
    "    Purpose:\n",
    "    - Removes conversations older than max_age\n",
    "    - Prevents memory from growing too large\n",
    "    - Maintains relevance of stored context\n",
    "    - Example: Removes conversations older than 24 hours\n",
    "\n",
    "5. **Saving Memories**:\n",
    "    ```python\n",
    "    def _save_memories(self):\n",
    "    ```\n",
    "    Purpose:\n",
    "    - Persists conversations to disk (memories.json)\n",
    "    - Handles datetime serialization\n",
    "    - Creates backup of conversation history\n",
    "    - Error handling for file operations\n",
    "\n",
    "    Key operations:\n",
    "    ```python\n",
    "    mem_copy['timestamp'] = mem_copy['timestamp'].isoformat()\n",
    "    json.dump({'memories': serializable_memories}, f)\n",
    "    ```\n",
    "\n",
    "6. **Loading Memories**:\n",
    "    ```python\n",
    "    def _load_memories(self):\n",
    "    ```\n",
    "    Purpose:\n",
    "    - Loads saved conversations from disk\n",
    "    - Restores conversation history\n",
    "    - Converts stored timestamps back to datetime\n",
    "    - Handles missing file and other errors\n",
    "\n",
    "    Key operations:\n",
    "    ```python\n",
    "    mem['timestamp'] = datetime.fromisoformat(mem['timestamp'])\n",
    "    self.memories.append(mem)\n",
    "    ```\n",
    "\n",
    "7. **Memory Structure**:\n",
    "    Each memory entry contains:\n",
    "    ```python\n",
    "    memory = {\n",
    "        'timestamp': datetime.now(),      # When interaction occurred\n",
    "        'user_input': user_input,         # What user asked\n",
    "        'system_response': system_response,# How system responded\n",
    "        'retrieved_context': retrieved_context # What context was used\n",
    "    }\n",
    "    ```\n",
    "\n",
    "8. **Important Features**:\n",
    "\n",
    "    a. Time-Based Management:\n",
    "    - Automatic cleanup of old conversations\n",
    "    - Timestamp-based organization\n",
    "    - Age-based filtering\n",
    "\n",
    "    b. Persistence:\n",
    "    - Saves conversations to disk\n",
    "    - Recovers from previous sessions\n",
    "    - Handles serialization issues\n",
    "\n",
    "    c. Context Retrieval:\n",
    "    - Quick access to recent conversations\n",
    "    - Limit control for context window\n",
    "    - Ordered by recency\n",
    "\n",
    "9. **Usage Example**:\n",
    "    ```python\n",
    "    # Initialize memory cache\n",
    "    memory = MemoryCache(max_age_hours=48)\n",
    "\n",
    "    # Add new conversation\n",
    "    memory.add_memory(\n",
    "        user_input=\"What is AI?\",\n",
    "        system_response=\"AI is artificial intelligence...\",\n",
    "        retrieved_context=[{\"source\": \"textbook\", \"content\": \"...\"}]\n",
    "    )\n",
    "\n",
    "    # Get recent conversations\n",
    "    recent_context = memory.get_recent_context(limit=3)\n",
    "    ```\n",
    "\n",
    "10. **Benefits for Chatbot System**:\n",
    "- Maintains conversation continuity\n",
    "- Enables context-aware responses\n",
    "- Provides persistence across sessions\n",
    "- Manages memory efficiently\n",
    "- Enables recovery from crashes/restarts\n",
    "- Maintains conversation relevance through aging\n",
    "\n",
    "The MemoryCache class is crucial for:\n",
    "- Contextual understanding\n",
    "- Conversation persistence\n",
    "- Memory management\n",
    "- Session recovery\n",
    "- Temporal relevance\n",
    "\n",
    "This creates a more natural and context-aware conversation experience while managing system resources effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalSystem:\n",
    "    \"\"\"\n",
    "    Manages document storage and retrieval using semantic search.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 embedding_engine: EmbeddingEngine,\n",
    "                 top_k: int = 5,\n",
    "                 similarity_threshold: float = 0.5):\n",
    "        self.embedding_engine = embedding_engine\n",
    "        self.top_k = top_k\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.chunk_vectors = []\n",
    "        self.chunks = []\n",
    "        \n",
    "    def add_chunk(self, \n",
    "                  chunk: Dict[str, Any],\n",
    "                  vector: Optional[np.ndarray] = None) -> None:\n",
    "        \"\"\"Add a document chunk and its vector to the system.\"\"\"\n",
    "        if vector is None:\n",
    "            vector = self.embedding_engine.compute_embedding(chunk['text'])\n",
    "            \n",
    "        self.chunk_vectors.append(vector)\n",
    "        self.chunks.append(chunk)\n",
    "        \n",
    "    def search(self, \n",
    "               query: str,\n",
    "               filters: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Search for relevant chunks based on query similarity.\n",
    "        \"\"\"\n",
    "        query_vector = self.embedding_engine.compute_embedding(query)\n",
    "        results = []\n",
    "        \n",
    "        for idx, (chunk_vector, chunk) in enumerate(zip(self.chunk_vectors, self.chunks)):\n",
    "            if filters and not self._apply_filters(chunk, filters):\n",
    "                continue\n",
    "                \n",
    "            similarity = np.dot(query_vector, chunk_vector)\n",
    "            \n",
    "            if similarity >= self.similarity_threshold:\n",
    "                results.append({\n",
    "                    'chunk': chunk,\n",
    "                    'similarity': similarity,\n",
    "                    'index': idx\n",
    "                })\n",
    "                \n",
    "        results.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "        return results[:self.top_k]\n",
    "        \n",
    "    def _apply_filters(self, \n",
    "                      chunk: Dict[str, Any], \n",
    "                      filters: Dict[str, Any]) -> bool:\n",
    "        \"\"\"Apply metadata filters to a chunk.\"\"\"\n",
    "        for key, value in filters.items():\n",
    "            if key not in chunk or chunk[key] != value:\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Explanation:**\n",
    "\n",
    "Let us break down the RetrievalSystem class and explain each component:\n",
    "\n",
    "1. **Class Initialization**:\n",
    "    ```python\n",
    "    def __init__(self, embedding_engine: EmbeddingEngine, top_k: int = 5, \n",
    "                similarity_threshold: float = 0.5):\n",
    "    ```\n",
    "    Purpose:\n",
    "    - `embedding_engine`: Handles text-to-vector conversion\n",
    "    - `top_k`: Number of most relevant results to return\n",
    "    - `similarity_threshold`: Minimum similarity score to consider (0.5 = 50% similar)\n",
    "    - `chunk_vectors`: Stores vector representations of documents\n",
    "    - `chunks`: Stores actual document content\n",
    "\n",
    "2. **Adding Document Chunks**:\n",
    "    ```python\n",
    "    def add_chunk(self, chunk: Dict[str, Any], vector: Optional[np.ndarray] = None):\n",
    "    ```\n",
    "    Purpose:\n",
    "    - Adds new document chunks to the system\n",
    "    - Either uses provided vector or generates new one\n",
    "    - Maintains parallel lists of chunks and their vectors\n",
    "    Example usage:\n",
    "    ```python\n",
    "    system.add_chunk({\n",
    "        'text': 'Document content here',\n",
    "        'id': 'doc1',\n",
    "        'source': 'textbook'\n",
    "    })\n",
    "    ```\n",
    "\n",
    "3. **Search Functionality**:\n",
    "    ```python\n",
    "    def search(self, query: str, filters: Optional[Dict[str, Any]] = None):\n",
    "    ```\n",
    "    Purpose:\n",
    "    - Finds relevant documents for a given query\n",
    "    - Converts query to vector representation\n",
    "    - Computes similarity with all stored documents\n",
    "    - Applies optional metadata filters\n",
    "    - Returns top K most similar documents\n",
    "\n",
    "    Key operations:\n",
    "    ```python\n",
    "    # Convert query to vector\n",
    "    query_vector = self.embedding_engine.compute_embedding(query)\n",
    "\n",
    "    # Calculate similarities and filter results\n",
    "    for idx, (chunk_vector, chunk) in enumerate(zip(self.chunk_vectors, self.chunks)):\n",
    "        similarity = np.dot(query_vector, chunk_vector)\n",
    "    ```\n",
    "\n",
    "4. **Filter Application**:\n",
    "    ```python\n",
    "    def _apply_filters(self, chunk: Dict[str, Any], filters: Dict[str, Any]):\n",
    "    ```\n",
    "    Purpose:\n",
    "    - Applies metadata-based filtering\n",
    "    - Checks if chunk matches all filter criteria\n",
    "    - Enables searching within specific documents/sources\n",
    "    Example usage:\n",
    "    ```python\n",
    "    results = system.search(\"AI concepts\", filters={'source': 'textbook'})\n",
    "    ```\n",
    "\n",
    "5. **Key Features**:\n",
    "\n",
    "a. Semantic Search:\n",
    "- Uses vector similarity for matching\n",
    "- Goes beyond simple keyword matching\n",
    "- Understands semantic relationships\n",
    "\n",
    "b. Flexible Filtering:\n",
    "- Metadata-based filtering\n",
    "- Multiple filter criteria support\n",
    "- Filter by source, ID, or other attributes\n",
    "\n",
    "c. Relevance Ranking:\n",
    "- Similarity score calculation\n",
    "- Threshold-based filtering\n",
    "- Top-K result selection\n",
    "\n",
    "6. **Example Usage**:\n",
    "    ```python\n",
    "    # Initialize system\n",
    "    retrieval_system = RetrievalSystem(\n",
    "        embedding_engine=EmbeddingEngine(),\n",
    "        top_k=3,\n",
    "        similarity_threshold=0.6\n",
    "    )\n",
    "\n",
    "    # Add documents\n",
    "    retrieval_system.add_chunk({\n",
    "        'text': 'AI is a branch of computer science...',\n",
    "        'source': 'textbook',\n",
    "        'chapter': 1\n",
    "    })\n",
    "\n",
    "    # Search with filters\n",
    "    results = retrieval_system.search(\n",
    "        query=\"What is artificial intelligence?\",\n",
    "        filters={'source': 'textbook'}\n",
    "    )\n",
    "    ```\n",
    "\n",
    "7. **Search Process Flow**:\n",
    "1. Query embedding generation\n",
    "2. Similarity computation with all chunks\n",
    "3. Filter application (if specified)\n",
    "4. Threshold filtering\n",
    "5. Sorting by similarity\n",
    "6. Top-K selection\n",
    "\n",
    "8. **Benefits**:\n",
    "- Efficient semantic search\n",
    "- Flexible filtering options\n",
    "- Configurable relevance thresholds\n",
    "- Easy integration with other components\n",
    "- Scalable document storage\n",
    "\n",
    "9. **Common Use Cases**:\n",
    "- Finding relevant context for questions\n",
    "- Document similarity comparison\n",
    "- Content recommendation\n",
    "- Information retrieval\n",
    "- Knowledge base search\n",
    "\n",
    "10. **Performance Considerations**:\n",
    "- Vector similarity computations\n",
    "- In-memory storage of vectors\n",
    "- Filter application overhead\n",
    "- Sorting operation complexity\n",
    "\n",
    "This system is crucial for:\n",
    "- Accurate document retrieval\n",
    "- Context-aware responses\n",
    "- Efficient information search\n",
    "- Metadata-based filtering\n",
    "- Relevance ranking\n",
    "\n",
    "The RetrievalSystem acts as the search engine of the RAG system, enabling intelligent document retrieval based on semantic understanding rather than just keyword matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntegratedQASystem:\n",
    "    \"\"\"\n",
    "    Complete question-answering system combining RAG with context awareness.\n",
    "    \"\"\"\n",
    "    def __init__(self, openai_api_key: str):\n",
    "        # Initialize components\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.embedding_engine = EmbeddingEngine()\n",
    "        self.retrieval_system = RetrievalSystem(self.embedding_engine)\n",
    "        self.memory_cache = MemoryCache()\n",
    "        self.client = OpenAI(api_key=openai_api_key)\n",
    "        \n",
    "        # Load existing memories\n",
    "        self.memory_cache._load_memories()\n",
    "        \n",
    "        # Define system prompts\n",
    "        self.system_prompt = \"\"\"You are a helpful assistant with both access to reference \n",
    "        materials and memory of the ongoing conversation. Use the provided context and \n",
    "        conversation history to give accurate, contextual responses. When referring to \n",
    "        previous conversation points, be explicit about what was discussed earlier.\"\"\"\n",
    "        \n",
    "    def add_document(self, document: Dict[str, str]) -> None:\n",
    "        \"\"\"Process and add a document to the knowledge base.\"\"\"\n",
    "        # Process document into chunks\n",
    "        chunks = self.document_processor.create_chunks(document)\n",
    "        \n",
    "        # Add each chunk to the retrieval system\n",
    "        for chunk in chunks:\n",
    "            vector = self.embedding_engine.compute_embedding(chunk['text'])\n",
    "            self.retrieval_system.add_chunk(chunk, vector)\n",
    "            \n",
    "    def _format_conversation_history(self, recent_context: List[Dict]) -> str:\n",
    "        \"\"\"Format recent conversation history for the prompt.\"\"\"\n",
    "        formatted_history = []\n",
    "        for memory in recent_context:\n",
    "            formatted_history.append(f\"User: {memory['user_input']}\")\n",
    "            formatted_history.append(f\"Assistant: {memory['system_response']}\")\n",
    "        return \"\\n\".join(formatted_history)\n",
    "        \n",
    "    def query(self, question: str, temperature: float = 0.7) -> Dict:\n",
    "        \"\"\"\n",
    "        Process a question using both RAG and conversation context.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get relevant documents and conversation history\n",
    "            relevant_contexts = self.retrieval_system.search(question)\n",
    "            recent_context = self.memory_cache.get_recent_context()\n",
    "            conversation_history = self._format_conversation_history(recent_context)\n",
    "            \n",
    "            # Format contexts\n",
    "            rag_context = \"\\n\\n\".join([\n",
    "                f\"Reference {i+1}:\\n{ctx['chunk']['text']}\"\n",
    "                for i, ctx in enumerate(relevant_contexts)\n",
    "            ])\n",
    "            \n",
    "            # Generate response\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"\"\"\n",
    "                    Previous Conversation:\n",
    "                    {conversation_history}\n",
    "                    \n",
    "                    Reference Materials:\n",
    "                    {rag_context}\n",
    "                    \n",
    "                    Current Question: {question}\n",
    "                    \n",
    "                    Please provide a response that considers both the reference materials\n",
    "                    and our conversation history when relevant.\"\"\"}\n",
    "                ],\n",
    "                temperature=temperature\n",
    "            )\n",
    "            \n",
    "            answer = response.choices[0].message.content\n",
    "            \n",
    "            # Store interaction in memory\n",
    "            self.memory_cache.add_memory(\n",
    "                question, \n",
    "                answer, \n",
    "                relevant_contexts\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'answer': answer,\n",
    "                'rag_contexts': relevant_contexts,\n",
    "                'conversation_context': recent_context\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'error': f\"Error generating response: {str(e)}\",\n",
    "                'rag_contexts': relevant_contexts if 'relevant_contexts' in locals() else [],\n",
    "                'conversation_context': recent_context if 'recent_context' in locals() else []\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Explanation:**\n",
    "\n",
    "Let us break down the IntegratedQASystem class, which combines RAG and context-awareness:\n",
    "\n",
    "1. **Class Initialization**:\n",
    "    ```python\n",
    "    def __init__(self, openai_api_key: str):\n",
    "    ```\n",
    "    Key Components:\n",
    "    - `document_processor`: Handles text chunking\n",
    "    - `embedding_engine`: Creates vector representations\n",
    "    - `retrieval_system`: Manages document search\n",
    "    - `memory_cache`: Stores conversation history\n",
    "    - `client`: OpenAI API interface\n",
    "\n",
    "    System Prompt:\n",
    "    - Defines assistant's behavior\n",
    "    - Emphasizes use of context and history\n",
    "    - Guides response generation\n",
    "\n",
    "2. **Document Addition**:\n",
    "    ```python\n",
    "    def add_document(self, document: Dict[str, str]) -> None:\n",
    "    ```\n",
    "    Process:\n",
    "    1. Chunks document using DocumentProcessor\n",
    "    2. Creates embeddings for each chunk\n",
    "    3. Stores chunks in retrieval system\n",
    "\n",
    "    Example:\n",
    "    ```python\n",
    "    system.add_document({\n",
    "        'text': 'Document content...',\n",
    "        'id': 'doc1',\n",
    "        'source': 'textbook'\n",
    "    })\n",
    "    ```\n",
    "\n",
    "3. **Conversation History Formatting**:\n",
    "    ```python\n",
    "    def _format_conversation_history(self, recent_context: List[Dict]) -> str:\n",
    "    ```\n",
    "    Purpose:\n",
    "    - Formats previous conversations for context\n",
    "    - Creates clear user/assistant dialogue structure\n",
    "    - Prepares history for prompt inclusion\n",
    "\n",
    "4. **Query Processing**:\n",
    "    ```python\n",
    "    def query(self, question: str, temperature: float = 0.7) -> Dict:\n",
    "    ```\n",
    "    Key Steps:\n",
    "\n",
    "    a. Context Gathering:\n",
    "    ```python\n",
    "    relevant_contexts = self.retrieval_system.search(question)\n",
    "    recent_context = self.memory_cache.get_recent_context()\n",
    "    ```\n",
    "    - Retrieves relevant documents\n",
    "    - Gets recent conversation history\n",
    "\n",
    "    b. Context Formatting:\n",
    "    ```python\n",
    "    rag_context = \"\\n\\n\".join([\n",
    "        f\"Reference {i+1}:\\n{ctx['chunk']['text']}\"\n",
    "        for i, ctx in enumerate(relevant_contexts)\n",
    "    ])\n",
    "    ```\n",
    "    - Organizes retrieved documents\n",
    "    - Numbers references for clarity\n",
    "\n",
    "    c. Response Generation:\n",
    "    ```python\n",
    "    response = self.client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt}\n",
    "        ]\n",
    "    )\n",
    "    ```\n",
    "    - Uses GPT-3.5-turbo model\n",
    "    - Includes system prompt, history, and context\n",
    "\n",
    "    d. Memory Management:\n",
    "    ```python\n",
    "    self.memory_cache.add_memory(\n",
    "        question, \n",
    "        answer, \n",
    "        relevant_contexts\n",
    "    )\n",
    "    ```\n",
    "    - Stores new interaction\n",
    "    - Maintains conversation history\n",
    "\n",
    "5. **Error Handling**:\n",
    "    ```python\n",
    "    try:\n",
    "        # Query processing\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'error': f\"Error generating response: {str(e)}\",\n",
    "            'rag_contexts': relevant_contexts if 'relevant_contexts' in locals() else [],\n",
    "            'conversation_context': recent_context if 'recent_context' in locals() else []\n",
    "        }\n",
    "    ```\n",
    "    - Graceful error handling\n",
    "    - Returns available context even on failure\n",
    "    - Maintains system stability\n",
    "\n",
    "6. **Key Features**:\n",
    "\n",
    "a. Integration:\n",
    "- Combines RAG and context awareness\n",
    "- Seamless document and conversation handling\n",
    "- Unified response generation\n",
    "\n",
    "b. Context Management:\n",
    "- Document-based context (RAG)\n",
    "- Conversation history (Memory)\n",
    "- Combined context utilization\n",
    "\n",
    "c. Response Generation:\n",
    "- Temperature control for creativity\n",
    "- Context-aware responses\n",
    "- Structured output format\n",
    "\n",
    "7. **Usage Example**:\n",
    "    ```python\n",
    "    qa_system = IntegratedQASystem(openai_api_key)\n",
    "\n",
    "    # Add knowledge\n",
    "    qa_system.add_document({\n",
    "        'text': 'AI content...',\n",
    "        'source': 'textbook'\n",
    "    })\n",
    "\n",
    "    # Query system\n",
    "    response = qa_system.query(\n",
    "        \"What is AI?\",\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    print(response['answer'])\n",
    "    ```\n",
    "\n",
    "8. **Benefits**:\n",
    "- Contextual awareness from both documents and conversation\n",
    "- Improved response accuracy\n",
    "- Persistent memory\n",
    "- Flexible document integration\n",
    "- Robust error handling\n",
    "\n",
    "This class serves as the main interface for the entire system, orchestrating:\n",
    "- Document processing\n",
    "- Context retrieval\n",
    "- Memory management\n",
    "- Response generation\n",
    "\n",
    "It creates a sophisticated QA system that combines the benefits of both RAG and context-aware conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing the Integrated QA System...\n",
      "\n",
      "Adding documents to knowledge base...\n",
      "✓ Added document from source: ai_basics\n",
      "✓ Added document from source: neural_networks\n",
      "✓ Added document from source: nlp_overview\n",
      "\n",
      "System initialized and ready for questions!\n",
      "\n",
      "Example questions you can try:\n",
      "1. What is artificial intelligence?\n",
      "2. How do neural networks relate to what you explained earlier?\n",
      "3. Can you tell me more about natural language processing?\n",
      "\n",
      "Type 'exit' to quit, 'help' for example questions, or 'clear' to reset conversation history.\n",
      "\n",
      "Processing your question...\n",
      "\n",
      "Assistant: Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. It involves the development of algorithms and models that enable machines to perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation.\n",
      "\n",
      "In our previous conversations, we discussed various aspects of technology, including machine learning and automation. AI is a broader field that encompasses these topics and aims to create systems that can learn from data, adapt to new inputs, and perform tasks autonomously. It relies on neural networks, natural language processing, and other advanced techniques to mimic human cognitive functions.\n",
      "\n",
      "If you are interested in specific applications or examples of AI in various industries or domains, feel free to ask for more information.\n",
      "\n",
      "Processing your question...\n",
      "\n",
      "Assistant: It seems like you're indicating that you want to end or quit the conversation. If you have any more questions or need further assistance in the future, don't hesitate to reach out. Have a great day!\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate the integrated QA system with RAG and context awareness.\n",
    "    This provides an interactive interface to test the system's capabilities with\n",
    "    example documents and real-time question answering.\n",
    "    \"\"\"\n",
    "    # Check for API key\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"Please set OPENAI_API_KEY environment variable\")\n",
    "    \n",
    "    # Initialize the system\n",
    "    print(\"\\nInitializing the Integrated QA System...\")\n",
    "    qa_system = IntegratedQASystem(api_key)\n",
    "    \n",
    "    # Sample documents demonstrating different topics and relationships\n",
    "    documents = [\n",
    "        {\n",
    "            'id': '1',\n",
    "            'text': '''Artificial Intelligence (AI) focuses on creating intelligent machines.\n",
    "                   Machine learning is a subset of AI that enables systems to learn from data.\n",
    "                   Deep learning, a type of machine learning, uses neural networks to process\n",
    "                   complex patterns in data.''',\n",
    "            'source': 'ai_basics'\n",
    "        },\n",
    "        {\n",
    "            'id': '2',\n",
    "            'text': '''Neural networks are computing systems inspired by biological brains.\n",
    "                   They consist of layers of interconnected nodes that process information.\n",
    "                   Deep learning neural networks can have many layers, allowing them to\n",
    "                   learn increasingly complex features from data.''',\n",
    "            'source': 'neural_networks'\n",
    "        },\n",
    "        {\n",
    "            'id': '3',\n",
    "            'text': '''Natural Language Processing (NLP) is a branch of AI focused on\n",
    "                   enabling computers to understand and generate human language.\n",
    "                   Modern NLP systems use transformer architectures and attention\n",
    "                   mechanisms to process and generate text effectively.''',\n",
    "            'source': 'nlp_overview'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nAdding documents to knowledge base...\")\n",
    "    for doc in documents:\n",
    "        try:\n",
    "            qa_system.add_document(doc)\n",
    "            print(f\"✓ Added document from source: {doc['source']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error adding document from {doc['source']}: {str(e)}\")\n",
    "    \n",
    "    print(\"\\nSystem initialized and ready for questions!\")\n",
    "    print(\"\\nExample questions you can try:\")\n",
    "    print(\"1. What is artificial intelligence?\")\n",
    "    print(\"2. How do neural networks relate to what you explained earlier?\")\n",
    "    print(\"3. Can you tell me more about natural language processing?\")\n",
    "    print(\"\\nType 'exit' to quit, 'help' for example questions, or 'clear' to reset conversation history.\")\n",
    "    \n",
    "    # Interactive questioning loop\n",
    "    while True:\n",
    "        try:\n",
    "            # Get user input\n",
    "            question = input(\"\\nYou: \").strip()\n",
    "            \n",
    "            # Handle special commands\n",
    "            if question.lower() == 'exit':\n",
    "                print(\"\\nThank you for using the QA system. Goodbye!\")\n",
    "                break\n",
    "                \n",
    "            elif question.lower() == 'help':\n",
    "                print(\"\\nExample questions you can try:\")\n",
    "                print(\"1. What is artificial intelligence?\")\n",
    "                print(\"2. How do neural networks relate to what you explained earlier?\")\n",
    "                print(\"3. Can you tell me more about natural language processing?\")\n",
    "                continue\n",
    "                \n",
    "            elif question.lower() == 'clear':\n",
    "                qa_system.memory_cache.memories = []\n",
    "                qa_system.memory_cache._save_memories()\n",
    "                print(\"\\nConversation history cleared!\")\n",
    "                continue\n",
    "            \n",
    "            # Skip empty questions\n",
    "            if not question:\n",
    "                continue\n",
    "            \n",
    "            # Process the question\n",
    "            print(\"\\nProcessing your question...\")\n",
    "            result = qa_system.query(question)\n",
    "            \n",
    "            # Handle successful response\n",
    "            if 'error' not in result:\n",
    "                print(\"\\nAssistant:\", result['answer'])\n",
    "                \n",
    "                # Show reference information\n",
    "                if result['rag_contexts']:\n",
    "                    print(\"\\nReferences used:\")\n",
    "                    for i, ctx in enumerate(result['rag_contexts'], 1):\n",
    "                        similarity = ctx['similarity'] * 100  # Convert to percentage\n",
    "                        source = ctx['chunk'].get('source', 'unknown')\n",
    "                        print(f\"\\n{i}. Source: {source} (Relevance: {similarity:.1f}%)\")\n",
    "                \n",
    "            # Handle errors\n",
    "            else:\n",
    "                print(\"\\nError:\", result['error'])\n",
    "                print(\"Please try asking your question again.\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nExiting gracefully...\")\n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nAn unexpected error occurred: {str(e)}\")\n",
    "            print(\"The system is still running. Please try again.\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Explanation:**\n",
    "\n",
    "Let us break down the main function that demonstrates the integrated QA system:\n",
    "\n",
    "1. **Initialization and Setup**:\n",
    "    ```python\n",
    "    # API Key Check\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"Please set OPENAI_API_KEY environment variable\")\n",
    "\n",
    "    # System Initialization\n",
    "    print(\"\\nInitializing the Integrated QA System...\")\n",
    "    qa_system = IntegratedQASystem(api_key)\n",
    "    ```\n",
    "    Purpose:\n",
    "    - Verifies OpenAI API key availability\n",
    "    - Initializes the QA system\n",
    "\n",
    "2. **Knowledge Base Setup**:\n",
    "    ```python\n",
    "    documents = [\n",
    "        {\n",
    "            'id': '1',\n",
    "            'text': '''Artificial Intelligence (AI) focuses on...''',\n",
    "            'source': 'ai_basics'\n",
    "        },\n",
    "        # ... more documents ...\n",
    "    ]\n",
    "    ```\n",
    "    Features:\n",
    "    - Sample documents on related topics\n",
    "    - Structured document format\n",
    "    - Clear source attribution\n",
    "\n",
    "3. **Document Loading**:\n",
    "    ```python\n",
    "    print(\"\\nAdding documents to knowledge base...\")\n",
    "    for doc in documents:\n",
    "        try:\n",
    "            qa_system.add_document(doc)\n",
    "            print(f\"✓ Added document from source: {doc['source']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error adding document from {doc['source']}: {str(e)}\")\n",
    "    ```\n",
    "    Purpose:\n",
    "    - Loads documents into system\n",
    "    - Provides loading status feedback\n",
    "    - Handles loading errors gracefully\n",
    "\n",
    "4. **User Interface Setup**:\n",
    "    ```python\n",
    "    print(\"\\nSystem initialized and ready for questions!\")\n",
    "    print(\"\\nExample questions you can try:\")\n",
    "    print(\"1. What is artificial intelligence?\")\n",
    "    print(\"2. How do neural networks relate to what you explained earlier?\")\n",
    "    print(\"3. Can you tell me more about natural language processing?\")\n",
    "    ```\n",
    "    Features:\n",
    "    - Clear system status indication\n",
    "    - Example questions for guidance\n",
    "    - User-friendly interface\n",
    "\n",
    "5. **Command Handling**:\n",
    "    ```python\n",
    "    # Special commands\n",
    "    if question.lower() == 'exit':\n",
    "        print(\"\\nThank you for using the QA system. Goodbye!\")\n",
    "        break\n",
    "    elif question.lower() == 'help':\n",
    "        # Show help menu\n",
    "    elif question.lower() == 'clear':\n",
    "        qa_system.memory_cache.memories = []\n",
    "        qa_system.memory_cache._save_memories()\n",
    "    ```\n",
    "    Commands:\n",
    "    - `exit`: Ends the session\n",
    "    - `help`: Shows example questions\n",
    "    - `clear`: Resets conversation history\n",
    "\n",
    "6. **Question Processing**:\n",
    "    ```python\n",
    "    print(\"\\nProcessing your question...\")\n",
    "    result = qa_system.query(question)\n",
    "\n",
    "    if 'error' not in result:\n",
    "        print(\"\\nAssistant:\", result['answer'])\n",
    "        \n",
    "        # Show reference information\n",
    "        if result['rag_contexts']:\n",
    "            print(\"\\nReferences used:\")\n",
    "            for i, ctx in enumerate(result['rag_contexts'], 1):\n",
    "                similarity = ctx['similarity'] * 100\n",
    "                source = ctx['chunk'].get('source', 'unknown')\n",
    "                print(f\"\\n{i}. Source: {source} (Relevance: {similarity:.1f}%)\")\n",
    "    ```\n",
    "    Features:\n",
    "    - Question processing feedback\n",
    "    - Clear answer display\n",
    "    - Reference information showing\n",
    "    - Relevance scoring\n",
    "\n",
    "7. **Error Handling**:\n",
    "    ```python\n",
    "    try:\n",
    "        # Processing code\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nExiting gracefully...\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred: {str(e)}\")\n",
    "        print(\"The system is still running. Please try again.\")\n",
    "    ```\n",
    "    Handles:\n",
    "    - Keyboard interrupts\n",
    "    - Processing errors\n",
    "    - System continuity\n",
    "\n",
    "8. **Key Features**:\n",
    "\n",
    "a. Interactive Interface:\n",
    "- Real-time question answering\n",
    "- Command system\n",
    "- Status feedback\n",
    "\n",
    "b. Error Management:\n",
    "- Graceful error handling\n",
    "- Clear error messages\n",
    "- System recovery\n",
    "\n",
    "c. Reference Display:\n",
    "- Source attribution\n",
    "- Relevance scores\n",
    "- Context information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        os.environ[\"OPENAI_API_KEY\"] = 'OPENAI_API_KEY'\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"\\nCritical error: {str(e)}\")\n",
    "        print(\"Please ensure your OpenAI API key is set and all requirements are installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "## How the Integration Works\n",
    "\n",
    "Let's understand how this integrated system enhances our question-answering capabilities:\n",
    "\n",
    "1. **Memory Management**\n",
    "   - The `MemoryCache` stores recent conversations with timestamps\n",
    "   - It automatically removes old conversations to maintain relevance\n",
    "   - Each memory entry includes both the conversation and any RAG contexts used\n",
    "\n",
    "2. **Context Integration**\n",
    "   - When processing a question, the system considers:\n",
    "     * Relevant documents from the RAG system\n",
    "     * Recent conversation history from the memory cache\n",
    "   - The combined context helps generate more coherent and informed responses\n",
    "\n",
    "3. **Enhanced Response Generation**\n",
    "   - The system prompt instructs the model to use both reference materials and conversation history\n",
    "   - Responses can refer back to previous discussions while grounding answers in source documents\n",
    "   - The system maintains conversational flow while ensuring factual accuracy\n",
    "\n",
    "## Advantages of the Integrated Approach\n",
    "\n",
    "1. **Improved Accuracy**\n",
    "   - RAG provides factual grounding through reference documents\n",
    "   - Context awareness ensures consistency across the conversation\n",
    "\n",
    "2. **Better User Experience**\n",
    "   - The system can maintain coherent conversations\n",
    "   - It can refer back to previous topics naturally\n",
    "   - It combines fresh information with conversation history\n",
    "\n",
    "3. **Flexible Knowledge Base**\n",
    "   - Documents can be added or updated at any time\n",
    "   - Conversation context evolves naturally\n",
    "   - The system can handle both general and specific queries\n",
    "\n",
    "## Exercise: Building and Testing the System\n",
    "\n",
    "Try these exercises to understand the system better:\n",
    "\n",
    "1. **Basic Testing**\n",
    "```python\n",
    "# Initialize the system\n",
    "qa_system = IntegratedQASystem(your_api_key)\n",
    "\n",
    "# Add some test documents\n",
    "test_doc = {\n",
    "    'id': 'test1',\n",
    "    'text': 'Your test document text here',\n",
    "    'source': 'test_source'\n",
    "}\n",
    "qa_system.add_document(test_doc)\n",
    "\n",
    "# Try a series of related questions\n",
    "questions = [\n",
    "    \"What is the main topic of the document?\",\n",
    "    \"Can you elaborate on that?\",\n",
    "    \"How does this relate to what we discussed earlier?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    response = qa_system.query(question)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {response['answer']}\\n\")\n",
    "```\n",
    "\n",
    "2. **Context Analysis**\n",
    "   - Examine how the system uses both RAG and conversation context\n",
    "   - Observe how responses change with and without context\n",
    "   - Try questions that require understanding of previous conversation\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "By integrating RAG with context awareness, we've created a more sophisticated question-answering system that combines the benefits of both approaches. The system can provide accurate, source-based information while maintaining natural, contextual conversations. This integration represents a significant step toward more intelligent and user-friendly AI interactions.\n",
    "\n",
    "Remember to experiment with different settings and use cases to fully understand the capabilities and limitations of the integrated system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

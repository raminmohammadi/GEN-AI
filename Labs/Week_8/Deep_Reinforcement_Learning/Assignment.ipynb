{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning Assignment (Graded): The Mountain Car Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to your programming assignment on Deep Reinforcement Learning! You will build an RL-Model to solve the famous Mountain Car Problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Mountain Car is a classic control problem that elegantly demonstrates the core concepts of reinforcement learning. \n",
    "\n",
    "- In this environment, an underpowered car must drive up a steep hill. \n",
    "\n",
    "- The challenge lies in the fact that the car's engine is not powerful enough to climb the hill directly from a standing start.\n",
    "\n",
    "- To solve, this problem, you have yo build a Reinforcement Learning Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Characteristics:**\n",
    "\n",
    "- The car must learn to build momentum by swinging back and forth in the valley\n",
    "\n",
    "- The environment provides continuous state variables (position and velocity)\n",
    "\n",
    "- The agent has three discrete actions available: accelerate left, right, or neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset/Environment Specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**State Space:**\n",
    "- Two continuous variables:\n",
    "  - Position: Range (-1.2, 0.6)\n",
    "  - Velocity: Range (-0.07, 0.07)[2]\n",
    "\n",
    "**Action Space:**\n",
    "- Three discrete actions:\n",
    "  - -1: Accelerate left\n",
    "  - 0: No acceleration\n",
    "  - 1: Accelerate right[1]\n",
    "\n",
    "**Reward Structure:**\n",
    "- -1 reward for each time step\n",
    "- Episode terminates when either:\n",
    "  - The car reaches the goal position (≥ 0.6)\n",
    "  - Maximum steps are reached[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Neural Network Architecture Implementation:**\n",
    "   - Build a sequential model with three layers: two hidden layers with 128 neurons and ReLU activation, and an output layer with linear activation.\n",
    "   - Compile the model using the Adam optimizer and mean squared error (MSE) loss.\n",
    "\n",
    "2. **Experience Replay Implementation:**\n",
    "   - Store experience tuples in a deque, and during replay, sample a minibatch, compute target Q-values using Double DQN, and update the main network via gradient descent.\n",
    "\n",
    "3. **Action Selection (Epsilon-Greedy):**\n",
    "   - Implement epsilon-greedy strategy: return a random action with probability epsilon or the action with the highest Q-value from the model otherwise.\n",
    "\n",
    "4. **Target Network Update:**\n",
    "   - Perform a soft update of the target network by blending weights from the main network and the target network based on a parameter τ.\n",
    "\n",
    "5. **Custom Reward Function:**\n",
    "   - Compute reward based on the car’s position and velocity, reward goal achievement (+100), penalize failure (-10), and clip rewards if necessary.\n",
    "\n",
    "6. **Training Loop:**\n",
    "   - Create an episode-based loop, logging progress, updating epsilon, and saving/loading models, along with tracking performance metrics like rewards and success rates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Only write code when you see any of the below prompts,\n",
    "\n",
    "    ```\n",
    "    # YOUR CODE GOES HERE\n",
    "    # YOUR CODE ENDS HERE\n",
    "    # TODO\n",
    "    ```\n",
    "\n",
    "- Do not modify any other section of the code unless tated otherwise in the comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use a venv of Python 3.9.6 to solve this assignment\n",
    "\n",
    "- Install all the packages from requirements.txt so that you don't face any compatibility issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from helpers.methods import detect_and_set_device, plot_training_metrics, save_frames_as_gif\n",
    "from tests.test_methods import test_dqn_agent, test_compute_reward, test_run_episode, test_update_best_episode, test_train_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Initializing the OpenAI Gym Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Hints:**\n",
    "\n",
    "Complete the initialize_environment method.\n",
    "\n",
    "* Create and configure a Gym environment using the specified environment name and render mode.\n",
    "\n",
    "* Extract the state space dimensions from the environment's observation space.\n",
    "\n",
    "* Determine the number of possible actions from the environment's action space.\n",
    "\n",
    "* Return the initialized environment along with state and action dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_environment(env_name='MountainCar-v0', render_mode='rgb_array'):\n",
    "    # YOUR CODE GOES HERE\n",
    "    \n",
    "    # Create the environment with the specified name and render mode\n",
    "    env = \n",
    "    \n",
    "    # Get the state size from the environment observation space by taking the shape of the observation space\n",
    "    state_size = \n",
    "    \n",
    "    # Get the action size from the environment action space by taking the number of actions in the action space\n",
    "    action_size = \n",
    "    return env, state_size, action_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Implementing a DQN Agent Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Hints:**\n",
    "\n",
    "Complete the DQNAgent class implementation.\n",
    "\n",
    "* Initialize the agent with state and action dimensions, setting up:\n",
    "  * Experience replay memory using deque with max length 5000\n",
    "  * Hyperparameters (gamma, epsilon, epsilon decay)\n",
    "  * Neural network model\n",
    "  * History tracking for various metrics\n",
    "\n",
    "* Build a neural network model that:\n",
    "  * Takes state size as input\n",
    "  * Has two hidden layers of 64 units with ReLU activation\n",
    "  * Outputs Q-values for each action\n",
    "  * Uses Adam optimizer and MSE loss\n",
    "\n",
    "* Implement core DQN methods:\n",
    "  * Memory storage for experience replay\n",
    "  * Action selection with epsilon-greedy policy\n",
    "  * Training through replay with batch sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DQNAgent class\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=5000)\n",
    "        \n",
    "        # Define the hyperparameters\n",
    "        # YOUR CODE GOES HERE\n",
    "        # Define the discount factor to be used in the Bellman equation for updating the Q-values of the agent to be 0.99\n",
    "        self.gamma =\n",
    "        # Define the exploration rate of the agent to be 1.0\n",
    "        self.epsilon =\n",
    "        # Define the minimum exploration rate of the agent to be 0.01\n",
    "        self.epsilon_min =\n",
    "        # Define the decay rate of the exploration rate of the agent to be 0.995\n",
    "        self.epsilon_decay =\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "        # Initialize histories for plotting\n",
    "        self.rewards_history = []\n",
    "        self.epsilon_history = []\n",
    "        self.loss_history = []\n",
    "        self.position_history = []\n",
    "        self.velocity_history = []\n",
    "        self.action_history = []\n",
    "        \n",
    "    # Define the neural network model\n",
    "    def _build_model(self):\n",
    "        # YOUR CODE GOES HERE\n",
    "        # Define the model to be a Sequential model\n",
    "        # Add a Input layer with the shape of the state size for the input\n",
    "        # Add 2 Dense layers with 64 units and relu activation and a Dense layer with the action size and linear activation\n",
    "        model = \n",
    "        \n",
    "        # Compile the model with the Adam optimizer with learning rate 0.001 and mean squared error loss\n",
    "        \n",
    "        \n",
    "        return model\n",
    "    \n",
    "    # Define a method to remember the state, action, reward, next state, and done flag\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # YOUR CODE GOES HERE\n",
    "        # Append the tuple (state, action, reward, next state, done) to the memory\n",
    "        \n",
    "    # Define a method to act based on the state provided\n",
    "    def act(self, state):\n",
    "        # YOUR CODE GOES HERE\n",
    "        # If a random number is less than the epsilon value, return a random action\n",
    "        # Otherwise, return the action with the highest Q-value predicted by the model\n",
    "\n",
    "    # Define a method to train the agent based on a batch size of experiences\n",
    "    def replay(self, batch_size):\n",
    "        # YOUR CODE GOES HERE\n",
    "        # If the memory is less than the batch size, return nothing\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        # Sample a minibatch of experiences from the memory with the specified batch size using random.sample\n",
    "        minibatch = \n",
    "        \n",
    "        # Initialize the states, actions, rewards, next states, and dones arrays from the minibatch of experiences\n",
    "        states = \n",
    "        \n",
    "        # Predict the Q-values of the states using the model and store them in the targets variable\n",
    "        actions = \n",
    "        \n",
    "        # Get the rewards from the minibatch of experiences and store them in the rewards variable\n",
    "        rewards = \n",
    "        \n",
    "        # Get the next states from the minibatch of experiences and store them in the next_states variable\n",
    "        next_states = \n",
    "        \n",
    "        # Get the dones from the minibatch of experiences and store them in the dones variable\n",
    "        dones = \n",
    "\n",
    "        # Compute the targets for the Q-values of the states using the rewards, next states, and dones by applying the Bellman equation with the discount factor gamma and store them in the targets variable \n",
    "        targets = \n",
    "        \n",
    "        # Compute the Q-values of the states using the model and store them in the target_f variable by predicting the Q-values of the states using the model\n",
    "        target_f = \n",
    "        \n",
    "        # Update the Q-values of the actions in the minibatch of experiences using the computed targets and the Q-values of the states using the model by setting the target Q-values of the actions in the minibatch of experiences to the computed targets in the target_f variable \n",
    "        target_f[np.arange(len(actions)), actions] = targets\n",
    "\n",
    "        # Fit the model with the states and target Q-values for one epoch and store the loss in the history variable by calling the fit method of the model with the states and target Q-values for one epoch and verbose=0\n",
    "        history = \n",
    "        \n",
    "        # Append the loss to the loss history of the agent using the history variable and the history attribute of the history variable\n",
    "        \n",
    "\n",
    "        # If the epsilon value is greater than the minimum epsilon value, decay the epsilon value by the decay rate value\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "dummy_state_size = 2\n",
    "dummy_action_size = 3\n",
    "agent = DQNAgent(dummy_state_size, dummy_action_size)\n",
    "test_dqn_agent(agent, dummy_state_size, dummy_action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Implementing a Custom Reward Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Hints:**\n",
    "\n",
    "Complete the compute_reward method for the Mountain Car environment.\n",
    "\n",
    "* Design a reward function that encourages:\n",
    "  * Reaching higher positions (position component)\n",
    "  * Maintaining momentum (velocity component)\n",
    "  * Successfully reaching the goal position (≥ 0.5)\n",
    "  * Avoiding episode termination before reaching goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the compute_reward function\n",
    "def compute_reward(state, next_state, done):\n",
    "    # YOUR CODE GOES HERE\n",
    "    # Get the position from the next state by taking the first element of the next state\n",
    "    position = \n",
    "    \n",
    "    # Get the velocity from the next state by taking the second element of the next state\n",
    "    velocity = \n",
    "    \n",
    "    # Compute the reward based on the position by adding the position multiplied by 0.5 and 10 to the reward\n",
    "    reward =   # Height reward\n",
    "    \n",
    "    # Compute the reward based on the velocity by adding the absolute value of the velocity multiplied by 5 to the reward\n",
    "    reward +=    # Velocity reward\n",
    "    \n",
    "    # If the position is greater than or equal to 0.5, add 100 to the reward\n",
    "    if position >= 0.5:\n",
    "        \n",
    "    # If the episode is done and the position is less than 0.5, subtract 10 from the reward\n",
    "    if done and position < 0.5:\n",
    "        \n",
    "    return reward\n",
    "\n",
    "test_compute_reward(compute_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Implementing Episode Runner for DQN Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Hints:**\n",
    "\n",
    "Complete the run_episode method to handle a single training episode.\n",
    "\n",
    "* Initialize episode:\n",
    "  * Reset environment and extract initial state\n",
    "  * Set up tracking for rewards and frames\n",
    "  * Handle both old and new Gym API formats\n",
    "\n",
    "* Execute episode loop:\n",
    "  * Capture environment renders for visualization\n",
    "  * Get agent's action using epsilon-greedy policy\n",
    "  * Execute action and handle environment step\n",
    "  * Compute custom reward\n",
    "  * Store experience in agent's memory\n",
    "  * Perform training if enough samples available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, agent, batch_size, compute_reward):\n",
    "    # YOUR CODE GOES HERE\n",
    "    # Reset the environment and get the initial state\n",
    "    reset_result = \n",
    "    \n",
    "    # Get the state from the reset result by taking the first element of the reset result if the reset result is a tuple, otherwise take the reset result\n",
    "    state = \n",
    "    total_reward = 0\n",
    "    episode_frames = []\n",
    "    \n",
    "    # Loop until the episode is done\n",
    "    while True:\n",
    "        # Render the environment and get the frame\n",
    "        frame = \n",
    "        \n",
    "        # If the frame is not None and the frame has 3 dimensions, append the frame to the episode frames\n",
    "        if frame is not None and frame.ndim == 3:  # Append only valid RGB frames\n",
    "            \n",
    "        \n",
    "        # Get the action from the agent by calling the act method of the agent with the state as input\n",
    "        action = \n",
    "        \n",
    "        # Take a step in the environment with the action and get the step result\n",
    "        step_result = \n",
    "        \n",
    "        # If the step result has 5 elements, get the next state, reward, terminated, truncated, and _ from the step result\n",
    "        # Otherwise, get the next state, reward, done, and _ from the step result\n",
    "        if len(step_result) == 5:\n",
    "            \n",
    "        else:\n",
    "            \n",
    "        # Compute the reward using the state, next state, and done flag by calling the compute_reward function with the state, next state, and done flag as inputs\n",
    "        reward = \n",
    "        \n",
    "        # Remember the state, action, reward, next state, and done flag by calling the remember method of the agent with the state, action, reward, next state, and done flag as inputs\n",
    "        \n",
    "        \n",
    "        # Set the state to the next state\n",
    "        \n",
    "        \n",
    "        # Add the reward to the total reward\n",
    "        \n",
    "\n",
    "        # Train the agent with the batch size by calling the replay method of the agent with the batch size as input if the memory of the agent is greater than the batch size\n",
    "        \n",
    "            \n",
    "        # If the episode is done, break the loop\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_reward, episode_frames\n",
    "\n",
    "\n",
    "test_run_episode(run_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Updating Best Episode Tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Hints:**\n",
    "\n",
    "Complete the update_best_episode method to track the best performing episode.\n",
    "\n",
    "* Compare the current episode's total reward with the best reward seen so far\n",
    "* If current reward is better:\n",
    "  * Update the best reward value\n",
    "  * Store a copy of the current episode's frames\n",
    "* Otherwise:\n",
    "  * Maintain existing best reward and frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_best_episode function that takes the total reward, best reward, and episode frames as input and returns the best reward and episode frames\n",
    "def update_best_episode(total_reward, best_reward, episode_frames):\n",
    "    \n",
    "    # YOUR CODE GOES HERE\n",
    "    # If the total reward is greater than the best reward, return the total reward and the episode frame's copy\n",
    "    # Otherwise, return the best reward and the episode frames\n",
    "\n",
    "\n",
    "\n",
    "test_update_best_episode(update_best_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS FUNCTION: log_progress\n",
    "def log_progress(episode, total_reward, epsilon):\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode: {episode}, Total Reward: {total_reward}, Epsilon: {epsilon:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Implementing the DQN Training Loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Hints:**\n",
    "\n",
    "Complete the train_agent method to manage the full training process.\n",
    "\n",
    "* Initialize tracking variables:\n",
    "  * List for storing episode rewards\n",
    "  * List for tracking epsilon values\n",
    "  * Storage for best episode frames\n",
    "  * Variable for best reward achieved\n",
    "\n",
    "* Execute training loop:\n",
    "  * Run episodes using run_episode function\n",
    "  * Record episode rewards and epsilon values\n",
    "  * Update best episode information\n",
    "  * Log training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train_agent function\n",
    "def train_agent(env, agent, episodes, batch_size, compute_reward):\n",
    "    rewards_history = []\n",
    "    epsilon_history = []\n",
    "    best_frames = []\n",
    "    best_reward = float('-inf')\n",
    "    \n",
    "    # Loop through the episodes\n",
    "    for episode in range(episodes):\n",
    "        # Run an episode with the environment, agent, batch size, and compute reward function and get the total reward and episode frames\n",
    "        total_reward, episode_frames = \n",
    "        \n",
    "        # Append the total reward to the rewards history of the agent\n",
    "        \n",
    "        \n",
    "        # Append the epsilon value to the epsilon history of the agent\n",
    "        \n",
    "        # Update the best reward and best frames using the total reward, best reward, and episode frames by calling the update_best_episode function\n",
    "        best_reward, best_frames = \n",
    "        \n",
    "        # Log the progress of the episode using the log_progress function\n",
    "        log_progress(episode, total_reward, agent.epsilon)\n",
    "\n",
    "    return rewards_history, epsilon_history, best_frames\n",
    "\n",
    "\n",
    "test_train_agent(train_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Results and Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS FUNCTION\n",
    "import os\n",
    "\n",
    "def save_results(agent, rewards_history, epsilon_history, best_frames, gif_name='best_episode.gif'):\n",
    "    # Check if the output directory exists, if not create it\n",
    "    output_dir = './output'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Output directory '{output_dir}' created.\")\n",
    "    \n",
    "    # Save the training metrics plot\n",
    "    plot_training_metrics(agent, rewards_history, epsilon_history)\n",
    "    # Save the plot in the output directory\n",
    "    plot_file_path = os.path.join(output_dir, 'training_metrics.png')\n",
    "    plt.savefig(plot_file_path)\n",
    "    print(f\"Training metrics plot saved at {plot_file_path}\")\n",
    "\n",
    "    try:\n",
    "        # Save the frames as a GIF in the specified output directory\n",
    "        gif_path = os.path.join(output_dir, gif_name)\n",
    "        save_frames_as_gif(best_frames, gif_path)\n",
    "        print(f\"Best episode GIF saved at {gif_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving GIF: {e}\")\n",
    "        if best_frames:\n",
    "            print(f\"Frame shape: {np.array(best_frames[0]).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS FUNCTION\n",
    "def main():\n",
    "    env, state_size, action_size = initialize_environment()\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    episodes = 10\n",
    "    batch_size = 32\n",
    "\n",
    "    rewards_history, epsilon_history, best_frames = train_agent(\n",
    "        env, agent, episodes, batch_size, compute_reward\n",
    "    )\n",
    "    save_results(agent, rewards_history, epsilon_history, best_frames)\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS FUNCTION - Helper function to display the saved training metrics plot and best episode GIF\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def display_saved_results(training_plot_path='./output/training_metrics.png', gif_path='./output/best_episode.gif'):\n",
    "    \"\"\"Display the saved training metrics plot and best episode GIF.\"\"\"\n",
    "    try:\n",
    "        # Display the training metrics plot\n",
    "        print(f\"Displaying training metrics plot from: {training_plot_path}\")\n",
    "        img = plt.imread(training_plot_path)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')  # Hide axes for image\n",
    "        plt.show()\n",
    "\n",
    "        # Display the best episode GIF\n",
    "        print(f\"Displaying best episode GIF from: {gif_path}\")\n",
    "        display(Image(filename=gif_path))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying results: {e}\")\n",
    "        \n",
    "display_saved_results(training_plot_path='./output/training_metrics.png', gif_path='./output/best_episode.gif')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

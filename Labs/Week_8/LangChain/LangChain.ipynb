{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1>\n",
        "ü¶ú‚õìÔ∏è‚Äçüí•LangChain\n",
        "</h1>\n"
      ],
      "metadata": {
        "id": "rUgRVUfXQKl4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Brief Recap**"
      ],
      "metadata": {
        "id": "nDUpAOnYQQSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LangChain** is an open source framework for building applications based on large language models (LLMs). LangChain provides tools and abstractions to improve the customization, accuracy, and relevancy of the information the models generate. For example, developers can use LangChain components to build new prompt chains or customize existing templates. LangChain also includes components that allow LLMs to access new data sets without retraining.\n",
        "\n",
        "With LangChain, organizations can repurpose LLMs for domain-specific applications without retraining or fine-tuning. Development teams can build complex applications referencing proprietary information to augment model responses. For example, you can use LangChain to build applications that read data from stored internal documents and summarize them into conversational responses."
      ],
      "metadata": {
        "id": "BxHI6AdUQRdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Architecture**\n",
        "\n",
        "<img src='assets/arch.png' width=450>\n",
        "\n",
        "Here's an architecture and workflow of a LangChain-powered document processing and question-answering system. Let me break down its key components:\n",
        "\n",
        "* **Input Processing**\n",
        "  * The system starts with PDF documents on the left side\n",
        "  * These documents are split into multiple chunks of text/documents\n",
        "  * Each chunk goes through an embedding process (represented by binary code icons)\n",
        "* **Data Processing Flow**\n",
        "  * The text chunks are converted into embeddings (vector representations)\n",
        "  * These embeddings are stored in a Vector Store (shown as a database icon)\n",
        "  * The system uses specific technologies like:\n",
        "    * Amazon Aurora\n",
        "    * PostgreSQL with pgvector\n",
        "    * Knowledge Base functionality\n",
        "* **Query Processing**\n",
        "  * A user inputs a question (\"What is a neural network?\")\n",
        "  * The question goes through Question Embedding\n",
        "  * A Semantic Search is performed against the vector store\n",
        "  * The search produces Ranked Results\n",
        "* **Response Generation**\n",
        "  * The ranked results are processed by an LLM (Language Learning Model)\n",
        "  * The LLM generates the final Answer back to the user\n",
        "\n",
        "LangChain combines document processing, vector embeddings, and language models to create a comprehensive question-answering system.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vWQGPeAmYLMq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Use Cases**\n",
        "\n",
        "* **Document Processing and Analysis**\n",
        "  * Parsing complex documents to extract structured information into JSON or tables\n",
        "  * Automated extraction of dates, quantities, and transaction details from financial documents\n",
        "  * Document classification using few-shot prompting\n",
        "  * Processing and analyzing large volumes of text documents\n",
        "* **Question Answering Systems**\n",
        "  * Building chatbots and virtual assistants for customer support\n",
        "  * Creating retrieval-based QA systems with document context\n",
        "  * Implementing conversational agents that can access and reference internal documents\n",
        "  * Developing SQL-based QA systems that work with various database dialects\n",
        "* **Content Generation and Summarization**\n",
        "  * Generating executive summaries of documents and meeting notes\n",
        "  * Creating summaries of large documents using map-reduce techniques\n",
        "  * Producing concise summaries of financial reports and earnings documents\n",
        "  * Translating content across multiple languages\n",
        "* **Conversational AI**\n",
        "  * Context-aware chatbots for customer support\n",
        "  * Virtual agents that can access and reference company documentation\n",
        "  * Automated appointment scheduling and customer service systems\n",
        "\n",
        "* **Code-Related Tasks**\n",
        "  * Code analysis for detecting bugs and security vulnerabilities\n",
        "  * Development of coding assistants to improve programmer productivity\n",
        "  * Custom development environments with integrated LLM capabilities"
      ],
      "metadata": {
        "id": "qkFEkfkHaQtS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "FIUO2bEp01nA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementation**\n",
        "\n",
        "\n",
        "There are majorly 3 main components in LangChain:\n",
        "\n",
        "1. **Components**\n",
        "  * **LLM Wrappers**: There are LLM wrappers that allows us to connect to the LLMs like GPT4.\n",
        "  * **Prompt Templates**: They allow us to avoid hard code text the input to the LLMs.\n",
        "  * **Indexes**: Allow us to extract relevant info from the LLMs. For eg: PineCone VectorStore.\n",
        "\n",
        "2. **Chains**\n",
        "  * They allow us to combine multiple components together to solve a specific task building an entire LLM application\n",
        "\n",
        "3. **Agents**\n",
        "  * It allows the LLMs to interact with external APIs.\n",
        "\n",
        "We will be unpacking these concepts discussing and implementing each of them.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HfyOEN7AbTa9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Initial Setup**\n",
        "Firstly we are going to pip install the following libraries:\n",
        "```python\n",
        "python-dotenv==1.0.0\n",
        "langchain==0.0.137\n",
        "pinecone-client==2.2.1\n",
        "```\n",
        "* **python-dotenv**: To manage env file with passwords\n",
        "* **pinecone-client**: Vector store database\n",
        "\n",
        "### **API Keys**\n",
        "\n",
        "Sign up and gather your API keys from all the following websites:\n",
        "* [Pinecone](https://www.pinecone.io/?utm_term=pinecone%20database&utm_campaign=brand-us-e&utm_source=adwords&utm_medium=ppc&hsa_acc=3111363649&hsa_cam=21023369441&hsa_grp=167470667468&hsa_ad=690982708943&hsa_src=g&hsa_tgt=kwd-1627713670725&hsa_kw=pinecone%20database&hsa_mt=e&hsa_net=adwords&hsa_ver=3&gad_source=1&gclid=CjwKCAiAxqC6BhBcEiwAlXp453Gr9f08CPnE8_0JZfjvb1y_D0fb7uDaO-3btwyVnvrvI_tkfawjVBoCMeUQAvD_BwE)\n",
        "\n",
        "* [OpenAI](https://platform.openai.com/docs/overview)\n"
      ],
      "metadata": {
        "id": "x4HAfPNSy_Ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Dependencies\n",
        "\n",
        "!pip install python-dotenv==1.0.0\n",
        "!pip install langchain==0.0.137\n",
        "!pip install pinecone-client==2.2.1"
      ],
      "metadata": {
        "id": "XG_Xtz4uqp8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save your API keys in an env variable\n",
        "%%writefile .env\n",
        "\n",
        "OPENAI_API_KEY=\"YOUR API KEY\"\n",
        "PINECONE_API_KEY=\"YOUR API KEY\"\n",
        "PINECONE_ENV=\"YOUR API KEY\""
      ],
      "metadata": {
        "id": "Oibu7sbhlEe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load environment variables\n",
        "\n",
        "from dotenv import load_dotenv,find_dotenv\n",
        "load_dotenv(find_dotenv())"
      ],
      "metadata": {
        "id": "as16gjVmlFEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LLM Wrapper**\n",
        "\n",
        "In this section, we're going to see how LangChain allows us to interact with LLMs."
      ],
      "metadata": {
        "id": "h1xFDtxdyTvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run basic query with OpenAI wrapper\n",
        "\n",
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI(model_name=\"text-davinci-003\")\n",
        "llm(\"explain large language models in one sentence\")"
      ],
      "metadata": {
        "id": "3Bz8f6jpyOnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* It takes in a \"text-davinci-003\" LLM.\n",
        "* The output is something similar to when you run it by the OpenAI API directly.\n"
      ],
      "metadata": {
        "id": "HCfDnGvTy9Ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import schema for chat messages and ChatOpenAI in order to query chatmodels GPT-3.5-turbo or GPT-4\n",
        "\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "Nk7NAhtwyOkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the LLM\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\",temperature=0.3)\n",
        "\n",
        "# List of messages to send to the chat model\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are an expert data scientist\"),\n",
        "    HumanMessage(content=\"Write a Python script that trains a neural network on simulated data \")\n",
        "]\n",
        "\n",
        "# Response is stored here\n",
        "response=chat(messages)\n",
        "\n",
        "print(response.content,end='\\n')"
      ],
      "metadata": {
        "id": "q4-AWy01yOie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Breakdown**\n",
        "\n",
        "1. **Initializing the ChatOpenAI instance:**\n",
        "\n",
        "  ```python\n",
        "  chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.3)\n",
        "\n",
        "  ```\n",
        "\n",
        "* This creates an instance of the `ChatOpenAI` class, which is a wrapper for interacting with OpenAI's chat models.\n",
        "* `model_name=\"gpt-3.5-turbo\"` specifies that we're using the \"gpt-3.5-turbo\" model.\n",
        "* `temperature=0.3` sets the temperature parameter to 0.3, which controls the randomness of the model's output. A lower temperature like 0.3 makes the output more focused and deterministic, while a higher temperature makes it more creative and unpredictable.\n",
        "\n",
        "2. **Defining the messages:**\n",
        "\n",
        "```python\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are an expert data scientist\"),\n",
        "    HumanMessage(content=\"Write a Python script that trains a neural network on simulated data \")\n",
        "]\n",
        "```\n",
        "\n",
        "* This creates a list of messages that will be sent to the chat model.\n",
        "* **`SystemMessage`** provides overall instructions or context to the model. Here, it's telling the model to act as an \"expert data scientist.\"\n",
        "* **`HumanMessage`** represents the user's input or query. In this case, it's asking the model to \"Write a Python script that trains a neural network on simulated data.\"\n",
        "\n",
        "**In essence,**\n",
        "\n",
        "This sets up a conversation with the \"gpt-3.5-turbo\" chat model, provides it with a system prompt and a user query, and then prints the model's response.\n",
        "\n"
      ],
      "metadata": {
        "id": "Zo5vxVQ6zmol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Prompt Templates**\n",
        "\n",
        "Prompt templates are pre-defined recipes for generating prompts to feed to Language Models (LLMs). Instead of hardcoding the entire prompt string every time, you can use templates with placeholders (variables) that are filled in later. This makes your prompts more flexible and reusable.\n",
        "\n",
        "LangChain provides the `PromptTemplate` class for creating and managing these templates."
      ],
      "metadata": {
        "id": "brTPffCM1qEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import prompt and define PromptTemplate\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "You are an expert data scientist with an expertise in building deep learning models.\n",
        "Explain the concept of {concept} in a couple of lines\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"concept\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "# Run LLM with PromptTemplate\n",
        "\n",
        "llm(prompt.format(concept=\"autoencoder\"))"
      ],
      "metadata": {
        "id": "Sv33ycg51pzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Breakdown**\n",
        "\n",
        "1. **Prompt Creation**\n",
        "* `input_variables=[\"concept\"]`: This specifies the name of the variable(s) that will be used in the template.\n",
        "* `template=template`: This assigns the template string you defined earlier.\n",
        "\n",
        "2. **Using the Prompt**\n",
        "* `prompt.format(concept=\"autoencoder\")`: This fills in the {concept} placeholder with the value \"autoencoder\", generating the complete prompt string.\n",
        "* `llm(...)`: This sends the formatted prompt to the LLM (which you initialized earlier) to get a response.\n",
        "\n",
        "**In essence,**\n",
        "\n",
        "* It creates a reusable template for asking the LLM to explain a data science concept.\n",
        "* Provides a way to easily change the concept being asked about without rewriting the entire prompt.\n",
        "* Makes the code more organized and easier to understand.\n"
      ],
      "metadata": {
        "id": "NkHJULX55v3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Chains**\n",
        "\n",
        "\n",
        "In this section we're going to see how to link multiple LangChain components together to create more complex workflows. This is useful for building applications where you want to process information in stages or combine the outputs of different components.\n",
        "\n",
        "LangChain provides the `LLMChain` class for creating chains.\n"
      ],
      "metadata": {
        "id": "uiLmBl7r7AFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initializing Chain**\n",
        "\n",
        "* We will first define a chain using the language model (`llm`) and prompt (`prompt`) as arguments.\n",
        "* This chain combines the OpenAI language model (`llm`) with the previously defined prompt (`prompt`) that asks for an explanation of a data science concept."
      ],
      "metadata": {
        "id": "fJ25FaXt9WAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import LLMChain and define chain with language model and prompt as arguments.\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Run the chain only specifying the input variable.\n",
        "print(chain.run(\"autoencoder\"))"
      ],
      "metadata": {
        "id": "IqfYB2VO1ptY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining a Sequential Chain**\n",
        "\n",
        "* Then we define a second prompt (`second_prompt`) that asks for a simplified explanation of the concept, as if explaining it to a five-year-old."
      ],
      "metadata": {
        "id": "RbKxzuai94Tb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a second prompt\n",
        "\n",
        "second_prompt = PromptTemplate(\n",
        "    input_variables=[\"ml_concept\"],\n",
        "    template=\"Turn the concept description of {ml_concept} and explain it to me like I'm five in 500 words\",\n",
        ")\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
      ],
      "metadata": {
        "id": "WPC_DGcS64aL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now we import `SimpleSequentialChain` and define a new chain (`overall_chain`) that combines the first two chains in sequence.\n",
        "\n",
        "* This means that the output of the first chain (the initial explanation) will be used as input to the second chain (to generate the simplified explanation).\n",
        "\n",
        "* Finally, we run the `overall_chain` by specifying only the input variable for the first chain (e.g., \"autoencoder\")."
      ],
      "metadata": {
        "id": "lljwEDOxUtA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a sequential chain using the two chains above: the second chain takes the output of the first chain as input\n",
        "\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
        "\n",
        "# Run the chain specifying only the input variable for the first chain.\n",
        "explanation = overall_chain.run(\"autoencoder\")\n",
        "print(explanation)"
      ],
      "metadata": {
        "id": "2-JfCJ2g64XZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**In essence,**\n",
        "\n",
        "* We saw how to create and run individual chains using `LLMChain`.\n",
        "* It shows how to combine chains sequentially using `SimpleSequentialChain` to create more complex workflows.\n",
        "* The sequential chain enables the output of one chain to be used as input to another, allowing for multi-step information processing."
      ],
      "metadata": {
        "id": "Bm1_Ucnh-JlS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Embeddings & VectorStores**\n",
        "\n",
        "**Embeddings** are essentially numerical representations of text. They capture the semantic meaning of words and phrases by converting them into vectors (lists of numbers). Similar words and phrases have similar vector representations, allowing you to perform tasks like similarity search.\n",
        "\n",
        "**Vectorstores** are databases specifically designed to store and search these vector representations (embeddings). They enable efficient retrieval of relevant information based on semantic similarity.\n",
        "We will be using the **Pinecone** vector database.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UcsyV59M-vpS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Splitting**\n",
        "\n",
        "* We shall begin by importing `RecursiveCharacterTextSplitter` from LangChain for splitting text into smaller chunks.\n",
        "* `text_splitter` is initialized with a chunk size of 100 characters and no overlap.\n",
        "* `texts` is created by splitting the explanation variable into smaller documents using create_documents."
      ],
      "metadata": {
        "id": "KdHJe4tOUwoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import utility for splitting up texts and split up the explanation given above into document chunks\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 100,\n",
        "    chunk_overlap  = 0,\n",
        ")\n",
        "\n",
        "texts = text_splitter.create_documents([explanation])"
      ],
      "metadata": {
        "id": "tSkvw_iV64TA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Individual text chunks can be accessed with \"page_content\"\n",
        "\n",
        "texts[0].page_content"
      ],
      "metadata": {
        "id": "zYQdJv33zU5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embeddings**\n",
        "\n",
        "* `OpenAIEmbeddings` is imported to create embeddings using OpenAI's models.\n",
        "* `embeddings` is initialized using the \"ada\" model."
      ],
      "metadata": {
        "id": "mfyXIofyVC51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import and instantiate OpenAI embeddings\n",
        "\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model_name=\"ada\")"
      ],
      "metadata": {
        "id": "EZYzvIwCzU24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Query Embedding**\n",
        "\n",
        "* `embed_query` is used to turn the first text chunk (`texts[0].page_content`) into a vector representation (embedding).\n",
        "* The resulting embedding (`query_result`) is printed."
      ],
      "metadata": {
        "id": "ASJ7Q3h2VNwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn the first text chunk into a vector with the embedding\n",
        "\n",
        "query_result = embeddings.embed_query(texts[0].page_content)\n",
        "print(query_result)"
      ],
      "metadata": {
        "id": "3KbdYIBNzU0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vectorstore (Pinecone)**\n",
        "\n",
        "* We import `pinecone` and initialize the Pinecone client with API key and environment.\n",
        "* `Pinecone.from_documents` is used to create a Pinecone index named `langchain-quickstart` and upload the text chunks (`texts`) with their embeddings."
      ],
      "metadata": {
        "id": "EeeOzy1IVYem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import and initialize Pinecone client\n",
        "\n",
        "import os\n",
        "import pinecone\n",
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "\n",
        "pinecone.init(\n",
        "    api_key=os.getenv('PINECONE_API_KEY'),\n",
        "    environment=os.getenv('PINECONE_ENV')\n",
        ")"
      ],
      "metadata": {
        "id": "DG2uftVyzUyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload vectors to Pinecone\n",
        "\n",
        "index_name = \"langchain-quickstart\"\n",
        "search = Pinecone.from_documents(texts, embeddings, index_name=index_name)"
      ],
      "metadata": {
        "id": "l1Pke3ycyOfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Similarity Search**\n",
        "\n",
        "* A query is defined: `\"What is magical about an autoencoder?\"`\n",
        "* `similarity_search` is used to search the Pinecone index for documents that are semantically similar to the query.\n",
        "* The search results (`result`) are printed."
      ],
      "metadata": {
        "id": "h27CnB8hVoX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Do a simple vector similarity search\n",
        "\n",
        "query = \"What is magical about an autoencoder?\"\n",
        "result = search.similarity_search(query)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "YoNBG32OT4HJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In essence,**\n",
        "\n",
        "We learnt how to split text into chunks, create embeddings for these chunks using OpenAI's embedding models, store them in a Pinecone vectorstore, and perform a similarity search."
      ],
      "metadata": {
        "id": "zKYchCnpV1zQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Agents**\n",
        "\n",
        "We will learn how to use LangChain to create an agent that can execute Python code. This allows you to combine the power of LLMs with the ability to run code and interact with the outside world.\n",
        "\n",
        "We will be importing the following classes to achieve this:\n",
        "\n",
        "\n",
        "* `create_python_agent`: Used to create an agent specifically for executing Python code.\n",
        "* `PythonREPLTool`: Provides the functionality to interact with a Python Read-Eval-Print Loop (REPL), allowing the agent to run Python code.\n",
        "* `PythonREPL`: The actual Python REPL environment used by the tool.\n",
        "* `OpenAI`: The language model used by the agent (OpenAI's models in this case)."
      ],
      "metadata": {
        "id": "xmeffByPWHXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Python REPL tool and instantiate Python agent\n",
        "\n",
        "from langchain.agents.agent_toolkits import create_python_agent\n",
        "from langchain.tools.python.tool import PythonREPLTool\n",
        "from langchain.python import PythonREPL\n",
        "from langchain.llms.openai import OpenAI\n",
        ""
      ],
      "metadata": {
        "id": "YgLK1OlQT4C-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor = create_python_agent(\n",
        "    llm=OpenAI(temperature=0, max_tokens=1000),\n",
        "    tool=PythonREPLTool(),\n",
        "    verbose=True\n",
        ")\n",
        ""
      ],
      "metadata": {
        "id": "EZXWOhUQW-Ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Breakdown**\n",
        "* `create_python_agent` is called to create the agent.\n",
        "* `llm=OpenAI(temperature=0, max_tokens=1000)`: Specifies the language model to use (OpenAI with temperature 0 and a maximum of 1000 tokens).\n",
        "* `tool=PythonREPLTool()`: Provides the Python REPL tool to the agent.\n",
        "* `verbose=True`: Enables verbose output, showing the agent's thought process and actions."
      ],
      "metadata": {
        "id": "sHozAEl8W-0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the Python agent\n",
        "\n",
        "agent_executor.run(\"Find the roots (zeros) if the quadratic function 3 * x**2 + 2*x -1\")"
      ],
      "metadata": {
        "id": "htyP_08pT4As"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Breakdown**\n",
        "\n",
        "* `agent_executor.run(...)` is used to execute the agent with a given task.\n",
        "* The task in this case is to find the roots of the quadratic function."
      ],
      "metadata": {
        "id": "rglLtdXUXO-_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In essence,**\n",
        "\n",
        "We created a Python agent that leverages an LLM (OpenAI) to understand instructions and a Python REPL tool to execute Python code. This enables the agent to solve tasks that require code execution, such as finding the roots of a quadratic equation. The verbose=True option provides insights into how the agent decides on actions and generates code to accomplish the given task."
      ],
      "metadata": {
        "id": "I-He8uQ1XZj0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WH7PVkl3yOdT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
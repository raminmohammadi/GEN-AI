{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUgRVUfXQKl4"
      },
      "source": [
        "<h1>\n",
        "ðŸ¦œðŸ”—LangChain\n",
        "</h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDUpAOnYQQSo"
      },
      "source": [
        "# **Brief Recap**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxHI6AdUQRdh"
      },
      "source": [
        "**LangChain** is an open source framework for building applications based on large language models (LLMs). LangChain provides tools and abstractions to improve the customization, accuracy, and relevancy of the information the models generate. For example, developers can use LangChain components to build new prompt chains or customize existing templates. LangChain also includes components that allow LLMs to access new data sets without retraining.\n",
        "\n",
        "With LangChain, organizations can repurpose LLMs for domain-specific applications without retraining or fine-tuning. Development teams can build complex applications referencing proprietary information to augment model responses. For example, you can use LangChain to build applications that read data from stored internal documents and summarize them into conversational responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWQGPeAmYLMq"
      },
      "source": [
        "# **Architecture**\n",
        "\n",
        "<img src='assets/arch.png' width=450>\n",
        "\n",
        "Here's an architecture and workflow of a LangChain-powered document processing and question-answering system. Let me break down its key components:\n",
        "\n",
        "* **Input Processing**\n",
        "  * The system starts with PDF documents on the left side\n",
        "  * These documents are split into multiple chunks of text/documents\n",
        "  * Each chunk goes through an embedding process (represented by binary code icons)\n",
        "* **Data Processing Flow**\n",
        "  * The text chunks are converted into embeddings (vector representations)\n",
        "  * These embeddings are stored in a Vector Store (shown as a database icon)\n",
        "  * The system uses specific technologies like:\n",
        "    * Amazon Aurora\n",
        "    * PostgreSQL with pgvector\n",
        "    * Knowledge Base functionality\n",
        "* **Query Processing**\n",
        "  * A user inputs a question (\"What is a neural network?\")\n",
        "  * The question goes through Question Embedding\n",
        "  * A Semantic Search is performed against the vector store\n",
        "  * The search produces Ranked Results\n",
        "* **Response Generation**\n",
        "  * The ranked results are processed by an LLM (Language Learning Model)\n",
        "  * The LLM generates the final Answer back to the user\n",
        "\n",
        "LangChain combines document processing, vector embeddings, and language models to create a comprehensive question-answering system.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkFEkfkHaQtS"
      },
      "source": [
        "# **Use Cases**\n",
        "\n",
        "* **Document Processing and Analysis**\n",
        "  * Parsing complex documents to extract structured information into JSON or tables\n",
        "  * Automated extraction of dates, quantities, and transaction details from financial documents\n",
        "  * Document classification using few-shot prompting\n",
        "  * Processing and analyzing large volumes of text documents\n",
        "* **Question Answering Systems**\n",
        "  * Building chatbots and virtual assistants for customer support\n",
        "  * Creating retrieval-based QA systems with document context\n",
        "  * Implementing conversational agents that can access and reference internal documents\n",
        "  * Developing SQL-based QA systems that work with various database dialects\n",
        "* **Content Generation and Summarization**\n",
        "  * Generating executive summaries of documents and meeting notes\n",
        "  * Creating summaries of large documents using map-reduce techniques\n",
        "  * Producing concise summaries of financial reports and earnings documents\n",
        "  * Translating content across multiple languages\n",
        "* **Conversational AI**\n",
        "  * Context-aware chatbots for customer support\n",
        "  * Virtual agents that can access and reference company documentation\n",
        "  * Automated appointment scheduling and customer service systems\n",
        "\n",
        "* **Code-Related Tasks**\n",
        "  * Code analysis for detecting bugs and security vulnerabilities\n",
        "  * Development of coding assistants to improve programmer productivity\n",
        "  * Custom development environments with integrated LLM capabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIUO2bEp01nA"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfyOEN7AbTa9"
      },
      "source": [
        "# **Implementation**\n",
        "\n",
        "\n",
        "There are majorly 3 main components in LangChain:\n",
        "\n",
        "1. **Components**\n",
        "  * **LLM Wrappers**: There are LLM wrappers that allows us to connect to the LLMs like GPT 4, ollama, gemini.\n",
        "  * **Prompt Templates**: They allow us to avoid hard code text the input to the LLMs.\n",
        "  * **Indexes**: Allow us to extract relevant info from the LLMs. For eg: PineCone VectorStore.\n",
        "\n",
        "2. **Chains**\n",
        "  * They allow us to combine multiple components together to solve a specific task building an entire LLM application\n",
        "\n",
        "3. **Agents**\n",
        "  * It allows the LLMs to interact with external APIs.\n",
        "\n",
        "We will be unpacking these concepts discussing and implementing each of them.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4HAfPNSy_Ky"
      },
      "source": [
        "### **Initial Setup**\n",
        "\n",
        "Download ollama from [here](https://ollama.com/).\n",
        "\n",
        "Then,\n",
        "\n",
        "```\n",
        "ollama pull llama3.2\n",
        "```\n",
        "\n",
        "Then pip install the following libraries:\n",
        "```python\n",
        "langchain\n",
        "langchain_ollama\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1xFDtxdyTvV"
      },
      "source": [
        "### **LLM Wrapper**\n",
        "\n",
        "In this section, we're going to see how LangChain allows us to interact with LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Bz8f6jpyOnf"
      },
      "outputs": [],
      "source": [
        "# Run basic query with OpenAI wrapper\n",
        "\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain.schema import HumanMessage\n",
        "llm = ChatOllama(\n",
        "    model=\"llama3.2\",\n",
        "    temperature=0,\n",
        "    # other params...\n",
        ")\n",
        "response = llm([HumanMessage(content=\"Explain large language models in one sentence\")])\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCfDnGvTy9Ty"
      },
      "source": [
        "* It takes in a \"text-davinci-003\" LLM.\n",
        "* The output is something similar to when you run it by the OpenAI API directly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nk7NAhtwyOkV"
      },
      "outputs": [],
      "source": [
        "# import schema for chat messages and ChatOpenAI in order to query chatmodels GPT-3.5-turbo or GPT-4\n",
        "\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "from langchain_ollama import ChatOllama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo5vxVQ6zmol"
      },
      "source": [
        "**Breakdown**\n",
        "\n",
        "1. **Initializing the Ollama instance:**\n",
        "\n",
        "  ```python\n",
        "  chat = ChatOllama(model=\"llama3.2\",temperature=0.3)\n",
        "\n",
        "  ```\n",
        "\n",
        "* This creates an instance of the `Ollama` class, which is a wrapper for interacting with Meta's chat models that have been setup locally.\n",
        "* `model_name=\"llama3.2\"` specifies that we're using the \"llama3.2\" model.\n",
        "* `temperature=0.3` sets the temperature parameter to 0.3, which controls the randomness of the model's output. A lower temperature like 0.3 makes the output more focused and deterministic, while a higher temperature makes it more creative and unpredictable.\n",
        "\n",
        "2. **Defining the messages:**\n",
        "\n",
        "  ```python\n",
        "  messages = [\n",
        "      SystemMessage(content=\"You are an expert data scientist\"),\n",
        "      HumanMessage(content=\"Write a Python script that trains a neural network on simulated data \")\n",
        "  ]\n",
        "  ```\n",
        "\n",
        "* This creates a list of messages that will be sent to the chat model.\n",
        "* **`SystemMessage`** provides overall instructions or context to the model. Here, it's telling the model to act as an \"expert data scientist.\"\n",
        "* **`HumanMessage`** represents the user's input or query. In this case, it's asking the model to \"Write a Python script that trains a neural network on simulated data.\"\n",
        "\n",
        "**In essence,**\n",
        "\n",
        "This sets up a conversation with the \"llama3.2\" chat model, provides it with a system prompt and a user query, and then prints the model's response.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4-AWy01yOie"
      },
      "outputs": [],
      "source": [
        "# Initializing the LLM\n",
        "chat = ChatOllama(model=\"llama3.2\",temperature=0.3)\n",
        "\n",
        "# List of messages to send to the chat model\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are an expert data scientist\"),\n",
        "    HumanMessage(content=\"Write a Python script that trains a neural network on simulated data \")\n",
        "]\n",
        "\n",
        "# Response is stored here\n",
        "response=chat(messages)\n",
        "\n",
        "print(response.content,end='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brTPffCM1qEI"
      },
      "source": [
        "### **Prompt Templates**\n",
        "\n",
        "Prompt templates are pre-defined recipes for generating prompts to feed to Language Models (LLMs). Instead of hardcoding the entire prompt string every time, you can use templates with placeholders (variables) that are filled in later. This makes your prompts more flexible and reusable.\n",
        "\n",
        "LangChain provides the `PromptTemplate` class for creating and managing these templates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sv33ycg51pzu"
      },
      "outputs": [],
      "source": [
        "# Import prompt and define PromptTemplate\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "You are an expert data scientist with an expertise in building deep learning models.\n",
        "Explain the concept of {concept} in a couple of lines\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"concept\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "# Run LLM with PromptTemplate\n",
        "\n",
        "llm([HumanMessage(content = prompt.format(concept=\"autoencoder\"))])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkHJULX55v3c"
      },
      "source": [
        "**Breakdown**\n",
        "\n",
        "1. **Prompt Creation**\n",
        "* `input_variables=[\"concept\"]`: This specifies the name of the variable(s) that will be used in the template.\n",
        "* `template=template`: This assigns the template string you defined earlier.\n",
        "\n",
        "2. **Using the Prompt**\n",
        "* `prompt.format(concept=\"autoencoder\")`: This fills in the {concept} placeholder with the value \"autoencoder\", generating the complete prompt string.\n",
        "* `llm(...)`: This sends the formatted prompt to the LLM (which you initialized earlier) to get a response.\n",
        "\n",
        "**In essence,**\n",
        "\n",
        "* It creates a reusable template for asking the LLM to explain a data science concept.\n",
        "* Provides a way to easily change the concept being asked about without rewriting the entire prompt.\n",
        "* Makes the code more organized and easier to understand.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiLmBl7r7AFF"
      },
      "source": [
        "### **Chains**\n",
        "\n",
        "\n",
        "In this section we're going to see how to link multiple LangChain components together to create more complex workflows. This is useful for building applications where you want to process information in stages or combine the outputs of different components.\n",
        "\n",
        "LangChain provides the `LLMChain` class for creating chains.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ25FaXt9WAh"
      },
      "source": [
        "**Initializing Chain**\n",
        "\n",
        "* We will first define a chain using the language model (`llm`) and prompt (`prompt`) as arguments.\n",
        "* This chain combines the OpenAI language model (`llm`) with the previously defined prompt (`prompt`) that asks for an explanation of a data science concept."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqfYB2VO1ptY"
      },
      "outputs": [],
      "source": [
        "# Import LLMChain and define chain with language model and prompt as arguments.\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Run the chain only specifying the input variable.\n",
        "print(chain.run(\"autoencoder\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbKxzuai94Tb"
      },
      "source": [
        "**Defining a Sequential Chain**\n",
        "\n",
        "* Then we define a second prompt (`second_prompt`) that asks for a simplified explanation of the concept, as if explaining it to a five-year-old."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPC_DGcS64aL"
      },
      "outputs": [],
      "source": [
        "# Define a second prompt\n",
        "\n",
        "second_prompt = PromptTemplate(\n",
        "    input_variables=[\"ml_concept\"],\n",
        "    template=\"Turn the concept description of {ml_concept} and explain it to me like I'm five in 500 words\",\n",
        ")\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lljwEDOxUtA0"
      },
      "source": [
        "* Now we import `SimpleSequentialChain` and define a new chain (`overall_chain`) that combines the first two chains in sequence.\n",
        "\n",
        "* This means that the output of the first chain (the initial explanation) will be used as input to the second chain (to generate the simplified explanation).\n",
        "\n",
        "* Finally, we run the `overall_chain` by specifying only the input variable for the first chain (e.g., \"autoencoder\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-JfCJ2g64XZ"
      },
      "outputs": [],
      "source": [
        "# Define a sequential chain using the two chains above: the second chain takes the output of the first chain as input\n",
        "\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
        "\n",
        "# Run the chain specifying only the input variable for the first chain.\n",
        "explanation = overall_chain.run(\"autoencoder\")\n",
        "print(explanation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm1_Ucnh-JlS"
      },
      "source": [
        "\n",
        "\n",
        "**In essence,**\n",
        "\n",
        "* We saw how to create and run individual chains using `LLMChain`.\n",
        "* It shows how to combine chains sequentially using `SimpleSequentialChain` to create more complex workflows.\n",
        "* The sequential chain enables the output of one chain to be used as input to another, allowing for multi-step information processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmeffByPWHXG"
      },
      "source": [
        "### **Agents**\n",
        "\n",
        "We will learn how to use LangChain to create an agent that can execute Python code. This allows you to combine the power of LLMs with the ability to run code and interact with the outside world."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZXWOhUQW-Ql"
      },
      "outputs": [],
      "source": [
        "agent_executor = create_python_agent(\n",
        "    llm=OllamaLLM(model=\"llama3.2\", temperature=0, max_tokens=1000),\n",
        "    tool=PythonREPLTool(),\n",
        "    verbose=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htyP_08pT4As"
      },
      "outputs": [],
      "source": [
        "# Execute the Python agent\n",
        "\n",
        "agent_executor.run(\"Find the roots (zeros) if the quadratic function 3 * x**2 + 2*x -1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize the LLM**\n",
        "\n",
        "This creates an instance of the ChatOllama class, specifying the model name, temperature, and verbosity."
      ],
      "metadata": {
        "id": "9l6gAafX7Gk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "from langchain.agents import tool\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n",
        "from langchain.agents.format_scratchpad.openai_tools import format_to_openai_tool_messages\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = ChatOllama(\n",
        "    model=\"llama3.2\",\n",
        "    temperature=0,\n",
        "    verbose=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "WcNlxY7a7gFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defne and setup a simple tool**\n",
        "\n",
        "* This defines a custom tool called `get_word_length` that takes a word as input and returns its length. The `@tool` decorator registers this function as a tool that the agent can use.\n",
        "* Then create a list of tools that the agent will have access to. In this case, it only includes the `get_word_length` tool.\n"
      ],
      "metadata": {
        "id": "0O5hoTCm7i6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple tool\n",
        "@tool\n",
        "def get_word_length(word: str) -> int:\n",
        "    \"\"\"Returns the length of a word.\"\"\"\n",
        "    return len(word)\n",
        "\n",
        "# Set up tools\n",
        "tools = [get_word_length]\n"
      ],
      "metadata": {
        "id": "uFeZX-937iZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a prompt template**\n",
        "\n",
        "This defines a prompt template for the agent. The prompt includes system instructions, user input, and a placeholder for the agent's scratchpad, where it can store intermediate steps.\n"
      ],
      "metadata": {
        "id": "vb7897dK7ymT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create prompt template\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are very powerful assistant\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "])\n"
      ],
      "metadata": {
        "id": "jyTTk5b67w7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bind tools to LLM**\n",
        "\n",
        "This binds the defined tools to the LLM instance, allowing the agent to access and use the tools.\n"
      ],
      "metadata": {
        "id": "X0Xn04JH8Rgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bind tools to LLM\n",
        "llm_with_tools = llm.bind_tools(tools)\n"
      ],
      "metadata": {
        "id": "gheZHLY-8V7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create agent**\n",
        "\n",
        "This creates the agent by combining the input, scratchpad, prompt, LLM with tools, and output parser.\n"
      ],
      "metadata": {
        "id": "rzRWQ5BY8gK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create agent\n",
        "agent = (\n",
        "    {\n",
        "        \"input\": lambda x: x[\"input\"],\n",
        "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
        "            x[\"intermediate_steps\"]\n",
        "        ),\n",
        "    }\n",
        "    | prompt\n",
        "    | llm_with_tools\n",
        "    | OpenAIToolsAgentOutputParser()\n",
        ")\n"
      ],
      "metadata": {
        "id": "hQzFSdhi8fPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create agent executor**\n",
        "\n",
        "This creates an agent executor that manages the execution of the agent's actions.\n"
      ],
      "metadata": {
        "id": "MhH20wBt8lo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create agent executor\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n"
      ],
      "metadata": {
        "id": "rZgBBm738q3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test the agent**\n",
        "\n",
        "This invokes the agent with a test input and prints the output.\n"
      ],
      "metadata": {
        "id": "D3Y_3lLX8rRT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WH7PVkl3yOdT"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Test the agent\n",
        "result = agent_executor.invoke({\"input\": \"How many letters in the word education?\"})\n",
        "print(f\"[Output] --> {result['output']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In essence**,\n",
        "\n",
        "This demonstrates how to empower a Language Model (LLM) with the ability to interact with the external world and execute actions beyond simple text generation. It achieves this by creating an agent that can leverage external tools, in this case, a custom Python function."
      ],
      "metadata": {
        "id": "T7cUu2wr9rs_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzTcGkTrvx90"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Deep Reinforcement Learning</h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief Recap of Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deep Reinforcement Learning (DRL) is a machine learning approach that combines deep neural networks with reinforcement learning to create systems that can learn optimal behaviors through interaction with an environment.\n",
    "\n",
    "- It revolutionized autonomous decision-making by enabling agents to learn complex strategies directly from raw input data without explicit programming of rules or behaviors.\n",
    "\n",
    "- Deep Reinforcement Learning has several key advantages over traditional machine learning approaches:\n",
    "    1. End-to-end learning: Can learn directly from raw sensory inputs to actions\n",
    "    2. Adaptability: Capable of learning and adjusting strategies in dynamic environments\n",
    "    3. Generalization: Can transfer learned skills to similar but previously unseen situations\n",
    "\n",
    "- DRL uses various algorithms like Deep Q-Networks (DQN), Policy Gradients, and Actor-Critic methods to achieve efficient learning in complex environments with large state and action spaces.\n",
    "\n",
    "- It has become foundational for many cutting-edge applications, including game playing, robotics control, autonomous vehicles, and resource management systems.\n",
    "\n",
    "- Popular DRL frameworks and implementations include OpenAI Gym, Stable Baselines, RLlib, and TensorFlow-Agents, each offering different features and optimization capabilities.\n",
    "\n",
    "- These technologies continue to evolve, finding new applications across industries such as healthcare, finance, manufacturing, and logistics optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture of Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deep Reinforcement Learning architectures are specialized neural network systems designed to learn optimal decision-making policies through environment interaction and reward optimization.\n",
    "\n",
    "- They revolutionized autonomous learning by combining deep neural networks with traditional reinforcement learning principles, enabling end-to-end learning from raw inputs to actions.\n",
    "\n",
    "- Deep RL architectures have several fundamental components:\n",
    "    1. Input Processing: Handles raw state information from the environment\n",
    "    2. Feature Extraction: Transforms raw inputs into meaningful representations\n",
    "    3. Policy/Value Estimation: Determines actions or state values\n",
    "    4. Action Selection: Chooses optimal actions based on learned policies\n",
    "\n",
    "- The architecture typically consists of multiple interconnected layers:\n",
    "    1. Input Layer: Receives state observations from the environment\n",
    "    2. Hidden Layers: Process and transform state information\n",
    "    3. Output Layer: Generates action probabilities or value estimates\n",
    "    4. Memory Buffer: Stores experience for replay and learning\n",
    "\n",
    "- These architectures employ various optimization techniques:\n",
    "    1. Experience Replay: Stores and reuses past experiences\n",
    "    2. Target Networks: Stabilizes training through delayed updates\n",
    "    3. Advantage Estimation: Improves policy gradient calculations\n",
    "\n",
    "- Popular architectural variants include:\n",
    "    - Deep Q-Networks (DQN) for discrete action spaces\n",
    "    - Policy Gradient Networks for continuous action spaces\n",
    "    - Actor-Critic Networks for combined policy and value learning\n",
    "\n",
    "- These architectures continue to evolve, incorporating new advances in deep learning such as attention mechanisms, transformers, and multi-agent learning capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Reinforcement Learning (DRL) is a powerful approach that combines deep learning and reinforcement learning to solve complex problems in various domains such as:\n",
    "\n",
    "- Robotics\n",
    "    - **Autonomous Navigation**: DRL is used to train robots for navigation tasks in dynamic environments. Robots learn to make decisions based on sensory input to navigate through obstacles and reach goals.\n",
    "    - **Manipulation Tasks**: In industrial settings, DRL helps robots learn to perform tasks like picking, placing, and assembling objects with precision.\n",
    "\n",
    "- Game Playing\n",
    "    - **Atari Games**: DRL algorithms, such as Deep Q-Networks (DQN), have been successfully applied to play Atari games, achieving superhuman performance in many cases.\n",
    "    - **Board Games**: DRL has been used in games like Go (AlphaGo) and chess, where agents learn strategies through self-play, leading to groundbreaking results.\n",
    "\n",
    "- Finance\n",
    "    - **Algorithmic Trading**: DRL can optimize trading strategies by learning from historical market data and making buy/sell decisions to maximize returns.\n",
    "    - **Portfolio Management**: It helps in dynamically adjusting asset allocations in a portfolio to balance risk and reward based on changing market conditions.\n",
    "\n",
    "- Healthcare\n",
    "    - **Personalized Treatment Plans**: DRL can assist in developing personalized treatment strategies by learning optimal interventions for individual patients based on their health data.\n",
    "    - **Drug Discovery**: In pharmaceuticals, DRL models are utilized to explore molecular space for potential drug candidates by predicting interactions and outcomes.\n",
    "\n",
    "- Transportation\n",
    "    - **Traffic Signal Control**: DRL algorithms optimize traffic light timings to reduce congestion and improve traffic flow in urban areas.\n",
    "    - **Autonomous Vehicles**: DRL is integral to developing autonomous driving systems, enabling vehicles to make real-time decisions based on environmental conditions.\n",
    "\n",
    "- Natural Language Processing\n",
    "    - **Dialogue Systems**: DRL enhances conversational agents by optimizing response strategies based on user interactions, leading to more engaging and context-aware dialogues.\n",
    "    - **Text Summarization**: It is applied to learn how to summarize text effectively by evaluating the quality of generated summaries through reinforcement signals.\n",
    "\n",
    "- Energy Management\n",
    "    - **Smart Grid Optimization**: DRL can optimize energy distribution and consumption in smart grids, balancing supply and demand while minimizing costs.\n",
    "    - **Demand Response**: It helps in managing energy consumption patterns in response to changing prices and grid conditions, promoting efficient energy usage.\n",
    "\n",
    "- Game Development\n",
    "    - **Procedural Content Generation**: DRL is used to create dynamic and adaptive game content, enhancing player experience by adjusting difficulty levels based on player performance.\n",
    "    - **Player Behavior Modeling**: It can model and predict player behavior to improve game design and user engagement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing some core concepts of building a Deep Reinforcement Learning Model with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gym`: OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms. We use it to create the environment (CartPole in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `env = gym.make('CartPole-v1')`: We create an instance of the CartPole environment where the agent will interact and learn. The environment provides the state, action space, and rewards.\n",
    "- `state_shape`: This variable holds the shape of the state space (input) for the neural network. For CartPole, the state consists of position, velocity, angle, and angular velocity.\n",
    "- `num_actions`: This is the number of possible actions the agent can take, which is 2 (move left or right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_shape = env.observation_space.shape\n",
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Q-Network**: This neural network approximates the Q-value function. It takes the state as input and outputs the Q-values for all possible actions.\n",
    "    - `layers.Dense(128, activation='relu')`: We use fully connected layers (Dense) with 128 neurons, using the ReLU activation function to introduce non-linearity.\n",
    "    - `layers.Dense(num_actions)`: The output layer has `num_actions` neurons (2 for CartPole), each representing the predicted Q-value for that action.\n",
    "    - `Sequential`: A simple feed-forward neural network where each layer's output is the next layer's input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_q_network(state_shape, num_actions):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=state_shape),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(num_actions)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Replay Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Replay Buffer**: This class stores past experiences in the form of `(state, action, reward, next_state, done)` tuples.\n",
    "- **add()**: This method adds new experiences to the buffer. If the buffer is full, it replaces older experiences in a circular manner.\n",
    "- **sample()**: It randomly selects a batch of experiences from the buffer to break correlations between consecutive experiences. This helps stabilize training.\n",
    "- **Purpose**: Replay buffer allows the agent to learn from a wider variety of experiences, enhancing sample efficiency and reducing instability in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        self.buffer = []\n",
    "        self.max_size = size\n",
    "        self.size = 0\n",
    "\n",
    "    def add(self, experience):\n",
    "        if self.size < self.max_size:\n",
    "            self.buffer.append(experience)\n",
    "            self.size += 1\n",
    "        else:\n",
    "            self.buffer[self.size % self.max_size] = experience\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(len(self.buffer), batch_size)\n",
    "        return [self.buffer[i] for i in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon-Greedy Policy for Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Epsilon-Greedy Policy**: This policy balances exploration and exploitation.\n",
    "  - **Exploration**: With a probability `epsilon`, the agent takes a random action, encouraging exploration of new states.\n",
    "  - **Exploitation**: With a probability `1 - epsilon`, the agent chooses the action with the highest Q-value, exploiting current knowledge.\n",
    "- `np.random.rand() < epsilon`: Generates a random number between 0 and 1. If itâ€™s less than `epsilon`, the agent explores; otherwise, it exploits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(q_values, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(num_actions)\n",
    "    else:\n",
    "        return np.argmax(q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def train_dqn(env, episodes, batch_size=64, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, min_epsilon=0.1):\n",
    "    # Create Q-network and target Q-network\n",
    "    q_network = create_q_network(state_shape, num_actions)\n",
    "    target_q_network = create_q_network(state_shape, num_actions)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    replay_buffer = ReplayBuffer(100000)  # Large replay buffer\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- **Q-Network**: The main neural network that learns to approximate the Q-values.\n",
    "- **Target Q-Network**: A separate network used to stabilize training. The weights of this network are updated less frequently (every few episodes) to avoid oscillating Q-values during training.\n",
    "- **Optimizer**: Adam optimizer is used to minimize the loss by adjusting the network's weights.\n",
    "- **Loss Function**: Mean Squared Error is used to minimize the difference between the predicted Q-values and the target Q-values.\n",
    "- **Replay Buffer**: A buffer with a size of 100,000 to store experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            state_input = np.expand_dims(state, axis=0).astype(np.float32)\n",
    "            q_values = q_network(state_input)\n",
    "            action = epsilon_greedy_policy(q_values, epsilon)\n",
    "            \n",
    "            # Take the chosen action in the environment\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            replay_buffer.add((state, action, reward, next_state, done))\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- **Episode Loop**: The outer loop runs for the specified number of episodes. Each episode is a complete run of the environment (from start to terminal state).\n",
    "- `env.reset()`: Resets the environment to the initial state.\n",
    "- **Action Selection**: The agent selects an action using the epsilon-greedy policy based on the Q-values predicted by the Q-network.\n",
    "- **Environment Interaction**: `env.step(action)` executes the selected action and returns the next state, reward, and whether the episode is done.\n",
    "- **Experience Storage**: The `(state, action, reward, next_state, done)` tuple is stored in the replay buffer for future training.\n",
    "- **Total Reward**: Tracks the cumulative reward obtained by the agent in the episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "            if len(replay_buffer.buffer) > batch_size:\n",
    "                # Sample a batch from the replay buffer\n",
    "                experiences = replay_buffer.sample(batch_size)\n",
    "                states, actions, rewards, next_states, dones = map(np.array, zip(*experiences))\n",
    "\n",
    "                # Predict Q-values for the next states using the target network\n",
    "                next_q_values = target_q_network(next_states)\n",
    "                max_next_q_values = np.max(next_q_values, axis=1)\n",
    "\n",
    "                # Bellman equation for the target Q-value\n",
    "                targets = rewards + gamma * max_next_q_values * (1 - dones)\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- **Experience Sampling**: A batch of experiences is sampled from the replay buffer once there are enough experiences to fill the batch.\n",
    "- **Target Calculation**: \n",
    "  - `next_q_values`: The Q-values for the next states are predicted by the target network.\n",
    "  - `max_next_q_values`: The highest Q-value for the next state is chosen (maximizing future reward).\n",
    "  - **Bellman Equation**: The target Q-value is calculated using the Bellman equation: `reward + (discount factor * max future reward)`. The factor `(1 - dones)` ensures that no future reward is added if the episode is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "                # Gradient descent to update the Q-network\n",
    "                with tf.GradientTape() as tape:\n",
    "                    q_values = q_network(states)\n",
    "                    action_masks = tf.one_hot(actions, num_actions)\n",
    "                    q_values_taken = tf.reduce_sum(q_values * action_masks, axis=1)\n",
    "                    loss = loss_fn(targets, q_values_taken)\n",
    "\n",
    "                grads = tape.gradient(loss, q_network.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(grads, q_network.trainable_variables))\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- **Q-Value Prediction**: The Q-network predicts the Q-values for the batch of states.\n",
    "- **Action Mask**: A one-hot mask is created for the actions taken in those states to extract the Q-values corresponding to the actions the agent chose.\n",
    "- **Loss Calculation**: The difference between the predicted Q-values and the target Q-values is calculated using Mean Squared Error.\n",
    "- **Gradient Descent**: The gradients of the loss are computed with respect to the network's weights, and the optimizer applies these gradients to update the Q-network's parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "            if done:\n",
    "                print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")\n",
    "                break\n",
    "\n",
    "        # Update epsilon for the next episode\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "        # Periodically update the target network\n",
    "        if episode % 10 == 0:\n",
    "            target_q_network.set_weights(q_network.get_weights())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Build a Real world project to understand the concept of Deep Reinfocement Learning better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

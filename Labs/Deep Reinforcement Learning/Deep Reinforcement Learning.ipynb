{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Deep Reinforcement Learning</h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief Recap of Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deep Reinforcement Learning (DRL) is a machine learning approach that combines deep neural networks with reinforcement learning to create systems that can learn optimal behaviors through interaction with an environment.\n",
    "\n",
    "- It revolutionized autonomous decision-making by enabling agents to learn complex strategies directly from raw input data without explicit programming of rules or behaviors.\n",
    "\n",
    "- Deep Reinforcement Learning has several key advantages over traditional machine learning approaches:\n",
    "    1. End-to-end learning: Can learn directly from raw sensory inputs to actions\n",
    "    2. Adaptability: Capable of learning and adjusting strategies in dynamic environments\n",
    "    3. Generalization: Can transfer learned skills to similar but previously unseen situations\n",
    "\n",
    "- DRL uses various algorithms like Deep Q-Networks (DQN), Policy Gradients, and Actor-Critic methods to achieve efficient learning in complex environments with large state and action spaces.\n",
    "\n",
    "- It has become foundational for many cutting-edge applications, including game playing, robotics control, autonomous vehicles, and resource management systems.\n",
    "\n",
    "- Popular DRL frameworks and implementations include OpenAI Gym, Stable Baselines, RLlib, and TensorFlow-Agents, each offering different features and optimization capabilities.\n",
    "\n",
    "- These technologies continue to evolve, finding new applications across industries such as healthcare, finance, manufacturing, and logistics optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"static/image1.gif\" alt=\"Deep Reinforcement Learning Example\" style=\"width:50%;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture of Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deep Reinforcement Learning architectures are specialized neural network systems designed to learn optimal decision-making policies through environment interaction and reward optimization.\n",
    "\n",
    "- They revolutionized autonomous learning by combining deep neural networks with traditional reinforcement learning principles, enabling end-to-end learning from raw inputs to actions.\n",
    "\n",
    "- Deep RL architectures have several fundamental components:\n",
    "    1. Input Processing: Handles raw state information from the environment\n",
    "    2. Feature Extraction: Transforms raw inputs into meaningful representations\n",
    "    3. Policy/Value Estimation: Determines actions or state values\n",
    "    4. Action Selection: Chooses optimal actions based on learned policies\n",
    "\n",
    "- The architecture typically consists of multiple interconnected layers:\n",
    "    1. Input Layer: Receives state observations from the environment\n",
    "    2. Hidden Layers: Process and transform state information\n",
    "    3. Output Layer: Generates action probabilities or value estimates\n",
    "    4. Memory Buffer: Stores experience for replay and learning\n",
    "\n",
    "- These architectures employ various optimization techniques:\n",
    "    1. Experience Replay: Stores and reuses past experiences\n",
    "    2. Target Networks: Stabilizes training through delayed updates\n",
    "    3. Advantage Estimation: Improves policy gradient calculations\n",
    "\n",
    "- Popular architectural variants include:\n",
    "    - Deep Q-Networks (DQN) for discrete action spaces\n",
    "    - Policy Gradient Networks for continuous action spaces\n",
    "    - Actor-Critic Networks for combined policy and value learning\n",
    "\n",
    "- These architectures continue to evolve, incorporating new advances in deep learning such as attention mechanisms, transformers, and multi-agent learning capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"static/image2.jpg\" alt=\"Deep Reinforcement Learning\" style=\"width:50%;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Reinforcement Learning (DRL) is a powerful approach that combines deep learning and reinforcement learning to solve complex problems in various domains such as:\n",
    "\n",
    "- Robotics\n",
    "    - **Autonomous Navigation**: DRL is used to train robots for navigation tasks in dynamic environments. Robots learn to make decisions based on sensory input to navigate through obstacles and reach goals.\n",
    "    - **Manipulation Tasks**: In industrial settings, DRL helps robots learn to perform tasks like picking, placing, and assembling objects with precision.\n",
    "\n",
    "- Game Playing\n",
    "    - **Atari Games**: DRL algorithms, such as Deep Q-Networks (DQN), have been successfully applied to play Atari games, achieving superhuman performance in many cases.\n",
    "    - **Board Games**: DRL has been used in games like Go (AlphaGo) and chess, where agents learn strategies through self-play, leading to groundbreaking results.\n",
    "\n",
    "- Finance\n",
    "    - **Algorithmic Trading**: DRL can optimize trading strategies by learning from historical market data and making buy/sell decisions to maximize returns.\n",
    "    - **Portfolio Management**: It helps in dynamically adjusting asset allocations in a portfolio to balance risk and reward based on changing market conditions.\n",
    "\n",
    "- Healthcare\n",
    "    - **Personalized Treatment Plans**: DRL can assist in developing personalized treatment strategies by learning optimal interventions for individual patients based on their health data.\n",
    "    - **Drug Discovery**: In pharmaceuticals, DRL models are utilized to explore molecular space for potential drug candidates by predicting interactions and outcomes.\n",
    "\n",
    "- Transportation\n",
    "    - **Traffic Signal Control**: DRL algorithms optimize traffic light timings to reduce congestion and improve traffic flow in urban areas.\n",
    "    - **Autonomous Vehicles**: DRL is integral to developing autonomous driving systems, enabling vehicles to make real-time decisions based on environmental conditions.\n",
    "\n",
    "- Natural Language Processing\n",
    "    - **Dialogue Systems**: DRL enhances conversational agents by optimizing response strategies based on user interactions, leading to more engaging and context-aware dialogues.\n",
    "    - **Text Summarization**: It is applied to learn how to summarize text effectively by evaluating the quality of generated summaries through reinforcement signals.\n",
    "\n",
    "- Energy Management\n",
    "    - **Smart Grid Optimization**: DRL can optimize energy distribution and consumption in smart grids, balancing supply and demand while minimizing costs.\n",
    "    - **Demand Response**: It helps in managing energy consumption patterns in response to changing prices and grid conditions, promoting efficient energy usage.\n",
    "\n",
    "- Game Development\n",
    "    - **Procedural Content Generation**: DRL is used to create dynamic and adaptive game content, enhancing player experience by adjusting difficulty levels based on player performance.\n",
    "    - **Player Behavior Modeling**: It can model and predict player behavior to improve game design and user engagement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing some core concepts of building a Deep Reinforcement Learning Model with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gym`: OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms. We use it to create the environment (CartPole in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `env = gym.make('CartPole-v1')`: We create an instance of the CartPole environment where the agent will interact and learn. The environment provides the state, action space, and rewards.\n",
    "- `state_shape`: This variable holds the shape of the state space (input) for the neural network. For CartPole, the state consists of position, velocity, angle, and angular velocity.\n",
    "- `num_actions`: This is the number of possible actions the agent can take, which is 2 (move left or right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_shape = env.observation_space.shape\n",
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Q-Network**: This neural network approximates the Q-value function. It takes the state as input and outputs the Q-values for all possible actions.\n",
    "    - `layers.Dense(128, activation='relu')`: We use fully connected layers (Dense) with 128 neurons, using the ReLU activation function to introduce non-linearity.\n",
    "    - `layers.Dense(num_actions)`: The output layer has `num_actions` neurons (2 for CartPole), each representing the predicted Q-value for that action.\n",
    "    - `Sequential`: A simple feed-forward neural network where each layer's output is the next layer's input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_q_network(state_shape, num_actions):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=state_shape),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(num_actions)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Replay Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Replay Buffer**: This class stores past experiences in the form of `(state, action, reward, next_state, done)` tuples.\n",
    "- **add()**: This method adds new experiences to the buffer. If the buffer is full, it replaces older experiences in a circular manner.\n",
    "- **sample()**: It randomly selects a batch of experiences from the buffer to break correlations between consecutive experiences. This helps stabilize training.\n",
    "- **Purpose**: Replay buffer allows the agent to learn from a wider variety of experiences, enhancing sample efficiency and reducing instability in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        self.buffer = []\n",
    "        self.max_size = size\n",
    "        self.size = 0\n",
    "\n",
    "    def add(self, experience):\n",
    "        if self.size < self.max_size:\n",
    "            self.buffer.append(experience)\n",
    "            self.size += 1\n",
    "        else:\n",
    "            self.buffer[self.size % self.max_size] = experience\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(len(self.buffer), batch_size)\n",
    "        return [self.buffer[i] for i in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon-Greedy Policy for Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Epsilon-Greedy Policy**: This policy balances exploration and exploitation.\n",
    "  - **Exploration**: With a probability `epsilon`, the agent takes a random action, encouraging exploration of new states.\n",
    "  - **Exploitation**: With a probability `1 - epsilon`, the agent chooses the action with the highest Q-value, exploiting current knowledge.\n",
    "- `np.random.rand() < epsilon`: Generates a random number between 0 and 1. If it’s less than `epsilon`, the agent explores; otherwise, it exploits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(q_values, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(num_actions)\n",
    "    else:\n",
    "        return np.argmax(q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def train_dqn(env, episodes, batch_size=64, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, min_epsilon=0.1):\n",
    "    # Create Q-network and target Q-network\n",
    "    q_network = create_q_network(state_shape, num_actions)\n",
    "    target_q_network = create_q_network(state_shape, num_actions)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    replay_buffer = ReplayBuffer(100000)  # Large replay buffer\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- **Q-Network**: The main neural network that learns to approximate the Q-values.\n",
    "- **Target Q-Network**: A separate network used to stabilize training. The weights of this network are updated less frequently (every few episodes) to avoid oscillating Q-values during training.\n",
    "- **Optimizer**: Adam optimizer is used to minimize the loss by adjusting the network's weights.\n",
    "- **Loss Function**: Mean Squared Error is used to minimize the difference between the predicted Q-values and the target Q-values.\n",
    "- **Replay Buffer**: A buffer with a size of 100,000 to store experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            state_input = np.expand_dims(state, axis=0).astype(np.float32)\n",
    "            q_values = q_network(state_input)\n",
    "            action = epsilon_greedy_policy(q_values, epsilon)\n",
    "            \n",
    "            # Take the chosen action in the environment\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            replay_buffer.add((state, action, reward, next_state, done))\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- **Episode Loop**: The outer loop runs for the specified number of episodes. Each episode is a complete run of the environment (from start to terminal state).\n",
    "- `env.reset()`: Resets the environment to the initial state.\n",
    "- **Action Selection**: The agent selects an action using the epsilon-greedy policy based on the Q-values predicted by the Q-network.\n",
    "- **Environment Interaction**: `env.step(action)` executes the selected action and returns the next state, reward, and whether the episode is done.\n",
    "- **Experience Storage**: The `(state, action, reward, next_state, done)` tuple is stored in the replay buffer for future training.\n",
    "- **Total Reward**: Tracks the cumulative reward obtained by the agent in the episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "            if len(replay_buffer.buffer) > batch_size:\n",
    "                # Sample a batch from the replay buffer\n",
    "                experiences = replay_buffer.sample(batch_size)\n",
    "                states, actions, rewards, next_states, dones = map(np.array, zip(*experiences))\n",
    "\n",
    "                # Predict Q-values for the next states using the target network\n",
    "                next_q_values = target_q_network(next_states)\n",
    "                max_next_q_values = np.max(next_q_values, axis=1)\n",
    "\n",
    "                # Bellman equation for the target Q-value\n",
    "                targets = rewards + gamma * max_next_q_values * (1 - dones)\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- **Experience Sampling**: A batch of experiences is sampled from the replay buffer once there are enough experiences to fill the batch.\n",
    "- **Target Calculation**: \n",
    "  - `next_q_values`: The Q-values for the next states are predicted by the target network.\n",
    "  - `max_next_q_values`: The highest Q-value for the next state is chosen (maximizing future reward).\n",
    "  - **Bellman Equation**: The target Q-value is calculated using the Bellman equation: `reward + (discount factor * max future reward)`. The factor `(1 - dones)` ensures that no future reward is added if the episode is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "                # Gradient descent to update the Q-network\n",
    "                with tf.GradientTape() as tape:\n",
    "                    q_values = q_network(states)\n",
    "                    action_masks = tf.one_hot(actions, num_actions)\n",
    "                    q_values_taken = tf.reduce_sum(q_values * action_masks, axis=1)\n",
    "                    loss = loss_fn(targets, q_values_taken)\n",
    "\n",
    "                grads = tape.gradient(loss, q_network.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(grads, q_network.trainable_variables))\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- **Q-Value Prediction**: The Q-network predicts the Q-values for the batch of states.\n",
    "- **Action Mask**: A one-hot mask is created for the actions taken in those states to extract the Q-values corresponding to the actions the agent chose.\n",
    "- **Loss Calculation**: The difference between the predicted Q-values and the target Q-values is calculated using Mean Squared Error.\n",
    "- **Gradient Descent**: The gradients of the loss are computed with respect to the network's weights, and the optimizer applies these gradients to update the Q-network's parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "            if done:\n",
    "                print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")\n",
    "                break\n",
    "\n",
    "        # Update epsilon for the next episode\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "        # Periodically update the target network\n",
    "        if episode % 10 == 0:\n",
    "            target_q_network.set_weights(q_network.get_weights())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Build a Real world project to understand the concept of Deep Reinfocement Learning better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The goal of the Cart-Pole problem is to develop an agent that learns to balance the pole by applying the correct forces to the cart. \n",
    "\n",
    "- This requires the agent to continuously adjust its actions based on the pole's current angle and velocity, learning to maintain balance over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset in the CartPole environment is generated through agent-environment interactions. Each instance of the dataset represents the current state of the system, the action taken by the agent, and the resulting next state and reward. The dataset is used to train a reinforcement learning agent, such as a Deep Q-Network (DQN), to learn optimal policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features (State Space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state space in the CartPole environment is a continuous space described by four key features:\n",
    "\n",
    "1. **Cart Position**:\n",
    "   - **Type**: Continuous (Float)\n",
    "   - **Range**: [-2.4, 2.4]\n",
    "   - Describes the position of the cart along the horizontal axis of the track.\n",
    "\n",
    "2. **Cart Velocity**:\n",
    "   - **Type**: Continuous (Float)\n",
    "   - **Range**: [-Inf, Inf]\n",
    "   - Represents the speed and direction of the cart's movement.\n",
    "\n",
    "3. **Pole Angle**:\n",
    "   - **Type**: Continuous (Float)\n",
    "   - **Range**: [-41.8°, 41.8°] (measured in radians)\n",
    "   - Describes the angle of the pole relative to the vertical axis.\n",
    "\n",
    "4. **Pole Angular Velocity**:\n",
    "   - **Type**: Continuous (Float)\n",
    "   - **Range**: [-Inf, Inf]\n",
    "   - Represents the rate of change of the pole's angle, indicating how quickly it is falling or rising."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action space is discrete, consisting of two possible actions:\n",
    "\n",
    "1. **Move Cart Left (Action = 0)**:\n",
    "   - The agent applies a force to move the cart to the left.\n",
    "\n",
    "2. **Move Cart Right (Action = 1)**:\n",
    "   - The agent applies a force to move the cart to the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Reward**: A constant reward of **+1** is given for each time step where the pole remains upright (i.e., the agent successfully balances the pole).\n",
    "- The goal of the agent is to maximize cumulative rewards over an episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episode Termination Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment episode ends when one of the following conditions is met:\n",
    "1. **Pole Angle exceeds ±12°**: The pole falls beyond a certain angle threshold.\n",
    "2. **Cart Position exceeds ±2.4 units**: The cart moves off the edge of the track.\n",
    "3. **Max Time Steps (500)**: The agent successfully balances the pole for 500 time steps (win condition)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical dataset instance consists of the following elements:\n",
    "```plaintext\n",
    "[State, Action, Reward, Next State, Done]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt  # Importing Matplotlib for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`state_size`**: This variable will hold the size of the state space, which represents the number of features used to describe the environment. For the CartPole problem, this typically includes variables such as the cart position, cart velocity, pole angle, and pole velocity at the tip.\n",
    "\n",
    "- **`action_size`**: This variable represents the number of possible actions the agent can take. In the CartPole environment, there are generally two actions: moving the cart left or moving it right.\n",
    "\n",
    "- **`memory`**: This is a deque (double-ended queue) that will store experiences of the agent during training. The experiences are used for training the model and are limited to the most recent 2000 entries. This helps in efficient memory management by discarding older experiences that may not be relevant.\n",
    "\n",
    "- **`gamma`**: This is the discount factor used in reinforcement learning. It determines the importance of future rewards. A value closer to 1 means that future rewards are considered almost equally to immediate rewards, while a value closer to 0 makes the agent focus on immediate rewards. In this case, `gamma` is set to 0.95.\n",
    "\n",
    "- **`epsilon`**: This is the exploration rate, which controls how often the agent explores new actions versus exploiting known actions. Initially set to 1.0, it encourages the agent to explore the action space. Over time, `epsilon` will decay to promote more exploitation of learned behaviors.\n",
    "\n",
    "- **`epsilon_min`**: This variable defines the minimum value that `epsilon` can decay to. This ensures that the agent maintains some level of exploration even after extensive training.\n",
    "\n",
    "- **`epsilon_decay`**: This factor determines how quickly the exploration rate decays. In this case, it is set to 0.995, meaning the agent will gradually reduce its exploration rate after each episode, allowing it to exploit its learned knowledge more over time.\n",
    "\n",
    "- **`learning_rate`**: This variable sets the learning rate for the Q-learning algorithm, which controls how much the model's weights are updated during training. A smaller learning rate leads to smaller updates and slower convergence.\n",
    "\n",
    "- **`model`**: This variable will eventually hold the neural network model that approximates the Q-values for each action given a state. It will be defined and trained later in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following global variables are defined to set up the environment and the parameters for the Deep Q-Learning model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "state_size = 0\n",
    "action_size = 0\n",
    "memory = deque(maxlen=2000)\n",
    "gamma = 0.95    # discount rate\n",
    "epsilon = 1.0   # exploration rate\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "learning_rate = 0.001\n",
    "model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`patience`**: This parameter sets the maximum number of consecutive episodes that the training process will wait for an improvement in performance (reward). In this case, it is set to 20 episodes. If the agent does not achieve a better reward within these episodes, training will be halted.\n",
    "\n",
    "- **`best_reward`**: This variable is initialized to negative infinity (`-float('inf')`). It will hold the highest reward received by the agent during training. Each time the agent achieves a reward greater than `best_reward`, this variable will be updated. This allows us to track the best performance of the agent.\n",
    "\n",
    "- **`wait`**: This counter keeps track of the number of consecutive episodes since the last improvement in reward. It starts at 0 and increments each time the agent fails to achieve a reward higher than `best_reward`. If `wait` exceeds `patience`, training will stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping parameters\n",
    "patience = 20  # How many episodes to wait for improvement\n",
    "best_reward = -float('inf')  # Initialize best_reward\n",
    "wait = 0  # Counter for episodes without improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`rewards_per_episode`**: This list is used to store the total reward received by the agent in each episode. By recording the total rewards, we can visualize the agent's performance over time and observe trends in learning. A rising trend in this list indicates that the agent is successfully learning to balance the pole.\n",
    "\n",
    "- **`episode_lengths`**: This list keeps track of the number of time steps (or actions taken) in each episode before the episode ends (either by success or failure). Monitoring episode lengths helps us understand how effectively the agent is maintaining balance. Longer episodes suggest better performance.\n",
    "\n",
    "- **`epsilon_values`**: This list stores the values of `epsilon` (the exploration rate) at each episode. Tracking `epsilon` allows us to visualize how the exploration strategy changes over time. As training progresses and `epsilon` decays, we can expect to see a shift from exploration towards exploitation of learned actions.\n",
    "\n",
    "- **`average_rewards`**: This list is used to store the rolling average of rewards over a specified window of episodes. The rolling average smooths out the reward signal, making it easier to visualize trends in performance without being affected by the noise of individual episodes. This metric is particularly useful for observing long-term improvements in the agent's ability to balance the pole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward tracking for visualization\n",
    "rewards_per_episode = []  # To store total rewards per episode\n",
    "episode_lengths = []      # To store length of each episode\n",
    "epsilon_values = []       # To store epsilon decay values\n",
    "average_rewards = []      # To store the rolling average of rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`loss_values`**: This list is used to store the loss computed at each training step. The loss is typically calculated using a loss function, such as Mean Squared Error (MSE), which measures the difference between the predicted Q-values (outputs of the neural network) and the target Q-values (calculated using the Bellman equation). \n",
    "\n",
    "By recording the loss values, we can visualize how the training process evolves over time. A decreasing trend in the loss indicates that the agent's model is improving its ability to predict Q-values accurately, which is essential for effective decision-making in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss tracking for visualization\n",
    "loss_values = []  # Store loss per training step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Deep Q-Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`global model, state_size, action_size`**: This line declares that the variables `model`, `state_size`, and `action_size` are global, allowing the function to access and modify these variables defined outside the function.\n",
    "\n",
    "- **`model = Sequential()`**: This creates an instance of a sequential model, which is a linear stack of layers in Keras. The Sequential API is easy to use for building simple models.\n",
    "\n",
    "- **`model.add(Dense(24, input_dim=state_size, activation='relu'))`**: This adds the first hidden layer to the model. \n",
    "  - `Dense(24)` specifies a fully connected layer with 24 neurons.\n",
    "  - `input_dim=state_size` defines the input dimension, which corresponds to the size of the state space (i.e., the number of features describing the environment).\n",
    "  - `activation='relu'` uses the Rectified Linear Unit (ReLU) activation function, which helps introduce non-linearity to the model, enabling it to learn complex patterns.\n",
    "\n",
    "- **`model.add(Dense(24, activation='relu'))`**: This adds a second hidden layer to the model, also with 24 neurons and ReLU activation. Stacking multiple layers allows the network to learn more abstract representations of the input.\n",
    "\n",
    "- **`model.add(Dense(action_size, activation='linear'))`**: This adds the output layer, which corresponds to the Q-values for each possible action the agent can take.\n",
    "  - `action_size` specifies the number of output neurons, which should match the number of actions available in the environment.\n",
    "  - `activation='linear'` is used here because we want to output raw Q-values without any activation function limiting the range.\n",
    "\n",
    "- **`model.compile(loss='mse', optimizer=Adam(learning_rate=learning_rate))`**: This compiles the model with the following specifications:\n",
    "  - `loss='mse'` specifies the Mean Squared Error (MSE) as the loss function. MSE measures the average squared difference between predicted Q-values and target Q-values, making it suitable for regression tasks.\n",
    "  - `optimizer=Adam(learning_rate=learning_rate)` uses the Adam optimizer, which is popular for training neural networks due to its adaptive learning rate capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    global model, state_size, action_size\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, input_dim=state_size, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Management in Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reinforcement learning, especially with Deep Q-Learning, it is essential to store experiences from the agent's interactions with the environment. This storage allows the model to learn from past experiences, which is critical for improving its performance over time. The `remember` function is designed to facilitate this experience replay mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`state`**: This parameter represents the current state of the environment at the time of taking an action. It contains information about the CartPole system, such as the position and velocity of the cart and the angle and angular velocity of the pole.\n",
    "\n",
    "- **`action`**: This parameter denotes the action taken by the agent in the given state. In the CartPole problem, this could be moving the cart left or right.\n",
    "\n",
    "- **`reward`**: This parameter indicates the reward received after taking the action. In the context of the CartPole problem, the agent typically receives a positive reward for each time step the pole remains upright.\n",
    "\n",
    "- **`next_state`**: This parameter represents the state of the environment after the action has been taken. It contains updated information about the system after the agent's decision.\n",
    "\n",
    "- **`done`**: This boolean parameter indicates whether the episode has ended (i.e., whether the pole has fallen). It helps in understanding if the current experience was the last step in the episode.\n",
    "\n",
    "- **`memory.append(...)`**: This line appends the tuple `(state, action, reward, next_state, done)` to the `memory` deque (double-ended queue). The use of a deque allows for efficient appending and removing of old experiences. The maximum length of `memory` is predefined (as seen in the global variable section), ensuring that only the most recent experiences are stored, which is crucial for managing memory effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remember(state, action, reward, next_state, done):\n",
    "    memory.append((state, action, reward, next_state, done))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Selection in Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent’s ability to select the best action in any given state is crucial to solving the CartPole problem. The `act` function is responsible for deciding whether the agent should explore the environment by taking random actions or exploit its current knowledge by selecting the best-known action. This balance between exploration and exploitation is governed by an important parameter known as `epsilon` in the **epsilon-greedy policy**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`global epsilon`**: The function uses the global `epsilon` variable, which represents the exploration rate. This value determines the likelihood of the agent choosing a random action over exploiting the learned Q-values.\n",
    "\n",
    "- **Exploration Phase (`if np.random.rand() <= epsilon`)**: \n",
    "    - `np.random.rand()` generates a random float between 0 and 1.\n",
    "    - If this random value is less than or equal to `epsilon`, the agent explores by taking a random action.\n",
    "    - `random.randrange(action_size)` returns a random action from the set of possible actions (in CartPole, this is typically 0 or 1, corresponding to moving the cart left or right). This random action encourages exploration, allowing the agent to discover potentially better strategies.\n",
    "\n",
    "- **Exploitation Phase (`act_values = model.predict(state, verbose=0)`)**:\n",
    "    - If the random value is greater than `epsilon`, the agent uses its trained neural network to predict the Q-values for the current state. \n",
    "    - `model.predict(state, verbose=0)` runs the input `state` through the model to obtain predicted Q-values for all possible actions. These Q-values represent the agent's current estimate of the total expected reward for each action.\n",
    "    - `return np.argmax(act_values[0])`: The agent selects the action with the highest predicted Q-value by taking the `argmax` (index of the maximum value) of the output array. This is the exploitation part, where the agent chooses the action it believes will yield the highest reward based on its learning so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon-Greedy Policy\n",
    "\n",
    "The agent follows an **epsilon-greedy policy**, where:\n",
    "- With probability `epsilon`, the agent explores the environment by taking a random action.\n",
    "- With probability `1 - epsilon`, the agent exploits the learned Q-values by choosing the action with the highest predicted value.\n",
    "\n",
    "The exploration rate (`epsilon`) starts high (to encourage exploration) and decays over time as the agent becomes more confident in its learned strategy. The gradual reduction in exploration ensures that the agent focuses on exploiting its knowledge as training progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(state):\n",
    "    global epsilon\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return random.randrange(action_size)\n",
    "    act_values = model.predict(state, verbose=0)\n",
    "    return np.argmax(act_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay in Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `replay` function implements **experience replay**, a critical technique in Deep Q-Learning that allows the agent to learn from past experiences by sampling a random minibatch of stored experiences. This helps break the correlation between consecutive experiences and improves the stability and efficiency of the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **`minibatch = random.sample(memory, batch_size)`**:\n",
    "   - This line samples a random minibatch of experiences (of size `batch_size`) from the `memory` deque. Each experience contains the tuple `(state, action, reward, next_state, done)` that represents a single step in the environment.\n",
    "\n",
    "2. **`states = np.array([sample[0] for sample in minibatch])`**:\n",
    "   - This extracts the `state` part of each experience in the minibatch and converts it into a NumPy array for batch processing by the neural network.\n",
    "\n",
    "3. **Reshape States**:\n",
    "   - `states = states.reshape(batch_size, state_size)` ensures that the states have the correct shape for input into the model, with dimensions corresponding to the batch size and the size of the state space.\n",
    "\n",
    "4. **`targets = model.predict(states, verbose=0)`**:\n",
    "   - The model predicts the Q-values for all possible actions for each state in the minibatch. This generates a batch of Q-value predictions (`targets`), which will be used to compute the updated Q-values for training.\n",
    "\n",
    "5. **Tracking Loss**:\n",
    "   - `total_loss = 0` initializes a variable to keep track of the total loss for the current minibatch. Loss values are important for monitoring the model’s training progress.\n",
    "\n",
    "6. **Updating Q-Values in the Minibatch**:\n",
    "   - The loop `for i, (state, action, reward, next_state, done) in enumerate(minibatch)` iterates through each experience in the minibatch. For each experience:\n",
    "     - **Terminal States**: If `done` is `True`, the agent receives the immediate reward, as there are no future rewards.\n",
    "     - **Non-terminal States**: If `done` is `False`, the Q-value is updated as `reward + gamma * max(Q(next_state))`. The agent predicts the Q-values for the next state and selects the maximum future reward, discounted by `gamma`.\n",
    "     - **`targets[i][action] = target`**: The Q-value corresponding to the action taken is updated with the computed target value.\n",
    "\n",
    "7. **Training the Model**:\n",
    "   - `history = model.fit(states, targets, epochs=1, verbose=0)` trains the model on the batch of updated Q-values (`targets`). The `fit` function performs one epoch of training on the states and their corresponding target Q-values.\n",
    "   - The loss for this minibatch is extracted from `history.history['loss'][0]` and added to `total_loss`. This helps in monitoring how much the model is improving over time.\n",
    "\n",
    "8. **Track Loss**:\n",
    "   - `loss_values.append(total_loss)` appends the loss from the current minibatch to the `loss_values` list for visualization and analysis.\n",
    "\n",
    "9. **Epsilon Decay**:\n",
    "   - After each training step, the exploration rate (`epsilon`) decays, ensuring that the agent gradually shifts from exploration to exploitation as training progresses. This decay is defined as `epsilon *= epsilon_decay`, ensuring that `epsilon` never falls below `epsilon_min`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Concepts\n",
    "\n",
    "- **Experience Replay**: By randomly sampling experiences from memory, the agent learns from a diverse set of past experiences, which helps break the correlation between consecutive samples and improves learning efficiency.\n",
    "  \n",
    "- **Target Q-Values**: The target Q-values for each action are computed based on the Bellman equation, which updates the Q-value based on the reward and the discounted maximum future reward.\n",
    "\n",
    "- **Epsilon-Greedy Decay**: As training progresses, the exploration rate `epsilon` is reduced, allowing the agent to focus more on exploitation (choosing actions based on learned Q-values) as it becomes more confident in its knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay(batch_size):\n",
    "    global epsilon\n",
    "    minibatch = random.sample(memory, batch_size)\n",
    "    states = np.array([sample[0] for sample in minibatch])\n",
    "\n",
    "    # Ensure states are shaped correctly\n",
    "    states = states.reshape(batch_size, state_size)\n",
    "\n",
    "    targets = model.predict(states, verbose=0)\n",
    "    \n",
    "    # Tracking loss\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, (state, action, reward, next_state, done) in enumerate(minibatch):\n",
    "        target = reward\n",
    "        if not done:\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            target = (reward + gamma * np.amax(model.predict(next_state, verbose=0)[0]))\n",
    "        targets[i][action] = target\n",
    "    \n",
    "    # Train the model and track the loss\n",
    "    history = model.fit(states, targets, epochs=1, verbose=0)\n",
    "    total_loss += history.history['loss'][0]  # Capture loss for this minibatch\n",
    "\n",
    "    loss_values.append(total_loss)  # Track loss per batch\n",
    "\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model in Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `save_model` function is responsible for saving the trained weights of the neural network model at any given point in time. This is essential for preserving the model’s learning progress and reusing the trained model in future sessions without needing to retrain from scratch.\n",
    "\n",
    "**`model.save_weights(name)`**:\n",
    "   - The `save_weights` method is part of the Keras API, and it saves only the **weights** (parameters) of the neural network model. These weights represent the knowledge that the model has acquired through training.\n",
    "   - The `name` parameter specifies the filename or path where the model's weights will be stored. This allows flexibility in naming the saved file based on, for example, the number of episodes completed or the agent's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Save the Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Checkpointing**: Saving the model’s weights at regular intervals (e.g., after certain episodes or when the performance improves) acts as a checkpoint. This ensures that if the training process is interrupted, you can resume training or use the model for testing without losing progress.\n",
    "\n",
    "2. **Reusability**: Once the model has been trained to a satisfactory level, you can save its weights and load them later for use in predictions or deployment. This avoids retraining the model from scratch, saving time and computational resources.\n",
    "\n",
    "3. **Model Evaluation**: By saving models at different stages of training, you can compare their performance and identify the point at which the model performs best, which can be useful in tasks like hyperparameter tuning or avoiding overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(name):\n",
    "    # Ensure the filename ends with .weights.h5\n",
    "    if not name.endswith('.weights.h5'):\n",
    "        name = name.replace('.h5', '.weights.h5')\n",
    "    model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model in Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `load_model` function is responsible for loading previously saved weights into the neural network model. This allows you to restore the model's learned parameters and continue training or testing without retraining the model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(name):\n",
    "    model.load_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visualizations():\n",
    "    # Plot rewards per episode\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(rewards_per_episode)\n",
    "    plt.title('Total Rewards per Episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.grid()\n",
    "\n",
    "    # Plot episode lengths\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(episode_lengths)\n",
    "    plt.title('Episode Length per Episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Episode Length (Timesteps)')\n",
    "    plt.grid()\n",
    "\n",
    "    # Plot epsilon decay\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(epsilon_values)\n",
    "    plt.title('Epsilon Decay Over Time')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Epsilon Value')\n",
    "    plt.grid()\n",
    "\n",
    "    # Plot loss over time\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(loss_values)\n",
    "    plt.title('Loss Per Training Step')\n",
    "    plt.xlabel('Training Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function: Putting It All Together in Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `main` function orchestrates the entire training process for the CartPole problem using Deep Q-Learning. It sets up the environment, initializes the agent, and executes the training loop. This function is where the reinforcement learning process unfolds over multiple episodes, allowing the agent to learn by interacting with the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "```python\n",
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "build_model()\n",
    "```\n",
    "- **`env = gym.make('CartPole-v1')`**: This line creates the CartPole environment using OpenAI Gym, which provides a simulated environment for training reinforcement learning agents.\n",
    "- **`state_size`**: The state size corresponds to the number of features in the observation space, i.e., the state that the agent perceives (position, velocity, etc.).\n",
    "- **`action_size`**: This represents the number of possible actions (in the CartPole problem, this is moving left or right).\n",
    "- **`build_model()`**: The model is built using the neural network architecture defined earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters\n",
    "\n",
    "```python\n",
    "batch_size = 32\n",
    "episodes = 1000\n",
    "rolling_avg_window = 10\n",
    "```\n",
    "- **`batch_size`**: The number of experiences sampled from memory for each replay to update the model.\n",
    "- **`episodes`**: Total number of episodes for training.\n",
    "- **`rolling_avg_window`**: Window size for calculating the rolling average of rewards to monitor performance trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "```python\n",
    "for e in range(episodes):\n",
    "```\n",
    "The training loop runs for the specified number of episodes. In each episode, the agent interacts with the environment, gathers experiences, and uses them to improve its policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resetting the Environment\n",
    "\n",
    "```python\n",
    "state, _ = env.reset()\n",
    "state = np.reshape(state, [1, state_size])\n",
    "total_reward = 0\n",
    "```\n",
    "- **`env.reset()`**: Resets the environment at the beginning of each episode, returning the initial state.\n",
    "- **`np.reshape(state, [1, state_size])`**: Reshapes the state to match the input shape required by the neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Episode Execution\n",
    "\n",
    "```python\n",
    "for time in range(500):\n",
    "    action = act(state)\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    reward = reward if not done else -10\n",
    "    next_state = np.reshape(next_state, [1, state_size])\n",
    "    remember(state, action, reward, next_state, done)\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "```\n",
    "- **`act(state)`**: The agent selects an action based on the current state using an epsilon-greedy strategy (explore vs exploit).\n",
    "- **`env.step(action)`**: Takes the action in the environment and returns the next state, reward, and whether the episode has ended (`done`).\n",
    "- **`reward = reward if not done else -10`**: Assigns a negative reward if the pole falls over (`done`).\n",
    "- **`remember(...)`**: The experience (state, action, reward, next state, done) is stored in memory for replay.\n",
    "- **`total_reward += reward`**: Tracks the cumulative reward for the episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experience Replay\n",
    "\n",
    "```python\n",
    "if len(memory) > batch_size:\n",
    "    replay(batch_size)\n",
    "```\n",
    "Once the memory buffer has enough samples, the agent starts learning from past experiences by performing **experience replay**, which involves randomly sampling from memory and updating the model based on those experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "\n",
    "```python\n",
    "if total_reward > best_reward:\n",
    "    best_reward = total_reward\n",
    "    wait = 0\n",
    "else:\n",
    "    wait += 1\n",
    "\n",
    "if wait >= patience:\n",
    "    print(f\"Early stopping on episode {e + 1}. Best reward was {best_reward}.\")\n",
    "    break\n",
    "```\n",
    "- **Early Stopping**: The training stops early if there’s no improvement in total rewards after a certain number of episodes (controlled by the `patience` parameter). This helps avoid unnecessary episodes when the model has converged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Model\n",
    "\n",
    "```python\n",
    "if e % 10 == 0:\n",
    "    save_model(\"cartpole-dqn.h5\")\n",
    "```\n",
    "The model is saved every 10 episodes using the `save_model` function to keep checkpoints of the model's progress.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking Progress and Performance\n",
    "\n",
    "Throughout the loop, the function tracks key metrics like total rewards, episode lengths, and epsilon values for later visualization:\n",
    "\n",
    "```python\n",
    "rewards_per_episode.append(total_reward)\n",
    "episode_lengths.append(time)\n",
    "epsilon_values.append(epsilon)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progress Bar and Visualizations\n",
    "\n",
    "```python\n",
    "progress_bar = tqdm(total=episodes, desc='Training Progress', position=0, leave=True)\n",
    "progress_bar.update(1)\n",
    "progress_bar.set_postfix({'Episode': e + 1})\n",
    "```\n",
    "A progress bar (using `tqdm`) shows real-time progress through the episodes. At the end of training, the function calls `plot_visualizations()` to generate graphs for rewards, episode lengths, epsilon decay, and loss tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global state_size, action_size, model, best_reward, wait  # Declare as global\n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    build_model()\n",
    "    batch_size = 32\n",
    "    episodes = 1000\n",
    "    rolling_avg_window = 10  # Rolling average window for rewards\n",
    "\n",
    "    # Set up the progress bar\n",
    "    progress_bar = tqdm(total=episodes, desc='Training Progress', position=0, leave=True)\n",
    "\n",
    "    for e in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        total_reward = 0\n",
    "\n",
    "        for time in range(500):\n",
    "            action = act(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            reward = reward if not done else -10\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            if len(memory) > batch_size:\n",
    "                replay(batch_size)\n",
    "\n",
    "        # Store total reward for this episode\n",
    "        rewards_per_episode.append(total_reward)\n",
    "\n",
    "        # Store episode length\n",
    "        episode_lengths.append(time)\n",
    "\n",
    "        # Store epsilon values\n",
    "        epsilon_values.append(epsilon)\n",
    "\n",
    "        # Update the rolling average reward\n",
    "        if len(rewards_per_episode) >= rolling_avg_window:\n",
    "            avg_reward = np.mean(rewards_per_episode[-rolling_avg_window:])\n",
    "            average_rewards.append(avg_reward)\n",
    "        \n",
    "        # Update the progress bar\n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_postfix({'Episode': e + 1})\n",
    "\n",
    "        # Print episode details\n",
    "        print(f\"Episode: {e + 1}/{episodes}, Score: {total_reward}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if total_reward > best_reward:\n",
    "            best_reward = total_reward\n",
    "            wait = 0  # Reset wait counter\n",
    "        else:\n",
    "            wait += 1  # Increment wait counter\n",
    "        \n",
    "        if wait >= patience:\n",
    "            print(f\"Early stopping on episode {e + 1}. Best reward was {best_reward}.\")\n",
    "            break\n",
    "        \n",
    "        if e % 10 == 0:\n",
    "            save_model(\"cartpole-dqn.h5\")\n",
    "    \n",
    "    # Close the progress bar\n",
    "    progress_bar.close()\n",
    "\n",
    "    # Generate the visualizations\n",
    "    plot_visualizations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 1/1000 [00:00<00:00, 1691.25it/s, Episode=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/1000, Score: 22.0, Epsilon: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 2/1000 [00:09<1:17:54,  4.68s/it, Episode=2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2/1000, Score: 0.0, Epsilon: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 3/1000 [00:25<2:38:08,  9.52s/it, Episode=3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 3/1000, Score: 8.0, Epsilon: 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 4/1000 [00:51<4:19:41, 15.64s/it, Episode=4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 4/1000, Score: 19.0, Epsilon: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 5/1000 [01:05<4:07:25, 14.92s/it, Episode=5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 5/1000, Score: 5.0, Epsilon: 0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 6/1000 [01:24<4:29:37, 16.27s/it, Episode=6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 6/1000, Score: 11.0, Epsilon: 0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 7/1000 [01:48<5:12:58, 18.91s/it, Episode=7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 7/1000, Score: 17.0, Epsilon: 0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 8/1000 [02:39<8:00:22, 29.05s/it, Episode=8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 8/1000, Score: 45.0, Epsilon: 0.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 9/1000 [03:21<9:07:03, 33.12s/it, Episode=9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 9/1000, Score: 35.0, Epsilon: 0.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 10/1000 [03:30<7:01:16, 25.53s/it, Episode=10]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10/1000, Score: -1.0, Epsilon: 0.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 11/1000 [03:39<5:36:09, 20.39s/it, Episode=11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 11/1000, Score: -1.0, Epsilon: 0.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 12/1000 [03:51<4:56:01, 17.98s/it, Episode=12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 12/1000, Score: 3.0, Epsilon: 0.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|▏         | 13/1000 [03:59<4:07:03, 15.02s/it, Episode=13]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 13/1000, Score: -1.0, Epsilon: 0.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|▏         | 14/1000 [04:07<3:29:35, 12.75s/it, Episode=14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 14/1000, Score: -2.0, Epsilon: 0.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 15/1000 [04:17<3:17:12, 12.01s/it, Episode=15]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 15/1000, Score: 1.0, Epsilon: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 16/1000 [04:25<2:55:46, 10.72s/it, Episode=16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 16/1000, Score: -2.0, Epsilon: 0.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 17/1000 [04:36<2:57:40, 10.84s/it, Episode=17]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 17/1000, Score: 2.0, Epsilon: 0.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 18/1000 [06:12<9:55:48, 36.40s/it, Episode=18]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 18/1000, Score: 92.0, Epsilon: 0.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 19/1000 [06:20<7:36:16, 27.91s/it, Episode=19]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 19/1000, Score: -1.0, Epsilon: 0.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 20/1000 [06:29<6:05:00, 22.35s/it, Episode=20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 20/1000, Score: 0.0, Epsilon: 0.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 21/1000 [06:44<5:28:21, 20.12s/it, Episode=21]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 21/1000, Score: 6.0, Epsilon: 0.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 22/1000 [08:02<10:11:26, 37.51s/it, Episode=22]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 22/1000, Score: 75.0, Epsilon: 0.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 23/1000 [09:45<15:31:27, 57.20s/it, Episode=23]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 23/1000, Score: 102.0, Epsilon: 0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 24/1000 [10:23<13:56:21, 51.42s/it, Episode=24]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 24/1000, Score: 31.0, Epsilon: 0.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▎         | 25/1000 [11:11<13:39:22, 50.42s/it, Episode=25]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 25/1000, Score: 42.0, Epsilon: 0.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   3%|▎         | 26/1000 [12:36<16:23:31, 60.59s/it, Episode=26]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 26/1000, Score: 81.0, Epsilon: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   3%|▎         | 27/1000 [13:15<14:38:00, 54.14s/it, Episode=27]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 27/1000, Score: 32.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   3%|▎         | 28/1000 [14:26<16:01:32, 59.35s/it, Episode=28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 28/1000, Score: 66.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   3%|▎         | 29/1000 [14:53<13:24:08, 49.69s/it, Episode=29]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 29/1000, Score: 18.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   3%|▎         | 30/1000 [18:26<26:33:38, 98.58s/it, Episode=30]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 30/1000, Score: 215.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   3%|▎         | 31/1000 [21:14<32:09:46, 119.49s/it, Episode=31]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 31/1000, Score: 165.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   3%|▎         | 32/1000 [23:26<33:07:03, 123.16s/it, Episode=32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 32/1000, Score: 127.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   3%|▎         | 33/1000 [25:27<32:53:27, 122.45s/it, Episode=33]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 33/1000, Score: 116.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   3%|▎         | 34/1000 [27:56<35:00:17, 130.45s/it, Episode=34]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 34/1000, Score: 147.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|▎         | 35/1000 [30:35<37:14:57, 138.96s/it, Episode=35]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 35/1000, Score: 154.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|▎         | 36/1000 [31:33<30:43:53, 114.77s/it, Episode=36]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 36/1000, Score: 50.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|▎         | 37/1000 [32:49<27:33:59, 103.05s/it, Episode=37]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 37/1000, Score: 68.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|▍         | 38/1000 [35:53<34:04:03, 127.49s/it, Episode=38]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 38/1000, Score: 178.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|▍         | 39/1000 [38:09<34:38:58, 129.80s/it, Episode=39]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 39/1000, Score: 128.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|▍         | 40/1000 [40:45<36:44:38, 137.79s/it, Episode=40]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 40/1000, Score: 149.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|▍         | 41/1000 [43:08<37:09:15, 139.47s/it, Episode=41]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 41/1000, Score: 140.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|▍         | 42/1000 [46:54<44:01:53, 165.46s/it, Episode=42]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 42/1000, Score: 225.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|▍         | 43/1000 [48:53<40:16:49, 151.52s/it, Episode=43]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 43/1000, Score: 111.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|▍         | 44/1000 [51:06<38:44:51, 145.91s/it, Episode=44]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 44/1000, Score: 125.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|▍         | 45/1000 [53:02<36:19:47, 136.95s/it, Episode=45]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 45/1000, Score: 108.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   5%|▍         | 46/1000 [56:04<39:51:23, 150.40s/it, Episode=46]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 46/1000, Score: 175.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   5%|▍         | 47/1000 [58:07<37:37:19, 142.12s/it, Episode=47]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 47/1000, Score: 115.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   5%|▍         | 48/1000 [59:59<35:12:54, 133.17s/it, Episode=48]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 48/1000, Score: 104.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   5%|▍         | 49/1000 [1:01:55<33:49:58, 128.07s/it, Episode=49]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 49/1000, Score: 111.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   5%|▌         | 50/1000 [1:04:04<33:51:16, 128.29s/it, Episode=50]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 50/1000, Score: 124.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   5%|▌         | 51/1000 [1:07:03<37:47:07, 143.34s/it, Episode=51]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 51/1000, Score: 176.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   5%|▌         | 52/1000 [1:08:11<31:49:14, 120.84s/it, Episode=52]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 52/1000, Score: 61.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   5%|▌         | 53/1000 [1:10:27<32:58:25, 125.35s/it, Episode=53]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 53/1000, Score: 128.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   5%|▌         | 54/1000 [1:12:50<34:20:25, 130.68s/it, Episode=54]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 54/1000, Score: 134.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   6%|▌         | 55/1000 [1:14:58<34:04:09, 129.79s/it, Episode=55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 55/1000, Score: 117.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   6%|▌         | 56/1000 [1:17:10<34:12:08, 130.43s/it, Episode=56]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 56/1000, Score: 119.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   6%|▌         | 57/1000 [1:19:28<34:49:06, 132.92s/it, Episode=57]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 57/1000, Score: 128.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   6%|▌         | 58/1000 [1:21:35<34:19:06, 131.15s/it, Episode=58]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 58/1000, Score: 120.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   6%|▌         | 59/1000 [1:23:39<33:41:16, 128.88s/it, Episode=59]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 59/1000, Score: 118.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   6%|▌         | 60/1000 [1:27:49<43:09:24, 165.28s/it, Episode=60]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 60/1000, Score: 248.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   6%|▌         | 61/1000 [1:30:27<42:31:33, 163.04s/it, Episode=61]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 61/1000, Score: 153.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   6%|▌         | 62/1000 [1:32:34<39:40:16, 152.26s/it, Episode=62]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 62/1000, Score: 119.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   6%|▋         | 63/1000 [1:35:27<41:12:24, 158.32s/it, Episode=63]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 63/1000, Score: 162.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   6%|▋         | 64/1000 [1:38:01<40:52:13, 157.19s/it, Episode=64]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 64/1000, Score: 145.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   6%|▋         | 65/1000 [1:40:01<37:53:27, 145.89s/it, Episode=65]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 65/1000, Score: 110.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   7%|▋         | 66/1000 [1:42:35<38:28:45, 148.31s/it, Episode=66]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 66/1000, Score: 143.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   7%|▋         | 67/1000 [1:44:49<37:22:31, 144.21s/it, Episode=67]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 67/1000, Score: 127.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   7%|▋         | 68/1000 [1:47:13<37:16:12, 143.96s/it, Episode=68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 68/1000, Score: 137.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   7%|▋         | 69/1000 [1:49:46<37:56:22, 146.70s/it, Episode=69]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 69/1000, Score: 144.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   7%|▋         | 70/1000 [1:52:09<37:38:34, 145.71s/it, Episode=70]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 70/1000, Score: 137.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   7%|▋         | 71/1000 [1:55:07<40:07:08, 155.47s/it, Episode=71]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 71/1000, Score: 173.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   7%|▋         | 72/1000 [1:56:48<35:51:17, 139.09s/it, Episode=72]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 72/1000, Score: 94.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   7%|▋         | 73/1000 [1:58:46<34:10:11, 132.70s/it, Episode=73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 73/1000, Score: 110.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   7%|▋         | 74/1000 [2:01:59<38:48:42, 150.89s/it, Episode=74]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 74/1000, Score: 184.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   8%|▊         | 75/1000 [2:04:43<39:45:21, 154.73s/it, Episode=75]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 75/1000, Score: 151.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   8%|▊         | 76/1000 [2:07:45<41:46:48, 162.78s/it, Episode=76]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 76/1000, Score: 170.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   8%|▊         | 77/1000 [2:10:59<44:09:46, 172.25s/it, Episode=77]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 77/1000, Score: 185.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   8%|▊         | 78/1000 [2:15:59<53:56:01, 210.59s/it, Episode=78]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 78/1000, Score: 291.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   8%|▊         | 79/1000 [2:18:19<48:26:25, 189.34s/it, Episode=79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 79/1000, Score: 131.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   8%|▊         | 80/1000 [2:21:38<49:07:25, 192.22s/it, Episode=80]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 80/1000, Score: 192.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   8%|▊         | 81/1000 [2:24:08<45:53:00, 179.74s/it, Episode=81]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 81/1000, Score: 139.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   8%|▊         | 82/1000 [2:26:42<43:52:11, 172.04s/it, Episode=82]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 82/1000, Score: 142.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   8%|▊         | 83/1000 [2:29:48<44:51:35, 176.11s/it, Episode=83]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 83/1000, Score: 173.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   8%|▊         | 84/1000 [2:31:50<40:42:00, 159.96s/it, Episode=84]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 84/1000, Score: 110.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   8%|▊         | 85/1000 [2:34:13<39:22:29, 154.92s/it, Episode=85]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 85/1000, Score: 131.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   9%|▊         | 86/1000 [2:36:40<38:43:59, 152.56s/it, Episode=86]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 86/1000, Score: 134.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   9%|▊         | 87/1000 [2:38:49<36:52:21, 145.39s/it, Episode=87]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 87/1000, Score: 117.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   9%|▉         | 88/1000 [2:41:07<36:14:15, 143.04s/it, Episode=88]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 88/1000, Score: 126.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   9%|▉         | 89/1000 [2:43:22<35:35:59, 140.68s/it, Episode=89]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 89/1000, Score: 122.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   9%|▉         | 90/1000 [2:46:01<36:58:14, 146.26s/it, Episode=90]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 90/1000, Score: 149.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   9%|▉         | 91/1000 [2:48:07<35:24:00, 140.20s/it, Episode=91]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 91/1000, Score: 118.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   9%|▉         | 92/1000 [2:50:14<34:19:06, 136.06s/it, Episode=92]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 92/1000, Score: 120.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   9%|▉         | 93/1000 [2:52:05<32:24:15, 128.62s/it, Episode=93]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 93/1000, Score: 102.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   9%|▉         | 94/1000 [2:53:47<30:20:39, 120.57s/it, Episode=94]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 94/1000, Score: 93.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  10%|▉         | 95/1000 [2:55:47<30:16:23, 120.42s/it, Episode=95]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 95/1000, Score: 111.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  10%|▉         | 96/1000 [2:57:37<29:29:33, 117.45s/it, Episode=96]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 96/1000, Score: 103.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  10%|▉         | 97/1000 [2:59:12<27:47:06, 110.77s/it, Episode=97]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 97/1000, Score: 87.0, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  10%|▉         | 98/1000 [3:01:28<27:50:22, 111.11s/it, Episode=98]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 98/1000, Score: 127.0, Epsilon: 0.01\n",
      "Early stopping on episode 98. Best reward was 291.0.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1008x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of Training Stats for CartPole DQN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above four graphs in the image represent key metrics tracked during the training process of a Deep Q-Network (DQN) on the CartPole problem. These plots help visualize how the model learns over time. Here's a breakdown of each graph:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. **Top Left: Total Rewards per Episode**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Description**: This graph shows the total rewards achieved in each episode.\n",
    "- **Insight**: The reward in the CartPole environment is based on how long the pole stays upright. Initially, the rewards fluctuate a lot, indicating the agent is still learning. As training progresses, there might be an upward trend, showing the agent is getting better at balancing the pole for longer durations. However, fluctuations are common due to exploration.\n",
    "- **Takeaway**: As the model improves, we expect to see a general increase in the rewards over episodes, even though there may still be variations due to the exploration-exploitation trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Top Right: Rolling Average of Rewards**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Description**: This plot  represents a **rolling average** of the total rewards over a fixed window of episodes (e.g., a 10-episode rolling average). Smoothing out the raw reward data provides a clearer view of the agent's learning progress over time.\n",
    "- **Insight**: Unlike the more volatile raw rewards graph, this smoothed version helps identify the overall trend in learning performance. If the agent is learning, you'll see an upward trend, meaning the agent is consistently performing better.\n",
    "- **Takeaway**: A steady increase in the rolling average rewards signals consistent improvement in the agent’s ability to balance the pole over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **Bottom Left: Epsilon Decay (Exploration Rate)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - **Description**: This graph represents the **epsilon decay** over episodes. Epsilon controls the exploration-exploitation trade-off: at the beginning of training, the agent explores more by choosing random actions (high epsilon), and as training progresses, epsilon decreases, meaning the agent exploits its learned policy more.\n",
    "   - **Insight**: The graph shows a steep decline, indicating that early in training, the agent is exploring more, but as epsilon decays, it starts exploiting the learned Q-values. Eventually, epsilon reaches a minimum value (usually 0.01), where the agent almost always chooses actions based on its learned policy.\n",
    "   - **Takeaway**: This decline in exploration is crucial for converging to a good policy. Initially, exploration allows the agent to learn a variety of state-action pairs, and later exploitation ensures it uses what it has learned to maximize rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. **Bottom Right: Loss per Batch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Description**: This graph tracks the **loss per training step** during experience replay. The loss measures how far the predicted Q-values are from the target Q-values (calculated using the Bellman equation).\n",
    "- **Insight**: Initially, there may be large fluctuations in loss values as the agent randomly explores and updates its Q-values. Over time, the loss generally decreases as the model stabilizes and predictions become more accurate. Spikes in loss can still occur, especially when the agent encounters novel states or transitions.\n",
    "- **Takeaway**: A decreasing trend in loss is a sign that the model is learning effectively and its Q-value predictions are becoming more accurate. Consistent spikes are normal but should smooth out as the model converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Training Process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Exploration (High Epsilon)**: At the start, the agent explores the environment extensively, leading to a wide range of rewards. This phase is necessary for the agent to learn about different state-action pairs.\n",
    "   \n",
    "2. **Exploitation (Low Epsilon)**: As epsilon decays, the agent relies more on the learned Q-values and starts exploiting its knowledge to accumulate more rewards consistently.\n",
    "\n",
    "3. **Reward Growth**: As training progresses, both the raw rewards and rolling average rewards increase, showing that the agent is improving its ability to balance the cart.\n",
    "\n",
    "4. **Loss Reduction**: The loss values decrease over time, indicating the agent’s Q-values are becoming more accurate, and the network is converging toward an optimal policy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

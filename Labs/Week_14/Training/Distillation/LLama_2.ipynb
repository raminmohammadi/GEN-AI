{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the pretrained teacher model and tokenizer\n",
    "teacher_model_name = \"meta-llama/Llama-2-13b-hf\"\n",
    "teacher_model = TFAutoModelForCausalLM.from_pretrained(teacher_model_name)\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)\n",
    "\n",
    "# Define input texts\n",
    "texts = [\n",
    "    \"The movie was fantastic!\",\n",
    "    \"The plot was dull and predictable.\",\n",
    "]\n",
    "\n",
    "# Tokenize inputs\n",
    "inputs = teacher_tokenizer(texts, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "\n",
    "# Generate soft labels (logits) from the teacher model\n",
    "teacher_logits = teacher_model(inputs.input_ids).logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Load the smaller student model and tokenizer\n",
    "student_model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "student_model = TFAutoModelForCausalLM.from_pretrained(student_model_name)\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(student_model_name)\n",
    "\n",
    "# Ensure teacher and student tokenizers are consistent\n",
    "assert teacher_tokenizer.vocab == student_tokenizer.vocab, \"Mismatch in vocabularies!\"\n",
    "\n",
    "# Define KL Divergence loss function\n",
    "def kl_divergence_loss(teacher_logits, student_logits):\n",
    "    teacher_probs = tf.nn.softmax(teacher_logits, axis=-1)\n",
    "    student_probs = tf.nn.softmax(student_logits, axis=-1)\n",
    "    return tf.reduce_mean(\n",
    "        tf.reduce_sum(teacher_probs * tf.math.log(teacher_probs / student_probs), axis=-1)\n",
    "    )\n",
    "\n",
    "# Prepare optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "# Distillation training loop\n",
    "batch_size = 2  # Use small batches for demonstration\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    total_loss = 0\n",
    "\n",
    "    # Process inputs in batches\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "\n",
    "        # Tokenize for both teacher and student\n",
    "        student_inputs = student_tokenizer(batch_texts, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "        teacher_inputs = teacher_tokenizer(batch_texts, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "\n",
    "        # Forward pass\n",
    "        with tf.GradientTape() as tape:\n",
    "            student_logits = student_model(student_inputs.input_ids).logits\n",
    "            teacher_logits_batch = teacher_model(teacher_inputs.input_ids).logits\n",
    "\n",
    "            # Compute KL divergence loss\n",
    "            loss = kl_divergence_loss(teacher_logits_batch, student_logits)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        gradients = tape.gradient(loss, student_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, student_model.trainable_variables))\n",
    "\n",
    "        total_loss += loss.numpy()\n",
    "\n",
    "    print(f\"Loss: {total_loss / (len(texts) // batch_size):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def combined_loss(teacher_logits, student_logits, true_labels, alpha=0.5):\n",
    "    # Distillation loss\n",
    "    distill_loss = kl_divergence_loss(teacher_logits, student_logits)\n",
    "    # Supervised loss\n",
    "    supervised_loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        true_labels, student_logits, from_logits=True\n",
    "    )\n",
    "    return alpha * distill_loss + (1 - alpha) * tf.reduce_mean(supervised_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

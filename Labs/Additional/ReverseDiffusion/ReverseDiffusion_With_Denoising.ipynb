{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUgRVUfXQKl4"
   },
   "source": [
    "<h1>\n",
    "Reverse Diffusion With Denoising\n",
    "</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDUpAOnYQQSo"
   },
   "source": [
    "# Brief Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxHI6AdUQRdh"
   },
   "source": [
    "We'll be presenting a comprehensive guide to implementing reverse diffusion denoising using TensorFlow. We present a minimal yet complete implementation of a Denoising Diffusion Probabilistic Model (DDPM) trained on the CIFAR10 dataset, with extensibility for other image domains. The implementation emphasizes mathematical foundations, practical considerations, and modern architectural patterns while maintaining compatibility with standard deep learning workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWQGPeAmYLMq"
   },
   "source": [
    "# Architecture\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/raminmohammadi/GEN-AI/7642375d6798d75d28100008e3c74b5a96d79d21/Labs/Diffusion%20Models/assets/architecture.png' width=450>\n",
    "\n",
    "This diagram illustrates the architecture of a diffusion model, highlighting both the forward and reverse diffusion processes.\n",
    "\n",
    "1. **Input Data**: This is the original data that you want to model or generate, such as an image.\n",
    "\n",
    "2. **Noise Schedule**: A predefined schedule that determines how much noise is added at each step in the forward diffusion process. This schedule controls the transformation from the original data to noise.\n",
    "\n",
    "3. **Forward Diffusion Process**: In this phase, noise is progressively added to the input data according to the noise schedule, transforming it into noisy data. This simulates the degradation of data.\n",
    "\n",
    "4. **Noisy Data**: The result of the forward diffusion process, where the original data has been converted into a noisy version.\n",
    "\n",
    "5. **Neural Network**: A model trained to predict and reverse the noise added in the forward process. It learns to generate the original data from the noisy data.\n",
    "\n",
    "6. **Reverse Diffusion Process**: Guided by the neural network and using the noisy data as a starting point, this process iteratively removes noise, working backwards to recreate a denoised version of the original data.\n",
    "\n",
    "7. **Denoised Output**: The final output generated by the reverse diffusion process, ideally resembling the initial input data.\n",
    "\n",
    "Overall, this architecture shows how diffusion models use a combination of noise addition and removal to generate new data that mimics the original input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkFEkfkHaQtS"
   },
   "source": [
    "## Forward Diffusion Process\n",
    "\n",
    "<img src='https://cdn-ilclanb.nitrocdn.com/IekjQeaQhaYynZsBcscOhxvktwdZlYmf/assets/images/source/rev-b2d8ac0/learnopencv.com/wp-content/uploads/2023/02/denoising-diffusion-probabilistic-models_forward_process_changing_distribution.png' width=500>\n",
    "\n",
    "\n",
    "> ***It is easy to destroy but hard to create - Pearl S. Buck***\n",
    "\n",
    "1. In the ‚ÄúForward Diffusion‚Äù process, we slowly and iteratively add noise to (corrupt) the images in our training set such that they ‚Äúmove out or move away‚Äù from their existing subspace.\n",
    "2. What we are doing here is converting the unknown and complex distribution that our training set belongs to into one that is easy for us to sample a (data) point from and understand.\n",
    "3. At the end of the forward process, the images become entirely unrecognizable. The complex data distribution is wholly transformed into a (chosen) simple distribution. Each image gets mapped to a space outside the data subspace.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse Diffusion Process\n",
    "\n",
    "<img src='https://cdn-ilclanb.nitrocdn.com/IekjQeaQhaYynZsBcscOhxvktwdZlYmf/assets/images/source/rev-b2d8ac0/learnopencv.com/wp-content/uploads/2023/02/denoising-diffusion-probabilistic-models_moving_from_simple_to_data_space-1.png' width=400>\n",
    "\n",
    "> ***By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. - Stable Diffusion, 2022***\n",
    "\n",
    "\n",
    "1. In the ‚ÄúReverse Diffusion process,‚Äù the idea is to reverse the forward diffusion process.\n",
    "2. We slowly and iteratively try to reverse the corruption performed on images in the forward process.\n",
    "3. The reverse process starts where the forward process ends.\n",
    "4. The benefit of starting from a simple space is that we know how to get/sample a point from this simple distribution (think of it as any point outside the data subspace). \n",
    "5. And our goal here is to figure out how to return to the data subspace.\n",
    "6. However, the problem is that we can take infinite paths starting from a point in this ‚Äúsimple‚Äù space, but only a fraction of them will take us to the ‚Äúdata‚Äù subspace. \n",
    "7. In diffusion probabilistic models, this is done by referring to the small iterative steps taken during the forward diffusion process. \n",
    "8. The PDF that satisfies the corrupted images in the forward process differs slightly at each step.\n",
    "9. Hence, in the reverse process, we use a deep-learning model at each step to predict the PDF parameters of the forward process. \n",
    "10. And once we train the model, we can start from any point in the simple space and use the model to iteratively take steps to lead us back to the data subspace. \n",
    "11. In reverse diffusion, we iteratively perform the **‚Äúdenoising‚Äù** in small steps, starting from a noisy image.\n",
    "12. This approach for training and generating new samples is much more stable than GANs and better than previous approaches like variational autoencoders (VAE) and normalizing flows. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Foundations of Reverse Diffusion\n",
    "\n",
    "\n",
    "<img src='https://cdn-ilclanb.nitrocdn.com/IekjQeaQhaYynZsBcscOhxvktwdZlYmf/assets/images/source/rev-b2d8ac0/learnopencv.com/wp-content/uploads/2023/01/diffusion-models-forwardbackward_process_ddpm.png' width=600>\n",
    "\n",
    "\n",
    "* **Forward Process:**\n",
    "\n",
    "  Gradually adds Gaussian noise to data samples over T steps:\n",
    "\n",
    "  $$\n",
    "  q(x_t|x_{(t-1)})=ùí©(x_t;\\sqrt{1-\\beta_t}x_{t-1}, \\beta I)\n",
    "  $$\n",
    "\n",
    "  where $\\beta_t$ defines the noise schedule.\n",
    "\n",
    "* **Reverse Process:** \n",
    "\n",
    "  Learned transition that iteratively denoises samples:\n",
    "  $$\n",
    "  p_Œ∏(x_{t‚àí1}|x_t)=ùí©(x_{t‚àí1};Œº_Œ∏(x_t,t),Œ£_Œ∏(x_t,t))\n",
    "  $$\n",
    "\n",
    "  The neural network $œµ_Œ∏$ predicts the noise component for denoising.\n",
    "\n",
    "\n",
    "* **Training Objective** \n",
    "\n",
    "  The simplified objective minimizes the MSE between predicted and actual noise:\n",
    "  $$\n",
    "  {L} = \\mathbb{E}_{t, x_0, \\epsilon} \\left[ \\left\\| \\epsilon - \\epsilon_{\\theta} \\left( \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, t \\right) \\right\\|^2 \\right]\n",
    "  $$\n",
    "\n",
    "  where ,\n",
    "  $$\n",
    "  \\bar{\\alpha}_t = \\prod_{s=1}^{t} (1 - \\beta_s).\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfyOEN7AbTa9"
   },
   "source": [
    "# **Implementation**\n",
    "\n",
    "### **Key Components**\n",
    "* **Data Preprocessing:** Prepare the dataset, typically normalizing images to ensure they fit within the model's expected range.\n",
    "\n",
    "* **Diffusion Process:**\n",
    "\n",
    "  * **Forward Diffusion:** Incrementally adds noise to the data.\n",
    "  * **Reverse Diffusion:** The model learns to remove this noise to reconstruct the data.\n",
    "* **Noise Schedule:** Determines how noise is added at each step of the forward process.\n",
    "\n",
    "* **Neural Network Architecture:** Usually a U-Net, which is effective for capturing spatial information and reconstructing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lbrPUXtVtXlv"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example U-Net like architecture\n",
    "def get_unet_model(input_shape):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Encoding path\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "\n",
    "    # Bottom\n",
    "    x = layers.Conv2D(256, 3, activation='relu', padding='same')(x)\n",
    "\n",
    "    # Decoding path\n",
    "    x = layers.UpSampling2D(2)(x)\n",
    "    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.UpSampling2D(2)(x)\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "\n",
    "    outputs = layers.Conv2D(3, 1, activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1Cf-iyMyt3S1"
   },
   "source": [
    "### **Explanation**\n",
    "\n",
    "**U-Net Architecture:**\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/raminmohammadi/GEN-AI/7642375d6798d75d28100008e3c74b5a96d79d21/Labs/Diffusion%20Models/assets/unet.jpg' width=500>\n",
    "\n",
    "Utilizes encoding and decoding paths with skip connections to capture and reconstruct image features effectively.\n",
    "  * **Training Process:** The model is optimized using a mean squared error loss, attempting to learn the denoising process.\n",
    "  * **Epochs:** Set to a low number for quick demonstration; more epochs would be needed for real data.\n",
    "\n",
    "**Use Cases**\n",
    "\n",
    "This basic setup can be extended for advanced applications like super-resolution, style transfer, or generative tasks by adjusting the architecture and training protocols. For real implementations, consider using additional noise schedules and more complex architectures tailored to specific tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cjamLERTtYg4"
   },
   "outputs": [],
   "source": [
    "# Instantiate and compile the model\n",
    "image_size = (128, 128, 3)\n",
    "model = get_unet_model(input_shape=image_size)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Example dummy data for training\n",
    "import numpy as np\n",
    "dummy_images = np.random.rand(100, 128, 128, 3)\n",
    "\n",
    "# Train the model\n",
    "model.fit(dummy_images, dummy_images, epochs=5, batch_size=10)\n",
    "\n",
    "# Display some outputs\n",
    "sample_output = model.predict(dummy_images[:5])\n",
    "for img in sample_output:\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "JNNcguv4uW6F"
   },
   "source": [
    "# **Denoising Images using Reverse Diffusion**\n",
    "\n",
    "\n",
    "### **Idea**\n",
    "<img src='https://raw.githubusercontent.com/raminmohammadi/GEN-AI/7642375d6798d75d28100008e3c74b5a96d79d21/Labs/Diffusion%20Models/assets/denoise.jpg' width=500>\n",
    "\n",
    "The idea is to see how a diffusion model deconstructs an image into noise and reversely constructs it back to a recognizable form, displaying the effectiveness of learned noise removal in generating high-quality images.\n",
    "\n",
    "* **Forward Diffusion (Top Row):**\n",
    "\n",
    "  Starts with a clear image and progressively adds noise over several steps, making the image noisier.\n",
    "\n",
    "* **Reverse Diffusion (Bottom Row):**\n",
    "\n",
    "  The model learns to gradually remove noise, starting from a noisy image and reconstructing it back into a clear image.\n",
    "  Arrows indicate how the model refines the image at each step, effectively demonstrating its capability to denoise and generate the original content.\n",
    "\n",
    "\n",
    "\n",
    "### **Reverse process**\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/raminmohammadi/GEN-AI/7642375d6798d75d28100008e3c74b5a96d79d21/Labs/Diffusion%20Models/assets/denoise_flow.jpg' width=500>\n",
    "\n",
    "The reverse process of a diffusion model is illustrated above:\n",
    "\n",
    "1. **Noise Initialization** (First Input \\( t = 0 \\)):\n",
    "   - The model begins with an entirely noisy image.\n",
    "\n",
    "2. **Model Processing**:\n",
    "   - The model takes the noisy image and predicts a slightly denoised version.\n",
    "   - This process iteratively refines the image by reducing noise step by step.\n",
    "\n",
    "3. **Output and Feedback Loop**:\n",
    "   - The output image, with reduced noise $( \\text{noise} - 1 )$, becomes the input for the next iteration (\\( t > 0 \\)).\n",
    "   - This feedback loop continues until the image is sufficiently denoised, progressively reconstructing the original content.\n",
    "\n",
    "Essentially, the model uses iterative predictions to transform noise into a coherent image through multiple steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjS7bE76ughp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clgwSrDigerH"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ifo1nrQ3FDzh"
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "X_train = X_train[y_train.squeeze() == 1]\n",
    "X_train = (X_train / 127.5) - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNEekOGYFDxl"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 32     # input image size, CIFAR-10 is 32x32\n",
    "BATCH_SIZE = 128  # for training batch size\n",
    "timesteps = 16    # how many steps for a noisy image into clear\n",
    "time_bar = 1 - np.linspace(0, 1.0, timesteps + 1) # linspace for timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6v15PLm4FDvx"
   },
   "outputs": [],
   "source": [
    "plt.plot(time_bar, label='Noise')\n",
    "plt.plot(1 - time_bar, label='Clarity')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTyPGFKogpBm"
   },
   "source": [
    "## Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8HwkhJ4_FDt7"
   },
   "outputs": [],
   "source": [
    "def cvtImg(img):\n",
    "    img = img - img.min()\n",
    "    img = (img / img.max())\n",
    "    return img.astype(np.float32)\n",
    "\n",
    "def show_examples(x):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(25):\n",
    "        plt.subplot(5, 5, i+1)\n",
    "        img = cvtImg(x[i])\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "\n",
    "show_examples(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Fml0NRFkbEi"
   },
   "source": [
    "**Key Components**\n",
    "\n",
    "* **Normalization**: This function normalizes the input image (img) to a range between 0 and 1.\n",
    "\n",
    "* **Data Type:** It converts the image data type to np.float32, which is often preferred for numerical computations in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKb6NYQpgsrQ"
   },
   "source": [
    "## Noise Generation\n",
    "\n",
    "We are going to simulate a diffusion process where an image is gradually degraded by adding noise over multiple steps. This process is essential for training diffusion models, as they learn to reverse this noise addition to generate images.\n",
    "\n",
    "\n",
    "The `forward_noise` function takes an image and a timestep as input and applies Gaussian noise according to a pre-defined schedule. By repeating this process, you can progressively degrade the clarity of an image. This technique is fundamental to how diffusion models learn to generate images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMABZYe_FDr6"
   },
   "outputs": [],
   "source": [
    "def forward_noise(x, t):\n",
    "    a = time_bar[t]        # base noise level\n",
    "    b = time_bar[t + 1]    # next noise level\n",
    "\n",
    "    noise = np.random.normal(size=x.shape)\n",
    "    # Reshape 'a' and 'b' to add dimensions compatible with 'x'\n",
    "    a = a[:, np.newaxis, np.newaxis, np.newaxis]  # Add 3 new axes\n",
    "    b = b[:, np.newaxis, np.newaxis, np.newaxis]  # Add 3 new axes\n",
    "\n",
    "    img_a = x * (1 - a) + noise * a\n",
    "    img_b = x * (1 - b) + noise * b\n",
    "    return img_a, img_b\n",
    "\n",
    "def generate_ts(num):\n",
    "    return np.random.randint(0, timesteps, size=num)\n",
    "\n",
    "# t = np.full((25,), timesteps - 1) # if you want see clarity\n",
    "# t = np.full((25,), 0)             # if you want see noisy\n",
    "t = generate_ts(25)             # random for training data\n",
    "a, b = forward_noise(X_train[:25], t)\n",
    "show_examples(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqWrkKfPj7j9"
   },
   "source": [
    "**Key Components**\n",
    "\n",
    "* **Input:** It takes an image (x) and a timestep (t) as input.\n",
    "* **Noise Levels:** It determines the noise levels for the current timestep (a) and the next timestep (b) using the time_bar array. time_bar contains pre-calculated noise levels that decrease over time. This is the \"noise schedule\" you mentioned earlier.\n",
    "* **Adding Noise:** Gaussian noise is added to the input image based on the noise level:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NnKFFudgvMa"
   },
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbvti6zplWBd"
   },
   "source": [
    "**Block Function**\n",
    "\n",
    "\n",
    "This function defines a basic building block of the U-Net, consisting of convolutional layers, activation functions, and layer normalization. It processes both the image features (x_img) and timestep information (x_ts) to learn how noise affects the image at different stages of the diffusion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nD_7PHI3FDp1"
   },
   "outputs": [],
   "source": [
    "def block(x_img, x_ts):\n",
    "    x_parameter = layers.Conv2D(128, kernel_size=3, padding='same')(x_img)\n",
    "    x_parameter = layers.Activation('relu')(x_parameter)\n",
    "\n",
    "    time_parameter = layers.Dense(128)(x_ts)\n",
    "    time_parameter = layers.Activation('relu')(time_parameter)\n",
    "    time_parameter = layers.Reshape((1, 1, 128))(time_parameter)\n",
    "    x_parameter = x_parameter * time_parameter\n",
    "\n",
    "    # -----\n",
    "    x_out = layers.Conv2D(128, kernel_size=3, padding='same')(x_img)\n",
    "    x_out = x_out + x_parameter\n",
    "    x_out = layers.LayerNormalization()(x_out)\n",
    "    x_out = layers.Activation('relu')(x_out)\n",
    "\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCV0Vm3hlaVA"
   },
   "source": [
    "**`make_model` Function**\n",
    "\n",
    "This function assembles the complete U-Net model:\n",
    "\n",
    "* **Input Layers:** It creates input layers for the image (x_input) and the timestep (x_ts_input).\n",
    "* **Timestep Embedding:** The timestep is embedded into a higher-dimensional representation to provide more context to the model about the diffusion stage.\n",
    "* **Encoding Path:** The image is processed through a series of convolutional blocks, gradually downsampling the feature maps to capture larger-scale patterns.\n",
    "* **Bottleneck:** A central block processes the most compressed representation of the image.\n",
    "* **Decoding Path:** The feature maps are upsampled and combined with features from the encoding path using skip connections to reconstruct fine details.\n",
    "* **Output Layer:** A final convolutional layer produces the denoised image.\n",
    "* **Model Creation:** The function returns a Keras model object that takes the imag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1XoKCAnFDj4"
   },
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    x = x_input = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3), name='x_input')\n",
    "\n",
    "    x_ts = x_ts_input = layers.Input(shape=(1,), name='x_ts_input')\n",
    "    x_ts = layers.Dense(192)(x_ts)\n",
    "    x_ts = layers.LayerNormalization()(x_ts)\n",
    "    x_ts = layers.Activation('relu')(x_ts)\n",
    "\n",
    "    # ----- left ( down ) -----\n",
    "    x = x32 = block(x, x_ts)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "\n",
    "    x = x16 = block(x, x_ts)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "\n",
    "    x = x8 = block(x, x_ts)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "\n",
    "    x = x4 = block(x, x_ts)\n",
    "\n",
    "    # ----- MLP -----\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Concatenate()([x, x_ts])\n",
    "    x = layers.Dense(128)(x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Dense(4 * 4 * 32)(x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Reshape((4, 4, 32))(x)\n",
    "\n",
    "    # ----- right ( up ) -----\n",
    "    x = layers.Concatenate()([x, x4])\n",
    "    x = block(x, x_ts)\n",
    "    x = layers.UpSampling2D(2)(x)\n",
    "\n",
    "    x = layers.Concatenate()([x, x8])\n",
    "    x = block(x, x_ts)\n",
    "    x = layers.UpSampling2D(2)(x)\n",
    "\n",
    "    x = layers.Concatenate()([x, x16])\n",
    "    x = block(x, x_ts)\n",
    "    x = layers.UpSampling2D(2)(x)\n",
    "\n",
    "    x = layers.Concatenate()([x, x32])\n",
    "    x = block(x, x_ts)\n",
    "\n",
    "    # ----- output -----\n",
    "    x = layers.Conv2D(3, kernel_size=1, padding='same')(x)\n",
    "    model = tf.keras.models.Model([x_input, x_ts_input], x)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MisjfR6HlxZy"
   },
   "source": [
    "The U-Net architecture is characterized by its symmetrical encoding and decoding paths, resembling a \"U\" shape. Skip connections between corresponding layers in the encoding and decoding paths allow the model to preserve fine-grained details during the denoising process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Am59LJCg7dm"
   },
   "source": [
    "## Model Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTKr1u5CmRKw"
   },
   "source": [
    "#### **Compilation**\n",
    "\n",
    "\n",
    "The model is compiled with an Adam optimizer and a mean absolute error (MAE) loss function. During training, it learns to predict the noise added to the image at each timestep. By iteratively denoising images, it learns to reverse the diffusion process and generate new images from noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJvScoPvGmNb"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0008)\n",
    "loss_func = tf.keras.losses.MeanAbsoluteError()\n",
    "model.compile(loss=loss_func, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mbFjw83GmLi"
   },
   "outputs": [],
   "source": [
    "def predict(x_idx=None):\n",
    "    x = np.random.normal(size=(32, IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "    for i in trange(timesteps):\n",
    "        t = i\n",
    "        x = model.predict([x, np.full((32), t)], verbose=0)\n",
    "    show_examples(x)\n",
    "\n",
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_step():\n",
    "    xs = []\n",
    "    x = np.random.normal(size=(8, IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "    for i in trange(timesteps):\n",
    "        t = i\n",
    "        x = model.predict([x, np.full((8),  t)], verbose=0)\n",
    "        if i % 2 == 0:\n",
    "            xs.append(x[0])\n",
    "\n",
    "    plt.figure(figsize=(20, 2))\n",
    "    for i in range(len(xs)):\n",
    "        plt.subplot(1, len(xs), i+1)\n",
    "        plt.imshow(cvtImg(xs[i]))\n",
    "        plt.title(f'{i}')\n",
    "        plt.axis('off')\n",
    "\n",
    "predict_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74EdexeCGmJh"
   },
   "outputs": [],
   "source": [
    "def train_one(x_img):\n",
    "    x_ts = generate_ts(len(x_img))\n",
    "    x_a, x_b = forward_noise(x_img, x_ts)\n",
    "    loss = model.train_on_batch([x_a, x_ts], x_b)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OV_8H7QOGmDN"
   },
   "outputs": [],
   "source": [
    "def train(R=50):\n",
    "    bar = trange(R)\n",
    "    total = 100\n",
    "    for i in bar:\n",
    "        for j in range(total):\n",
    "            x_img = X_train[np.random.randint(len(X_train), size=BATCH_SIZE)]\n",
    "            loss = train_one(x_img)\n",
    "            pg = (j / total) * 100\n",
    "            if j % 5 == 0:\n",
    "                bar.set_description(f'loss: {loss:.5f}, p: {pg:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdhEl-MpmNUF"
   },
   "source": [
    "#### **Training**\n",
    "\n",
    "The training process involves repeatedly presenting the model with noisy images and their corresponding timesteps, and guiding it to predict less noisy versions. This process allows the model to learn the underlying patterns in the data and effectively reverse the diffusion process to generate high-quality images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBIImkmlGmBJ"
   },
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    train()\n",
    "    # reduce learning rate for next training\n",
    "    model.optimizer.learning_rate = max(0.000001, model.optimizer.learning_rate * 0.9)\n",
    "\n",
    "    # show result\n",
    "    predict()\n",
    "    predict_step()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdvId2pO5xpn"
   },
   "source": [
    "Diffusion Models are a conceptually simple and elegant approach to the problem of generating data. Their State-of-the-Art results combined with non-adversarial training has propelled them to great heights, and further improvements can be expected in the coming years given their nascent status. In particular, Diffusion Models have been found to be essential to the performance of cutting-edge models like DALL-E 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zo2B5pV0ucWC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJqAJq6pucUM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PkhISyboucRj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "olkzIyKHucO_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s1EZUqWzEHdK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

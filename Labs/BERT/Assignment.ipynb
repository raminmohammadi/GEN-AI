{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CgVSVaOctqQ"
      },
      "source": [
        "# BERT (Graded)\n",
        "\n",
        "Welcome to your graded programming assignment on BERT! In this task, you will delve into the exciting realm of **Question Answering** by leveraging the power of the BERT model. This hands-on assignment will guide you through building a model that can interpret and answer questions based on a given set of texts.\n",
        "\n",
        "You will be utilizing the [SQuAD (Stanford Question Answering Dataset)](https://rajpurkar.github.io/SQuAD-explorer/) for this purpose. SQuAD is a widely recognized dataset in the field of natural language processing, consisting of questions posed on a set of Wikipedia articles, where the goal is to extract the answer to these questions from the provided context.\n",
        "\n",
        "Your task is to create a functional Question Answering system which uses the capabilities of BERT to understand and respond accurately to questions based on contextual information. By the end of this assignment, you will have deepened your understanding of how BERT works and how it can be applied to solve real-world NLP problems.\n",
        "\n",
        "**Instructions:**\n",
        "* Do not modify any of the codes.\n",
        "* Only write code when prompted. For example in some sections you will find the following,\n",
        "```\n",
        "# YOUR CODE GOES HERE\n",
        "# YOUR CODE STARTS HERE\n",
        "# TODO\n",
        "```\n",
        "Only modify those sections of the code.\n",
        "* You will find **REFLECTION** under few code cells where you are asked to write your thoughts or interpretations on the outputs.\n",
        "\n",
        "\n",
        "**You will learn to:**\n",
        "\n",
        "* Understand the architecture of BERT and its application in NLP tasks.\n",
        "* Preprocess datasets to be compatible with BERT inputs.\n",
        "* Implement a BERT-based question answering system using TensorFlow.\n",
        "* Fine-tune a pre-trained BERT model on the SQuAD dataset.\n",
        "* Analyze and reflect on the outputs of your model and the effectiveness of your preprocessing and training steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-PQBoDMLusq"
      },
      "source": [
        "# Question Answering using BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kwbh6oD6MYHW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from helpers import *\n",
        "from tests import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGR2k0tzMbjC"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEOOMI6XM2ix"
      },
      "source": [
        "### Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeW6b0JPM4BR"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the SQuAD dataset\n",
        "squad_dataset = load_dataset(\"squad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLGolKIRNEjt"
      },
      "outputs": [],
      "source": [
        "# Inspect the dataset\n",
        "print(squad_dataset[\"train\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvFrMr1_NZK3"
      },
      "source": [
        "#### Dataset Representation\n",
        "It consists of a large number of question-answer pairs, where each question is paired with a corresponding context paragraph from Wikipedia. The answer to each question is a span of text within the context paragraph.\n",
        "\n",
        "Here's how the SQuAD dataset is typically represented:\n",
        "\n",
        "```python\n",
        "{\n",
        "  \"title\": \"Article Title\",\n",
        "  \"context\": \"Context paragraph\",\n",
        "  \"question\": \"Question\",\n",
        "  \"answer\": {\n",
        "    \"answer_start\": 123,\n",
        "    \"text\": \"Answer Text\"\n",
        "  }\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dUqMV-7N7ku"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7h_QSmjXcE0"
      },
      "source": [
        "#### Initialize the tokenizer\n",
        "\n",
        "We're going to intialize the `distilbert-base-uncased` tokenizer.\n",
        "\n",
        "**DistilBERT:**\n",
        "\n",
        "* A distilled version of BERT, a powerful language model developed by Google AI.\n",
        "* Smaller and faster than BERT, while maintaining a significant portion of its performance.\n",
        "* Well-suited for various NLP tasks like text classification, question answering, and text generation.\n",
        "\n",
        "```python\n",
        "DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased'):\n",
        "```\n",
        "\n",
        "* `DistilBertTokenizerFast`: This class is a fast tokenizer implementation for DistilBERT. It leverages the tokenizers library for efficient tokenization.\n",
        "* `from_pretrained('distilbert-base-uncased')`: This part loads a pre-trained DistilBERT tokenizer from the Hugging Face Transformers library. The 'distilbert-base-uncased' string specifies the specific model architecture and vocabulary to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFTnBix_Xbfm"
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained tokenizer\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Tokenize a text input\n",
        "text = \"This is a sample text to be tokenized.\"\n",
        "tokens = tokenizer(text)\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk2Q96EkYPg9"
      },
      "source": [
        "This will output a dictionary containing the tokenized input, including token IDs, attention masks, and other relevant information.\n",
        "\n",
        "By using a pre-trained tokenizer, you can leverage the knowledge and capabilities of the DistilBERT model for various NLP tasks without having to train a tokenizer from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPu54yhO1f1f"
      },
      "source": [
        "#### Preprocessing steps\n",
        "\n",
        "1. **Strip Whitespace:**\n",
        "\n",
        "   - **Purpose:** Cleans the data by removing any leading or trailing whitespace from each question and context string, ensuring consistency before tokenization.\n",
        "\n",
        "2. **Tokenization Using BERT Tokenizer:**\n",
        "\n",
        "   - **Tokenization:** Transforms the text data into tokens that BERT can process.\n",
        "   - **Settings:**\n",
        "     - `max_length=384`: Ensures the sequences do not exceed 384 tokens, a practical length that fits within BERT's constraints of 512 tokens (considering special tokens added by BERT).\n",
        "     - `truncation=\"only_second\"`: Truncates tokens from the context (`input_ids`) if the combined length of the question and context exceeds `max_length`.\n",
        "     - `stride=128`: Allows a sliding window approach by overlapping context parts by 128 tokens, improving the chance of context coverage.\n",
        "     - `return_overflowing_tokens=True`: Keeps all splits from truncated contexts, resulting in multiple input sets for contexts longer than `max_length`.\n",
        "     - `return_offsets_mapping=True`: Returns offsets showing the start and end character positions of each token, crucial for mapping back to original text.\n",
        "     - `padding=\"max_length\"`: Pads sequences to `max_length`, ensuring uniform input size for model batches.\n",
        "\n",
        "3. **Offset Mapping and Answer Positioning:**\n",
        "   ```python\n",
        "   offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "   sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "   answers = examples[\"answers\"]\n",
        "   start_positions = []\n",
        "   end_positions = []\n",
        "   ```\n",
        "   - **offset_mapping:** Provides a character span to token index map, useful for locating answer positions in context.\n",
        "   - **sample_map:** Connects each split of overflowing tokens back to the original sample.\n",
        "   - **Answers Extraction:** Prepares to determine the start and end token positions of each answer within the tokenized inputs.\n",
        "\n",
        "4. **Determine Start and End Positions of Answers:**\n",
        "   ```python\n",
        "   for i, offset in enumerate(offset_mapping):\n",
        "       sample_idx = sample_map[i]\n",
        "       answer = answers[sample_idx]\n",
        "       start_char = answer[\"answer_start\"][0]\n",
        "       end_char = start_char + len(answer[\"text\"][0])\n",
        "       sequence_ids = inputs.sequence_ids(i)\n",
        "       context_start = sequence_ids.index(1)\n",
        "       context_end = sequence_ids.index(1, context_start + 1) - 1\n",
        "   ```\n",
        "   - **Sequence IDs:** Identifies which tokens belong to the question and which to the context.\n",
        "   - **Locate Answers:** Utilize the character offsets to locate the respective tokens within the context span.\n",
        "\n",
        "5. **Handling Answers Outside the Context Span:**\n",
        "   ```python\n",
        "   if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "       start_positions.append(0)  # No answer available in this span\n",
        "       end_positions.append(0)\n",
        "   ```\n",
        "   - **No Overlap Handling:** In cases where the answer is not fully contained in the truncated context, use zero as a placeholder indicating no answer.\n",
        "\n",
        "6. **Identify Token Indices for Answer Bounds:**\n",
        "   ```python\n",
        "   else:\n",
        "       idx = context_start\n",
        "       while idx <= context_end and offset[idx][0] <= start_char:\n",
        "           idx += 1\n",
        "       start_positions.append(idx - 1)\n",
        "\n",
        "       idx = context_end\n",
        "       while idx >= context_start and offset[idx][1] >= end_char:\n",
        "           idx -= 1\n",
        "       end_positions.append(idx + 1)\n",
        "   ```\n",
        "   - **Token Indices:** Finds the precise start (`start_positions`) and end indices (`end_positions`) of the answer within the tokenized context by iterating over token offsets.\n",
        "\n",
        "7. **Return Processed Inputs:**\n",
        "\n",
        "   - **Addition of Positions:** Attaches the calculated start and end positions to the tokenized input data, crucial for model training to learn exact answer boundaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA7NE2V2MkVA"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_function(examples):\n",
        "\n",
        "    # Defining the tokenizer\n",
        "    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "    # TODO\n",
        "    # Step 1: Strip whitespace and prepare lists of questions and contexts\n",
        "    questions =\n",
        "    contexts =\n",
        "\n",
        "    # Implement tokenization using the BERT tokenizer\n",
        "    # Step 2: Use the tokenizer to tokenize questions and contexts\n",
        "    inputs = tokenizer(\n",
        "\n",
        "        questions,\n",
        "        contexts,\n",
        "        # TODO: Set max_length to 384\n",
        "        # TODO: Set truncation to \"only_second\"\n",
        "        # TODO: Set stride to 128\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        # Set padding to max length\n",
        "    )\n",
        "\n",
        "    # Step 3: Extract the position of answers\n",
        "    # Initialize lists to hold start and end positions\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    # Step 4: Iterate through each offset to find the token indices matching the start and end of answers\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        sample_idx = sample_map[i]\n",
        "        answer = answers[sample_idx]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = start_char + len(answer[\"text\"][0])\n",
        "\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "        context_start = sequence_ids.index(1)\n",
        "        context_end = sequence_ids.index(1, context_start + 1) - 1\n",
        "\n",
        "        # Step 5: Check if the entire answer is present in the context\n",
        "        # If the answer is outside the present context, label it appropriately (e.g., (0, 0))\n",
        "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Step 6: Find the tokens that correspond to the start and end positions of the answer\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    # TODO\n",
        "    # Step 7: Add the start and end positions to the model inputs\n",
        "    inputs[\"start_positions\"] =\n",
        "    inputs[\"end_positions\"] =\n",
        "\n",
        "\n",
        "    return inputs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q39HhcVj10Et"
      },
      "source": [
        "This preprocessing function handles preparing the SQuAD dataset for use with BERT by ensuring text is appropriately tokenized, answer positions are aligned, and that the model receives uniform input. The function’s logic ensures BERT can effectively learn to map questions to their respective answers within the given context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrwlCjcr4BLh"
      },
      "source": [
        "## Model Training and Evaluation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D90Bw1P84VHC"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "\n",
        "# Import necessary libraries\n",
        "\n",
        "# 2. Create Model and Configure Training\n",
        "def create_qa_model():\n",
        "    # TODO\n",
        "    # Step 1: Load the pre-trained Distill-BERT model for question answering from Hugging Face\n",
        "    model =\n",
        "\n",
        "    # TODO: Setup an optimizer\n",
        "    # Step 2: Use create_optimizer compatible with HF model for training\n",
        "    num_train_steps =  # This should be the total number of training steps\n",
        "    # - init_lr: The initial learning rate\n",
        "    # - num_warmup_steps: Gradually increase the learning rate to the target value\n",
        "\n",
        "    # TODO: Choose and configure the loss function\n",
        "    # Step 3: Define the SparseCategoricalCrossentropy loss function for the model, ensuring it's suitable for the question-answering task\n",
        "    loss =\n",
        "\n",
        "    # TODO: Compiling\n",
        "    # Step 4: Compile the model with the optimizer and the loss function\n",
        "\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ulwVvROU3EJ"
      },
      "source": [
        "**Relection**\n",
        "\n",
        "\\<Write your thoughts about the model structure here>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUgc2Q0393fy"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "def train_model(model, train_dataset, validation_dataset):\n",
        "    # Convert to tf.data.Dataset\n",
        "\n",
        "    # TODO: Prepare train and validation datasets as TensorFlow datasets\n",
        "    # Step 1: Use the model's prepare_tf_dataset method to create the training and validation datasets\n",
        "    #         - Set shuffle=True for randomizing training data\n",
        "    #         - Batch size should be reasonable to fit in memory (e.g., 16)\n",
        "    train_set =\n",
        "\n",
        "    val_set =\n",
        "\n",
        "    # TODO: Add any extra callbacks necessary for saving models or early stopping\n",
        "    # Step 2: Configure training callbacks\n",
        "    callbacks = [\n",
        "      #  - EarlyStopping stops training when there is no improvement to prevent overfitting\n",
        "      #  - ModelCheckpoint saves the model at the epoch with the lowest validation loss\n",
        "    ]\n",
        "\n",
        "    # Adjust callback hooks if needed\n",
        "    for callback in callbacks:\n",
        "        if not hasattr(callback, '_implements_train_batch_hooks'):\n",
        "            callback._implements_train_batch_hooks = lambda: False\n",
        "            callback._implements_test_batch_hooks = lambda: False\n",
        "            callback._implements_predict_batch_hooks = lambda: False\n",
        "            callback._implements_call_batch_hooks = lambda: False\n",
        "\n",
        "    # Train the model\n",
        "    # Step 3: Fit the model to the training data while validating on validation data\n",
        "    #         - Use the history object to analyze or plot training and validation loss\n",
        "    history = model.fit(\n",
        "        train_set,\n",
        "        validation_data=val_set,\n",
        "        epochs=1,  # Set epochs (minimal to observe speed and preventing long runs)\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TtETGpWMsKS"
      },
      "source": [
        "\n",
        "#### **Key Features**\n",
        "\n",
        "The model training is responsible for configuring the training process and optimizing the BERT model's performance on the SQuAD dataset. Here's a breakdown of the steps involved:\n",
        "\n",
        "1. **Dataset Preparation**:\n",
        "\n",
        "   - **Conversion to TensorFlow Dataset**: Utilizes BERT's `prepare_tf_dataset` method to convert the preprocessed tokenized dataset into a TensorFlow dataset format suitable for training.\n",
        "   - **Batching and Shuffling**:\n",
        "     - `shuffle=True` for the training set ensures that each epoch sees data in a different order, promoting better model generalization.\n",
        "     - `batch_size=16` defines the number of samples processed before updating the model, balancing memory use and training speed.\n",
        "\n",
        "2. **Training Callbacks**:\n",
        "\n",
        "   - **EarlyStopping**: Halts training if the validation loss does not improve for 2 consecutive epochs, preventing further overfitting and saving computational time.\n",
        "   - **ModelCheckpoint**: Saves the model only when it achieves a new lowest validation loss, ensuring the best-performing model is retained.\n",
        "\n",
        "3. **Model Fitting**:\n",
        "\n",
        "   - **Training Execution**: The `fit` method iteratively optimizes the model parameters over the training data for the specified number of epochs.\n",
        "   - **Epochs**: Determines how many times the entire training dataset will pass through the model. Adjust depending on performance and overfitting.\n",
        "   - **Validation**: Continuously evaluates the model's performance on a separate validation dataset to monitor overfitting and generalization capabilities.\n",
        "\n",
        "4. **Model History**:\n",
        "   - **Output Analysis**: The `history` object stores details about the training process, such as loss and accuracy metrics for each epoch, which can be plotted for performance evaluation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "930cQIc79loe"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "\n",
        "    print(\"Running all tests...\")\n",
        "    run_all_tests()\n",
        "\n",
        "    print(\"Loading and preprocessing dataset...\")\n",
        "    # Apply preprocessing to the dataset\n",
        "    tokenized_dataset = squad_dataset.map(\n",
        "        preprocess_function,\n",
        "        batched=True,\n",
        "        remove_columns=squad_dataset[\"train\"].column_names,\n",
        "    )\n",
        "\n",
        "    print(\"Creating model...\")\n",
        "    model = create_qa_model()\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    history = train_model(\n",
        "        model,\n",
        "        tokenized_dataset[\"train\"],\n",
        "        tokenized_dataset[\"validation\"]\n",
        "    )\n",
        "\n",
        "    # Save the model\n",
        "    model.save_pretrained(\"qa_model_saved\")\n",
        "\n",
        "    test_model_loss(history.history['val_loss'])\n",
        "\n",
        "    return model, tokenizer, history\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seed for reproducibility\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    # Run training\n",
        "    model, tokenizer, history = main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgC4yexEUwpu"
      },
      "source": [
        "**Reflection**\n",
        "\n",
        "\\<Write your observations here>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8YDTjYFtNNy"
      },
      "source": [
        "#### Improvement Strategies\n",
        "\n",
        "Here are some model improvement strategies you can consider to improve the model:\n",
        "\n",
        "* **Hyperparameter Tuning:**\n",
        "\n",
        "  Adjust learning rates (e.g., 2e-5), batch sizes (e.g., 16 or 32), and epochs (2-4).\n",
        "* **Data Augmentation:**\n",
        "\n",
        "  Use paraphrasing to create more data. Ensure long contexts are managed with overlapping windows.\n",
        "* **Advanced Models:**\n",
        "\n",
        "  Use larger models like BERT-large or switch to RoBERTa/ALBERT for potentially better performance.\n",
        "* **Regularization:**\n",
        "\n",
        "  Increase dropout (e.g., 0.2), employ weight decay, and use early stopping to minimize overfitting.\n",
        "* **Optimizing Training:**\n",
        "\n",
        "  Implement learning rate warmup and consider layer-wise learning rate decay.\n",
        "* **Fine-tuning Tips:**\n",
        "\n",
        "  Freeze lower layers initially; apply task-specific model tweaks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsPtwI8uVGEC"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3lVVDjjwQkp"
      },
      "outputs": [],
      "source": [
        "question = \"Who wrote Hamlet?\"\n",
        "context = \"Hamlet is a tragedy written by William Shakespeare sometime between 1599 and 1601.\"\n",
        "answer = get_answer(question, context, model, tokenizer)\n",
        "print(f\"\\nQuestion: {question}\")\n",
        "print(f\"Context: {context}\")\n",
        "print(f\"Answer: {answer}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ng0z-FNwQh8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fkhc1WDsvp_F"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

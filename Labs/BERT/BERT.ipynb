{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpPSpgoL1jES"
      },
      "source": [
        "<center>\n",
        "    <h1>BERT</h1>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6ttICWQjzTd"
      },
      "source": [
        "# Brief Recap\n",
        "**BERT** was introduced by Google in a research paper titled \"[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](arxiv.org/abs/1810.04805)\" in 2018.\n",
        "\n",
        "BERT revolutionized natural language processing (NLP) by enabling deep bidirectional context in language models. This means it considers the context from both the left and right of a word during training, allowing for better understanding of word meanings in context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnPJ3z4Hj21S"
      },
      "source": [
        "# Architecture\n",
        "\n",
        "<img src='https://www.researchgate.net/publication/349546860/figure/fig2/AS:994573320994818@1614136166736/The-Transformer-based-BERT-base-architecture-with-twelve-encoder-blocks.ppm' width=500>\n",
        "\n",
        "1. **Input Layer:**\n",
        "\n",
        "  * The input text is tokenized into individual words or subwords (t1, t2, t3, etc.).\n",
        "  * Each token is represented by a numerical embedding.\n",
        "  * Positional encoding is added to the embeddings to provide the model with\n",
        "  * information about the relative position of each token in the sequence.\n",
        "2. **Transformer Encoder:**\n",
        "\n",
        "  * This is the core of the BERT model. It consists of multiple layers (typically 12 in the original BERT model) that process the input sequence in parallel.\n",
        "  * Each layer contains two main components:\n",
        "  * Multi-Head Attention: This layer allows the model to weigh the importance of different parts of the input sequence when processing a particular token. It can capture long-range dependencies in the text.\n",
        "  * Feed-Forward Neural Network: This layer applies non-linear transformations to the input, helping the model learn complex patterns.\n",
        "  * After each layer, an \"Add & Norm\" operation is applied, which involves adding the input to the output of the layer and then normalizing the result. This helps stabilize the training process.\n",
        "3. **Classification Layer:**\n",
        "\n",
        "  * The final layer of the model is a classification layer, which is used to make predictions based on the input sequence.\n",
        "  * It typically consists of a dense layer with a softmax activation function, which outputs probabilities for different classes.\n",
        "4. **Masked Language Modeling (MLM):**\n",
        "\n",
        "  * One of the pre-training tasks used to train BERT.\n",
        "  * Some tokens in the input sequence are randomly masked (represented as \"[MASK]\" in the image).\n",
        "  * The model is then trained to predict the original masked tokens based on the context provided by the surrounding tokens.\n",
        "5. **Next Sentence Prediction (NSP):**\n",
        "\n",
        "  * Another pre-training task used to train BERT.\n",
        "  * Two sentences are input to the model, and the model is trained to predict whether the second sentence is the actual next sentence in the original text.\n",
        "\n",
        "Overall, the BERT model is trained on a massive amount of text data using the MLM and NSP tasks. This pre-training allows the model to learn a deep understanding of language and context, which can then be applied to a wide range of natural language processing tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF5m_FV8yLCi"
      },
      "source": [
        "# Applications\n",
        "BERT has diverse applications across several domains:\n",
        "\n",
        "1. **Text Classification:**\n",
        "\n",
        "  * **Sentiment analysis:** Determining the sentiment of a piece of text (positive, negative, neutral).\n",
        "Topic classification: Categorizing text into predefined topics.\n",
        "  * **Intent classification:** Identifying the intent behind a user's query.\n",
        "2. **Question Answering:**\n",
        "\n",
        "  * Answering questions based on a given context, such as a document or a knowledge base.\n",
        "  * **Extractive question answering:** Identifying the specific span of text that answers the question.\n",
        "  * **Generative question answering:** Generating a textual answer to a question.\n",
        "3. **Text Generation:**\n",
        "\n",
        "  * Generating text, such as articles, poems, or code.\n",
        "  * **Text summarization:** Condensing long texts into shorter summaries.\n",
        "  * **Text translation:** Translating text from one language to another.\n",
        "\n",
        "4. **Named Entity Recognition (NER):**\n",
        "\n",
        "  * Identifying named entities in text, such as people, organizations, and locations.\n",
        "\n",
        "8. **Search Engines:**\n",
        "\n",
        "  * Improving search engine results by understanding the semantic meaning of queries.\n",
        "9. **Chatbots and Virtual Assistants:**\n",
        "\n",
        "  * Enhancing the ability of chatbots and virtual assistants to understand and respond to natural language queries.\n",
        "10. **Biomedicine:**\n",
        "\n",
        "  * Analyzing medical text, such as clinical notes and research papers.\n",
        "Identifying drug-drug interactions and adverse effects.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9dogSW87zsK"
      },
      "source": [
        "# Implementation of BERT using TensorFlow\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pD6wQ1F_GUUC"
      },
      "source": [
        "## Approach 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4fetQUv6NMT"
      },
      "source": [
        "\n",
        "\n",
        "### **Step 1: Define Positional Encoding**\n",
        "\n",
        "Positional encoding helps the model understand the order of tokens in the input sequence.\n",
        "\n",
        "```python\n",
        "get_positional_encoding(max_seq_len, d_model)\n",
        "```\n",
        "* `max_seq_len` determines the maximum length of input sequences.\n",
        "* `d_model` is the dimensionality of the embedding space.\n",
        "\n",
        "We create a grid of positions and dimensions, applying the sine and cosine transformations to populate the encoding.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuKtsNCu7TuT"
      },
      "outputs": [],
      "source": [
        "def get_positional_encoding(max_seq_len, d_model):\n",
        "    pos = np.arange(max_seq_len)[:, np.newaxis]\n",
        "    i = np.arange(d_model)[np.newaxis, :]\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    positional_encoding = pos * angle_rates\n",
        "    positional_encoding[:, 0::2] = np.sin(positional_encoding[:, 0::2])\n",
        "    positional_encoding[:, 1::2] = np.cos(positional_encoding[:, 1::2])\n",
        "    return tf.cast(positional_encoding, dtype=tf.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00pDNxq67VhB"
      },
      "source": [
        "### Step 2: Define the Multi-Head Attention Layer\n",
        "\n",
        "Multi-head attention allows the model to jointly attend to information from different representation subspaces. This helps the model capture various aspects of the input sequences.\n",
        "\n",
        "**Key Components:**\n",
        "\n",
        "1. Linear Transformations:\n",
        "\n",
        "  * Input $x$ is projected into three different spaces: Query (Q), Key (K), and Value (V) using dense layers.\n",
        "2. Splitting Heads:\n",
        "\n",
        "  * The model splits the projected vectors into multiple heads. Each head independently performs attention calculations.\n",
        "3. Scaled Dot-Product Attention:\n",
        "\n",
        "  * The attention weights are computed using the scaled dot-product of Q and K:\n",
        "  $$ \\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V $$\n",
        "  * The dot product is scaled by $\\sqrt{d_k}$\n",
        "  (the depth of the keys) to prevent overly large gradients.\n",
        "4. Concatenation and Final Dense Layer:\n",
        "\n",
        "  * The outputs of all heads are concatenated and passed through a final dense layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sp9l87457WUF"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_heads, d_model):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        assert d_model % self.num_heads == 0  # Check if divisible\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = Dense(d_model)  # Query\n",
        "        self.wk = Dense(d_model)  # Key\n",
        "        self.wv = Dense(d_model)  # Value\n",
        "        self.dense = Dense(d_model)  # Final output layer\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])  # (batch_size, num_heads, seq_len, depth)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        q = self.wq(inputs)\n",
        "        k = self.wk(inputs)\n",
        "        v = self.wv(inputs)\n",
        "\n",
        "        q = self.split_heads(q)  # Split into heads\n",
        "        k = self.split_heads(k)\n",
        "        v = self.split_heads(v)\n",
        "\n",
        "        scaled_attention_logits = tf.matmul(q, k, transpose_b=True)\n",
        "        scaled_attention_logits /= tf.math.sqrt(tf.cast(self.depth, tf.float32))\n",
        "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "        output = tf.matmul(attention_weights, v)  # Compute weighted sum of values\n",
        "        output = tf.transpose(output, perm=[0, 2, 1, 3])  # (batch_size, seq_len, num_heads, depth)\n",
        "        output = tf.reshape(output, (tf.shape(output)[0], -1, self.d_model))  # Concatenate heads\n",
        "\n",
        "        return self.dense(output)  # Final projection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjSQSEWZ8ak7"
      },
      "source": [
        "### Step 3: Define the Feed-Forward Layer\n",
        "\n",
        "The feed-forward network applies a transformation to each position independently and identically, helping to learn complex representations.\n",
        "\n",
        "**Structure:**\n",
        "\n",
        "The feed-forward layer consists of two dense layers:\n",
        "* The first dense layer expands the dimensionality (with a non-linear activation, typically ReLU).\n",
        "* The second layer projects back to the original model dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwFzqzCV8k0-"
      },
      "outputs": [],
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.dense1 = Dense(d_ff, activation='relu')  # Expanding dimension\n",
        "        self.dense2 = Dense(d_model)  # Projecting back to d_model\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.dense2(self.dense1(inputs))  # Apply the two dense layers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1KznmeG8m8l"
      },
      "source": [
        "### Step 4: Define the Encoder Layer\n",
        "\n",
        "An encoder layer consists of the multi-head attention mechanism and the feed-forward network. It also includes residual connections and layer normalization, which help in stabilizing and improving the training of deep networks.\n",
        "\n",
        "**Components:**\n",
        "\n",
        "1. **Multi-Head Attention:**\n",
        "Computes the attention outputs for the input sequence.\n",
        "2. **Add & Norm:**\n",
        "The output of the attention layer is added to the input (residual connection) and then normalized.\n",
        "3. **Feed-Forward Network:**\n",
        "The result from the previous step goes through the feed-forward network, followed by another residual connection and normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-x_Qr628ky1"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_heads, d_model, d_ff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(num_heads, d_model)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.attention(inputs)  # Multi-head attention output\n",
        "        out1 = self.layernorm1(inputs + self.dropout1(attn_output, training=training))  # Add & Norm\n",
        "        ffn_output = self.ffn(out1)  # Feed-forward network\n",
        "        return self.layernorm2(out1 + self.dropout2(ffn_output, training=training))  # Add & Norm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TnWoAMC84Ua"
      },
      "source": [
        "### Summary\n",
        "\n",
        "\n",
        "* **Positional Encoding** helps the model understand the order of tokens.\n",
        "Multi-Head Attention allows the model to focus on different parts of the input.\n",
        "* **Feed-Forward Layers** help transform and process the output from the attention mechanism.\n",
        "* **Encoder Layers** stack attention and feed-forward mechanisms with normalization and residual connections for stability.\n",
        "* The **BERT Model** integrates these components to process sequences effectively, making it suitable for various NLP tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_cfeDFH9588"
      },
      "source": [
        "## Approach 2\n",
        "\n",
        "We'll leverage the powerful transformers library to streamline the process and provide a solid foundation for further exploration.\n",
        "\n",
        "\n",
        "```\n",
        "from transformers import TFBertModel\n",
        "\n",
        "model = TFBertModel.from_pretrained('bert-base-uncased', **kwargs)\n",
        "\n",
        "```\n",
        "The bare Bert Model transformer outputting raw hidden-states without any specific head on top.\n",
        "\n",
        "This model inherits from [TFPreTrainedModel](https://huggingface.co/docs/transformers/v4.45.2/en/main_classes/model#transformers.TFPreTrainedModel). Check the superclass documentation for the generic methods the library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)\n",
        "\n",
        "This model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and behavior.\n",
        "\n",
        "\n",
        "**Key Arguments**\n",
        "\n",
        "* `pretrained_model_name_or_path:`\n",
        "\n",
        "  The name of the pre-trained model or the path to a directory containing model weights. For example, 'bert-base-uncased'.\n",
        "* `config:`\n",
        "\n",
        "  An instance of BertConfig or a configuration dictionary to customize the model architecture.\n",
        "* `cache_dir:`\n",
        "\n",
        "  Optional. Directory to cache the pre-trained models.\n",
        "* `from_pt:`\n",
        "\n",
        "  Optional. If True, loads the model from a PyTorch checkpoint.\n",
        "\n",
        "\n",
        "**Key Call Arguments**\n",
        "```python\n",
        "outputs = model(inputs, **kwargs)\n",
        "\n",
        "```\n",
        "* `input_ids:`\n",
        "\n",
        "  A tensor of shape (batch_size, sequence_length) containing token IDs.\n",
        "* `attention_mask:`\n",
        "\n",
        "  (Optional) A tensor of the same shape as input_ids, where 1 indicates a token to be attended to, and 0 indicates a padding token.\n",
        "* `token_type_ids:`\n",
        "\n",
        "  (Optional) A tensor that distinguishes between different sentences in tasks like question answering. It's typically of the same shape as input_ids.\n",
        "* `training:`\n",
        "\n",
        "  (Optional) A boolean that specifies whether the model should be in training mode. If set to True, dropout will be applied.\n",
        "\n",
        "**Outputs**\n",
        "\n",
        "The outputs of the TFBertModel are generally a BaseModelOutput object, which includes:\n",
        "\n",
        "* `last_hidden_state:`\n",
        "\n",
        "  A tensor of shape (batch_size, sequence_length, hidden_size), representing the hidden states of the last layer of the model. Each token's representation can be used for downstream tasks.\n",
        "* `pooler_output:`\n",
        "\n",
        "  (Optional) A tensor of shape (batch_size, hidden_size) that contains the hidden state of the first token (usually the [CLS] token) after a linear transformation and Tanh activation. This can be used for classification tasks.\n",
        "* `hidden_states:`\n",
        "\n",
        "  (Optional) If output_hidden_states=True, this will contain the hidden states from all layers.\n",
        "* `attentions:`\n",
        "\n",
        "  (Optional) If output_attentions=True, this will contain the attention weights from all layers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o75qRmFEnMg"
      },
      "source": [
        "### Example Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "494MlzOh95Lk"
      },
      "outputs": [],
      "source": [
        "from transformers import TFBertModel, BertTokenizer\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Example text input\n",
        "text = \"Hello, how are you?\"\n",
        "\n",
        "# Tokenize and encode the input\n",
        "inputs = tokenizer(text, return_tensors='tf')\n",
        "\n",
        "# Get model outputs\n",
        "outputs = model(inputs)\n",
        "\n",
        "# The outputs include hidden states and attention outputs\n",
        "last_hidden_state = outputs.last_hidden_state\n",
        "print(last_hidden_state.shape)  # Shape: (batch_size, sequence_length, hidden_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrInmC0Rgdrv"
      },
      "source": [
        "# Named Entity Recognition using BERT\n",
        "\n",
        "<img src='https://miro.medium.com/v2/resize:fit:1400/0*gs2eAAiVleveib9x' width=500>\n",
        "\n",
        "Named Entity Recognition (NER) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories.\n",
        "\n",
        "**How NER works:**\n",
        "\n",
        "* **Tokenization:** The text is broken down into individual words or tokens.\n",
        "* **Part-of-Speech Tagging:** Each token is assigned a part-of-speech tag (e.g., noun, verb, adjective).\n",
        "* **Entity Recognition:** The system identifies sequences of tokens that form named entities.\n",
        "* **Entity Classification:** The identified entities are classified into predefined categories (e.g., person, organization, location).\n",
        "\n",
        "## Dataset Description\n",
        "\n",
        "\n",
        "[Annotate Corpus for Named Entity Recognition](https://www.kaggle.com/datasets/abhinavwalia95/entity-annotated-corpus)\n",
        " using GMB(Groningen Meaning Bank) corpus for entity classification with enhanced and popular features by Natural Language Processing applied to the data set.\n",
        "This dataset is an extract from GMB corpus which is tagged, annotated and built specifically to train the classifier to predict named entities such as name, location, etc.\n",
        "\n",
        "**Number of tagged entities:**\n",
        "```python\n",
        "'O': 1146068', geo-nam': 58388, 'org-nam': 48034, 'per-nam': 23790, 'gpe-nam': 20680, 'tim-dat': 12786, 'tim-dow': 11404, 'per-tit': 9800, 'per-fam': 8152, 'tim-yoc': 5290, 'tim-moy': 4262, 'per-giv': 2413, 'tim-clo': 891, 'art-nam': 866, 'eve-nam': 602, 'nat-nam': 300, 'tim-nam': 146, 'eve-ord': 107, 'per-ini': 60, 'org-leg': 60, 'per-ord': 38, 'tim-dom': 10, 'per-mid': 1, 'art-add': 1\n",
        "```\n",
        "\n",
        "**Essential info about entities:**\n",
        "\n",
        "* geo = Geographical Entity\n",
        "* org = Organization\n",
        "* per = Person\n",
        "* gpe = Geopolitical Entity\n",
        "* tim = Time indicator\n",
        "* art = Artifact\n",
        "* eve = Event\n",
        "* nat = Natural Phenomenon\n",
        "\n",
        "<br>\n",
        "Total Words Count = 1354149 <br>\n",
        "Target Data Column: \"tag\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20se3ST-94ys"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "import transformers\n",
        "from transformers import BertTokenizerFast\n",
        "from transformers import TFBertModel\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from helpers import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7Sdtg79iNxJ"
      },
      "source": [
        "## Load and Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jeU7gx994wi"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"ner_dataset.csv\",encoding = 'ISO-8859-1')\n",
        "df = df.dropna()\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "km6RgpF-94uT"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of Tags : {len(df.Tag.unique())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_aJ-7v98kwj"
      },
      "outputs": [],
      "source": [
        "pie = df['Tag'].value_counts()\n",
        "px.pie(names = pie.index,values= pie.values,hole = 0.5,title ='Total Count of Tags')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BTggavZjDRH"
      },
      "source": [
        "### Grouping, Tokenizing and Padding\n",
        "\n",
        "We are going to group, tokenize and pad our data for the BERT model by organizing it by sentences, converting the text into numerical IDs using BERT tokenizer, ensuring all sentences have the same length by padding or truncating them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLkECNvn8kuJ"
      },
      "outputs": [],
      "source": [
        "enc_pos = preprocessing.LabelEncoder()\n",
        "enc_tag = preprocessing.LabelEncoder()\n",
        "\n",
        "df.loc[:, \"POS\"] = enc_pos.fit_transform(df[\"POS\"])\n",
        "df.loc[:, \"Tag\"] = enc_tag.fit_transform(df[\"Tag\"])\n",
        "\n",
        "sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
        "pos = df.groupby(\"Sentence #\")[\"POS\"].apply(list).values\n",
        "tag = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFcKBvHisKh8"
      },
      "source": [
        "The following tokenize method transforms raw text sentences into numerical data that the BERT model can understand and process. This is an essential preprocessing step for applying BERT to natural language processing tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-SUEJsQhPqX"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "MAX_LEN = 128\n",
        "def tokenize(data,max_len = MAX_LEN):\n",
        "    input_ids = list()\n",
        "    attention_mask = list()\n",
        "    for i in tqdm(range(len(data))):\n",
        "        encoded = tokenizer.encode_plus(data[i],\n",
        "                                        add_special_tokens = True,\n",
        "                                        max_length = MAX_LEN,\n",
        "                                        is_split_into_words=True,\n",
        "                                        return_attention_mask=True,\n",
        "                                        padding = 'max_length',\n",
        "                                        truncation=True,return_tensors = 'np')\n",
        "\n",
        "\n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        attention_mask.append(encoded['attention_mask'])\n",
        "    return np.vstack(input_ids),np.vstack(attention_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdPQH5zUvK0u"
      },
      "source": [
        "For each sentence, it uses the `tokenizer.encode_plus method` from the `transformers` library to perform the following:\n",
        "\n",
        "* **Tokenization**: Breaks down the sentence into individual words or subwords.\n",
        "Encoding: Converts each token into a unique numerical ID.\n",
        "* **Special Tokens**: Adds special tokens like [CLS] (classification) and [SEP] (separator) to the beginning and end of the sequence.\n",
        "* **Padding and Truncation**: Ensures all sequences have the same length by padding shorter sequences with zeros and truncating longer sequences to the maximum length (MAX_LEN).\n",
        "* **Attention Mask**: Creates an attention mask where 1 indicates tokens to attend to and 0 indicates padding tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCp-v_NshQRE"
      },
      "outputs": [],
      "source": [
        "# Train test split our dataset\n",
        "X_train,X_test,y_train,y_test = train_test_split(sentences,tag,random_state=42,test_size=0.1)\n",
        "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BM9shOJmhQPE"
      },
      "outputs": [],
      "source": [
        "input_ids,attention_mask = tokenize(X_train,max_len = MAX_LEN)\n",
        "val_input_ids,val_attention_mask = tokenize(X_test,max_len = MAX_LEN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rN4qLZSwn7Ya"
      },
      "outputs": [],
      "source": [
        "# TEST: Checking Padding and Truncation length's\n",
        "was = list()\n",
        "for i in range(len(input_ids)):\n",
        "    was.append(len(input_ids[i]))\n",
        "set(was)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bQMHuJqn9uZ"
      },
      "outputs": [],
      "source": [
        "# Train Padding\n",
        "test_tag = list()\n",
        "for i in range(len(y_test)):\n",
        "    test_tag.append(np.array(y_test[i] + [0] * (128-len(y_test[i]))))\n",
        "\n",
        "# TEST:  Checking Padding Length\n",
        "was = list()\n",
        "for i in range(len(test_tag)):\n",
        "    was.append(len(test_tag[i]))\n",
        "set(was)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUpCqXzNoEnz"
      },
      "outputs": [],
      "source": [
        "# Train Padding\n",
        "train_tag = list()\n",
        "for i in range(len(y_train)):\n",
        "    train_tag.append(np.array(y_train[i] + [0] * (128-len(y_train[i]))))\n",
        "\n",
        "# TEST:  Checking Padding Length\n",
        "was = list()\n",
        "for i in range(len(train_tag)):\n",
        "    was.append(len(train_tag[i]))\n",
        "set(was)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHyoLeTGkY0N"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMppjUO3hQII"
      },
      "outputs": [],
      "source": [
        "from transformers import TFBertModel\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class BertLayer(Layer):\n",
        "    def __init__(self, bert_model, **kwargs):\n",
        "        super(BertLayer, self).__init__(**kwargs)\n",
        "        self.bert_model = bert_model\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_ids, attention_mask = inputs\n",
        "        # Call the TFBertModel within the call method\n",
        "        bert_output = self.bert_model(input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "        return bert_output[\"last_hidden_state\"]\n",
        "\n",
        "def create_model(bert_model, max_len=MAX_LEN):\n",
        "    input_ids = tf.keras.Input(shape=(max_len,), dtype='int32')\n",
        "    attention_masks = tf.keras.Input(shape=(max_len,), dtype='int32')\n",
        "\n",
        "    # Use the custom BertLayer\n",
        "    bert_output = BertLayer(bert_model)([input_ids, attention_masks])\n",
        "\n",
        "    embedding = tf.keras.layers.Dropout(0.3)(bert_output)\n",
        "    output = tf.keras.layers.Dense(17, activation='softmax')(embedding)\n",
        "    model = tf.keras.models.Model(inputs=[input_ids, attention_masks], outputs=[output])\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3JIjQOwkFBe"
      },
      "outputs": [],
      "source": [
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "model = create_model(bert_model,MAX_LEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV1U7x6G2oNa"
      },
      "source": [
        "**Key Features:**\n",
        "* **BertLayer:** A custom layer encapsulating the TFBertModel from the transformers library. It takes token IDs and attention masks as input and outputs the last hidden state from BERT.\n",
        "* **Input Layers:** Two input layers are defined for token IDs (input_ids) and attention masks (attention_masks).\n",
        "* **BERT Output:** The BertLayer is called with the input layers to obtain BERT's output.\n",
        "* **Dropout:** A dropout layer is added to reduce overfitting.\n",
        "* **Dense Layer:** A dense layer with a softmax activation is used for classification, predicting the entity tag for each token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZMyoxWPkE_U"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueGMuks-2IPu"
      },
      "source": [
        "**Reflection**\n",
        "\n",
        "* The model has over 100 million parameters.\n",
        "* This is a large number of parameters, but it is necessary for the model to be able to learn the complex patterns in the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YctQYT6rllHJ"
      },
      "source": [
        "# Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJjkhflckE5R"
      },
      "outputs": [],
      "source": [
        "early_stopping = EarlyStopping(mode='min',patience=5)\n",
        "history_bert = model.fit([input_ids,attention_mask],np.array(train_tag),\\\n",
        "                         validation_data = ([val_input_ids,val_attention_mask],np.array(test_tag)),\\\n",
        "                         epochs = 25,batch_size = 30*2,callbacks = early_stopping,verbose = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3NvR3qGkE3E"
      },
      "outputs": [],
      "source": [
        "plot_metrics(history_bert)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riX18DQBrMu2"
      },
      "source": [
        "**Reflection**\n",
        "> Under 25 epochs, our model has achieved quite an amazing performance, 95%. Now, It should be able to effortlessly recognize all the tags."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_5o6aQxq1aG"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ki1nHA_QqnLd"
      },
      "outputs": [],
      "source": [
        "def pred(val_input_ids,val_attention_mask):\n",
        "    return model.predict([val_input_ids,val_attention_mask])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yt8ORASjkE02"
      },
      "outputs": [],
      "source": [
        "def testing(val_input_ids,val_attention_mask,enc_tag,y_test):\n",
        "    val_input = val_input_ids.reshape(1,128)\n",
        "    val_attention = val_attention_mask.reshape(1,128)\n",
        "\n",
        "    # Print Original Sentence\n",
        "    sentence = tokenizer.decode(val_input_ids[val_input_ids > 0])\n",
        "    print(\"Original Text : \",str(sentence))\n",
        "    print(\"\\n\")\n",
        "    true_enc_tag = enc_tag.inverse_transform(y_test)\n",
        "\n",
        "    print(\"Original Tags : \" ,str(true_enc_tag))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    pred_with_pad = np.argmax(pred(val_input,val_attention),axis = -1)\n",
        "    pred_without_pad = pred_with_pad[pred_with_pad>0]\n",
        "    pred_enc_tag = enc_tag.inverse_transform(pred_without_pad)\n",
        "    print(\"Predicted Tags : \",pred_enc_tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSKdgxQ3hQEY"
      },
      "outputs": [],
      "source": [
        "testing(val_input_ids[0],val_attention_mask[0],enc_tag,y_test[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDQMkmnAq9N1"
      },
      "source": [
        "**Reflection**\n",
        "\n",
        "> Looking at original tags and predicted tags, both seems equal. Which means, model has performed well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jh-6b8-QhPoO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

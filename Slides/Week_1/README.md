## Video Recording

You can watch the video recoding for the first class on our Youtube channel. [Video Link](https://youtu.be/CtI7gWx5mEU)

# Week 1: Neural Network Foundations

Welcome to Week 1! This week introduces fundamental concepts essential for understanding neural networks, laying the groundwork for advanced topics in deep learning and AI.

## Learning Objectives
- Understand the basic structure and components of neural networks
- Explore essential algorithms: gradient descent and backpropagation
- Familiarize yourself with different activation functions and their roles
- Learn about optimization techniques, including momentum and Newton's method

## Key Topics Covered

### 1. Neural Network Architecture
- **Inputs and Outputs:** Basic structure of neural networks and how information flows.
- **Layers and Neurons:** What layers consist of, including hidden layers and the output layer.
- **Weights and Biases:** Their role in influencing neuron outputs.

### 2. Activation Functions
- **Linear vs. Non-linear Functions:** Importance of non-linearity in neural networks.
- **Common Activation Functions:** Sigmoid, Tanh, and ReLU—how and when to use them.

### 3. Gradient Descent
- Understanding the algorithm to optimize neural networks.
- Step-by-step process for updating weights to minimize errors.

### 4. Backpropagation
- Essential concepts behind computing gradients efficiently.
- Application of the chain rule in the context of neural networks.

### 5. Optimization Techniques
- **Momentum:** Improving gradient descent convergence speed.
- **Newton’s Method:** Leveraging Hessian and Jacobian matrices for optimization.

## Recommended Practice
- Implement basic neural network components (forward and backward passes).
- Experiment with different activation functions to understand their impact.
- Practice deriving gradients using the chain rule.

## Next Steps
Prepare yourself for Week 2 by reviewing and mastering these core concepts, as they are crucial for understanding advanced neural network architectures and deep learning methods.

Happy learning!

